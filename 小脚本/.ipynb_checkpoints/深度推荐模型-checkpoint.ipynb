{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "812beba3",
   "metadata": {},
   "source": [
    "## 推荐模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1103654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.utils.data.dataset as Dataset\n",
    "import torch.utils.data.dataloader as DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb44fa99",
   "metadata": {},
   "source": [
    "### Wide&Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "593958c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module): # Wide部分，实质上就是一个nn.Linear\n",
    "    def __init__(self, input_dim): \n",
    "        super(Linear, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=input_dim, out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class Dnn(nn.Module): # Deep部分\n",
    "    def __init__(self, hidden_units, dropout=0.):\n",
    "        \"\"\"\n",
    "        hidden_units: 列表， 每个元素表示每一层的神经单元个数， 比如[256, 128, 64], 两层网络， 第一层神经单元128， 第二层64， 第一个维度是输入维度\n",
    "        dropout: 失活率\n",
    "        \"\"\"\n",
    "        super(Dnn, self).__init__()\n",
    "        # 下面这句是创建全连接网络，具体层数由hidden_units长度而定，下面是先按顺序拼成输入和输出维度的对应组，然后创建多个线性层，存成模块列表\n",
    "        self.dnn_network = nn.ModuleList([nn.Linear(layer[0], layer[1]) for layer in list(zip(hidden_units[:-1], hidden_units[1:]))])\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for linear in self.dnn_network:\n",
    "            x = linear(x)\n",
    "            x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class WideDeep(nn.Module):\n",
    "    def __init__(self, feature_columns, hidden_units, dnn_dropout=0.):# feature_columns的输入很有讲究，详情见下\n",
    "        super(WideDeep, self).__init__()\n",
    "        \"\"\"\n",
    "        举个feature_columns的例子，假设有四个特征:a,b,c,d。其中a和b是连续型，c和d是类别型，那么c和d就要先labelEncoder转成索引，然后Embeeding，\n",
    "        据此将特征分为两组，稠密组和稀疏组，前者有a和b，后者是c和d；在特征输入到deep部分前，需要将c和d编码成稠密向量然后跟a和b拼接输入，所以需要\n",
    "        在数据输入后对c和d进行Embedding操作，那么就要告诉模型c和d是一组的，a和b是一组，让模型仅对c和d操作即可，除此之外，还要告诉模型c和d各自的\n",
    "        类别数和想要embedding的维度（一般所有稀疏特征Embedding的维度是统一的）。本模型中所设计的输入是这样的:\n",
    "        feature_columns = [[{'feat_num':1, 'embed_dim':1}, {'feat_num':1, 'embed_dim':1}], \n",
    "                            [{'feat_num':vocab_c_size, 'embed_dim':5}, {'feat_num':vocab_c_size, 'embed_dim':5}]]\n",
    "        解读：输入可拆成两部分，ab一组，cd一组，分别都是列表，里面存放很多字典，一个特征对应一个字典，字典里有两个key；\n",
    "        对于稠密组的每个特征，feat_num和embed_dim都是1即可；对于稀疏组每个特征，feat_num是类别数，embed_dim是Embedding后的维度。（这里假设都是5）\n",
    "    \n",
    "        \"\"\"\n",
    "        self.dense_feature_cols, self.sparse_feature_cols = feature_columns # 分成两个组\n",
    "        \n",
    "        # embedding \n",
    "        self.embed_layers = nn.ModuleDict({ # 为每个稀疏特征创建对应的Embedding层，后面专门一对一转换\n",
    "            'embed_' + str(i): nn.Embedding(num_embeddings=feat['feat_num'], embedding_dim=feat['embed_dim'])\n",
    "            for i, feat in enumerate(self.sparse_feature_cols) # 循环取出稀疏组的每个特征对应的字典及特征位置索引，并用字典内信息定义Embedding层\n",
    "        })\n",
    "        \n",
    "        # 前面输入的hidden_units是Deep部分每个线性层的维度，这里要把Embedding后输入维度插入到第一个元素前，这样才能形成整个维度转化链\n",
    "        hidden_units.insert(0, len(self.dense_feature_cols) + len(self.sparse_feature_cols)*self.sparse_feature_cols[0]['embed_dim'])\n",
    "        self.dnn_network = Dnn(hidden_units, dnn_dropout) # 把维度转化链参数输入DNN中\n",
    "        self.linear = Linear(len(self.dense_feature_cols)) # 这个Wide部分的模型，就一个线性层（逻辑回归）\n",
    "        self.final_linear = nn.Linear(hidden_units[-1], 1) # 这个是将DNN输出结果转为一维数值的线性层\n",
    "    \n",
    "    def forward(self, x): # 注意：这里是输入的数据集，其中的稀疏特征一定得是转化为索引值得\n",
    "        dense_input, sparse_inputs = x[:, :len(self.dense_feature_cols)], x[:, len(self.dense_feature_cols):] # 把数据拆分成稠密部分和稀疏部分\n",
    "        sparse_inputs = sparse_inputs.long() # 稀疏部分类型转为LongTesnor\n",
    "        sparse_embeds = [self.embed_layers['embed_'+str(i)](sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])] # 对每个稀疏特征Embeeding，并以列表组在一起\n",
    "        sparse_embeds = torch.cat(sparse_embeds, axis=-1) # 将列表里每个特征得Embedding向量拼接在一起\n",
    "        dnn_input = torch.cat([sparse_embeds, dense_input], axis=-1) # 再将拼接好得稀疏部分和稠密部分拼接在一起，作为最终输入数据\n",
    "        \n",
    "        # Wide\n",
    "        wide_out = self.linear(dense_input) # Wide部分，这里写的是输入且仅输入所有稠密部分，但其实靠人为定义，一般是特殊得组合特征和稀疏id特征等，到时候可能要加个参数\n",
    "        \n",
    "        # Deep\n",
    "        deep_out = self.dnn_network(dnn_input) # 将拼接好的结果输入Deep层，得到输出结果（这里也未必输入所有数据）\n",
    "        deep_out = self.final_linear(deep_out) # 将Deep的输入结果转为一维\n",
    "        \n",
    "        # out\n",
    "        outputs = F.sigmoid(0.5 * (wide_out + deep_out)) # 最终输出结果是Wide和Deep部分的输出结果平均再过一个Sigmoid层，作为输出概率\n",
    "        \n",
    "        return outputs  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c17127",
   "metadata": {},
   "source": [
    "### Deep&Cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b5cb57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossNetwork(nn.Module): # Cross部分\n",
    "    def __init__(self, layer_num, input_dim):\n",
    "        super(CrossNetwork, self).__init__()\n",
    "        self.layer_num = layer_num\n",
    "        \n",
    "        # 定义Cross部分各个层的初始化参数，并装在一个列表里\n",
    "        self.cross_weights = nn.ParameterList([ # 这个列表跟list区别好像不大，但又有，就这么用吧\n",
    "            nn.Parameter(torch.rand(input_dim, 1)) # nn.Parameter与torch.Tensor的区别就是nn.Parameter会自动被认为是module的可训练参数\n",
    "            for i in range(self.layer_num)\n",
    "        ])\n",
    "        \n",
    "        # 定义Cross部分各个层的偏移量初始化参数，并装在一个列表里\n",
    "        self.cross_bias = nn.ParameterList([ \n",
    "            nn.Parameter(torch.rand(input_dim, 1))\n",
    "            for i in range(self.layer_num)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_0 = torch.unsqueeze(x, dim=2) # x是(None, dim)的形状， 先扩展一个维度到(None, dim, 1)\n",
    "        x = x_0.clone() # 深度复制变量\n",
    "        xT = x_0.clone().permute((0, 2, 1))     # 获取当前最新转化数据的shape:（None, 1, dim)，permute函数可以改变tensor型变量的维度顺序\n",
    "        for i in range(self.layer_num): # 这里是在执行Cross层交叉算法\n",
    "            \"\"\"\n",
    "            1. 这里解释下前面升级x维度的目的，我们需要让每个样本都执行cross过程，这中间必然涉及三维tensor，就比如第一步计算外积，得到方阵，\n",
    "            每个样本都要有这个方阵，方阵维度是2维，样本维度1个，加起来就是三维。如果开始不升级，用二维x去做，不可能出现3维，而且结果将\n",
    "            会是每个样本得到方阵的求和，所以必须开辟新维度，这样会将第一个样本维度单独提出来，只对后面两个维度进行计算。\n",
    "            2. 还要介绍下bmm和matmul两个函数，都是计算矩阵乘积的，区别是bmm只针对三维tensor间的计算，公式为：(b x m x n) @ (b x n x m) = (b x m x m)\n",
    "            正好符合需求；而matmul可以对两个维度数量不同的矩阵计算乘积，它会自动广播，比如：(b x m x n) @ (n x 1) = (b x m x 1)\n",
    "            它会进行\"后排维度对齐\"，最后两个维度彼此矩阵运算，其他维度互相广播，最后将后者广播成(b x n x 1)，然后进行计算，这实现了\n",
    "            外积结果与参数向量运算的过程；最后加上偏移参数和自己，得到每层的输出结果。\n",
    "            \"\"\"\n",
    "            x = torch.matmul(torch.bmm(x_0, xT), self.cross_weights[i]) + self.cross_bias[i] + x # (None, dim, 1)\n",
    "            xT = x.clone().permute((0, 2, 1)) # 获得新的下一层输入的X_T，形状是(None, 1, dim)\n",
    "        \n",
    "        x = torch.squeeze(x)  # 运算结束，把维度降回来：(None, dim)\n",
    "        return x\n",
    "\n",
    "class Dnn(nn.Module): # Deep部分，与wide&deep中的完全一样\n",
    "    def __init__(self, hidden_units, dropout=0.):\n",
    "        super(Dnn, self).__init__()\n",
    "        self.dnn_network = nn.ModuleList([nn.Linear(layer[0], layer[1]) for layer in list(zip(hidden_units[:-1], hidden_units[1:]))])\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for linear in self.dnn_network:\n",
    "            x = linear(x)\n",
    "            x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "class DCN(nn.Module):\n",
    "    def __init__(self, feature_columns, hidden_units, layer_num, dnn_dropout=0.):\n",
    "        super(DCN, self).__init__()\n",
    "        self.dense_feature_cols, self.sparse_feature_cols = feature_columns\n",
    "        self.embed_layers = nn.ModuleDict({\n",
    "            'embed_' + str(i): nn.Embedding(num_embeddings=feat['feat_num'], embedding_dim=feat['embed_dim'])\n",
    "            for i, feat in enumerate(self.sparse_feature_cols)\n",
    "        })\n",
    "        \n",
    "        hidden_units.insert(0, len(self.dense_feature_cols) + len(self.sparse_feature_cols)*self.sparse_feature_cols[0]['embed_dim'])\n",
    "        self.dnn_network = Dnn(hidden_units, dnn_dropout)\n",
    "        # 这上面所有操作与wide&deep相同\n",
    "        \n",
    "        self.cross_network = CrossNetwork(layer_num, hidden_units[0])  # layer_num是交叉网络的层数， hidden_units[0]表示输入的整体维度大小\n",
    "        self.final_linear = nn.Linear(hidden_units[-1]+hidden_units[0], 1) # 这里的输入维度设置维Cross和Deep两部分输出维度之和（Cross部分网络层不改变维度）\n",
    "    \n",
    "    def forward(self, x):\n",
    "        dense_input, sparse_inputs = x[:, :len(self.dense_feature_cols)], x[:, len(self.dense_feature_cols):]\n",
    "        sparse_inputs = sparse_inputs.long()\n",
    "        sparse_embeds = [self.embed_layers['embed_'+str(i)](sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])]\n",
    "        sparse_embeds = torch.cat(sparse_embeds, axis=-1)\n",
    "        x = torch.cat([sparse_embeds, dense_input], axis=-1)\n",
    "        # 此函数中，这上面所有操作与wide&deep中相同\n",
    "        \n",
    "        # cross Network\n",
    "        cross_out = self.cross_network(x)\n",
    "        # Deep Network\n",
    "        deep_out = self.dnn_network(x)\n",
    "        #  Concatenate\n",
    "        total_x = torch.cat([cross_out, deep_out], axis=-1) # 将Cross和DNN部分得到的结果拼接在一起\n",
    "        # out\n",
    "        outputs = F.sigmoid(self.final_linear(total_x)) # 拼接后的结果转为1维然后接sigmoid函数输出\n",
    "        \n",
    "        return outputs  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e160a3",
   "metadata": {},
   "source": [
    "### DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93b086f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(nn.Module): # FM部分\n",
    "    def __init__(self, latent_dim, fea_num):\n",
    "        \"\"\"\n",
    "        latent_dim: 各个离散特征隐向量的维度\n",
    "        input_shape: 这个最后离散特征embedding之后的拼接和dense拼接的总特征个数\n",
    "        \"\"\"\n",
    "        super(FM, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        # 初始化三个参数矩阵，注意这里的参数由于是可学习参数，需要用nn.Parameter进行定义\n",
    "        self.w0 = nn.Parameter(torch.zeros([1,])) # 全局偏置矩阵，就是FM里面的偏移项\n",
    "        self.w1 = nn.Parameter(torch.rand([fea_num, 1])) # 一阶参数矩阵，就是<w, x>里面的w\n",
    "        self.w2 = nn.Parameter(torch.rand([fea_num, latent_dim])) # 隐向量矩阵，每行代表一个特征的隐向量\n",
    "        \n",
    "    def forward(self, inputs):   \n",
    "        first_order = self.w0 + torch.mm(inputs, self.w1) # 一阶交叉，就是计算偏移项＋一阶特征，即：w0 + <w1, x>    \n",
    "        second_order = 1/2 * torch.sum( # FM的二阶交叉项求和，这个用的是FM的最终化简公式\n",
    "            torch.pow(torch.mm(inputs, self.w2), 2) - torch.mm(torch.pow(inputs,2), torch.pow(self.w2, 2)),\n",
    "            dim = 1,\n",
    "            keepdim = True\n",
    "        )         \n",
    "        \n",
    "        return first_order + second_order # 所有项相加得到输出\n",
    "\n",
    "class Dnn(nn.Module): # Deep部分，老生常谈，不解释\n",
    "    def __init__(self, hidden_units, dropout=0.):\n",
    "        super(Dnn, self).__init__()\n",
    "        self.dnn_network = nn.ModuleList([nn.Linear(layer[0], layer[1]) for layer in list(zip(hidden_units[:-1], hidden_units[1:]))])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):  \n",
    "        for linear in self.dnn_network:\n",
    "            x = linear(x)\n",
    "            x = F.relu(x)    \n",
    "        x = self.dropout(x) \n",
    "        return x\n",
    "\n",
    "class DeepFM(nn.Module): # DeepFM主架构\n",
    "    def __init__(self, feature_columns, hidden_units, dnn_dropout=0.):\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.dense_feature_cols, self.sparse_feature_cols = feature_columns\n",
    "        self.embed_layers = nn.ModuleDict({\n",
    "            'embed_' + str(i): nn.Embedding(num_embeddings=feat['feat_num'], embedding_dim=feat['embed_dim'])\n",
    "            for i, feat in enumerate(self.sparse_feature_cols)\n",
    "        })\n",
    "        self.fea_num = len(self.dense_feature_cols) + len(self.sparse_feature_cols)*self.sparse_feature_cols[0]['embed_dim']\n",
    "        hidden_units.insert(0, self.fea_num)\n",
    "        # 上面这些依然参照wide&deep的注解\n",
    "        \n",
    "        self.fm = FM(self.sparse_feature_cols[0]['embed_dim'], self.fea_num) # 定义FM模块\n",
    "        self.dnn_network = Dnn(hidden_units, dnn_dropout) # 定义DNN模块\n",
    "        self.nn_final_linear = nn.Linear(hidden_units[-1], 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        dense_inputs, sparse_inputs = x[:, :len(self.dense_feature_cols)], x[:, len(self.dense_feature_cols):]\n",
    "        sparse_inputs = sparse_inputs.long()\n",
    "        sparse_embeds = [self.embed_layers['embed_'+str(i)](sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])]          \n",
    "        sparse_embeds = torch.cat(sparse_embeds, dim=-1)\n",
    "        x = torch.cat([sparse_embeds, dense_inputs], dim=-1)\n",
    "        # 上面部分注解参见wide&deep\n",
    "        \n",
    "        # FM\n",
    "        fm_outputs = self.fm(x)\n",
    "        # deep\n",
    "        deep_outputs = self.nn_final_linear(self.dnn_network(x))\n",
    "        # 模型的最后输出\n",
    "        outputs = F.sigmoid(torch.add(fm_outputs, deep_outputs))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "919c2227",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-20845b1b1faf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mFM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfea_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatent_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlatent_num\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class FM(nn.Module):\n",
    "    def __init__(self, fea_num, latent_num):\n",
    "        super(FM, self).__init__()\n",
    "        self.w0 = nn.Parameter(torch.rand(1, ))\n",
    "        self.w1 = nn.Parameter(torch.rand(fea_num, 1))\n",
    "        self.w2 = nn.Parameter(torch.rand(fea_num, latent_num))\n",
    "    def forward(self, x):\n",
    "        first_order = self.w0 + torch.mm(x, self.w1)\n",
    "        second_order = 1/2 * torch.sum(torch.pow(torch.mm(x, self.w2), 2) - torch.mm(torch.pow(x, 2), torch.pow(self.w2, 2)), dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501303ce",
   "metadata": {},
   "source": [
    "### NFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fca8752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c3bc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e381f416",
   "metadata": {},
   "source": [
    "### DIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425deea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9c204d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e163ea5",
   "metadata": {},
   "source": [
    "### DIEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfdee75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
