{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动手深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 6, 7]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义张量（数组）\n",
    "x = torch.tensor([[1,2,3], [4,6,7]], dtype = torch.float32) # 相当于np.array()，极其相似，但是类型不同\n",
    "x = torch.arange(12) # 跟np.arange()效果相同\n",
    "torch.zeros(3,4) # 定义指定形状的数组，类似的numpy操作都可实现\n",
    "# tensor的四则运算规则同数组的也是相同的性质\n",
    "# 0维度张量，专门0维度张量的原因是：GPU中不能运行普通的1等标量，只能识别tensor(1)这种0维度张量\n",
    "torch.tensor([1]) # 1维张量\n",
    "torch.tensor(1) # 0维张量，也就是标量\n",
    "# 高维张量：类似于高维numpy数组，不做具体介绍\n",
    "\n",
    "\n",
    "# 查看张量信息\n",
    "x.ndim # 返回维度数（有几个维度）\n",
    "x.shape # 返回形状（每个层维度的个数组合）\n",
    "len(x) # 返回张量多个维度中最高层级维度的个数\n",
    "x.numel() # 最底层元素总数量\n",
    "x.dtype # 查看张量数据类型\n",
    "\n",
    "\n",
    "# 张量的形变\n",
    "x = torch.tensor([[1,2,3,4],[5,6,7,8],[9,9,9,9]])\n",
    "x.flatten() # 将任意维度张量拉伸为1维张量，包括0维张量\n",
    "x.reshape(3, 4) # 改变张量为指定形状，跟np.reshape相同\n",
    "x.reshape(12) # 变为1维张量，或者写作x.reshape(12,)，但是x.shape(12,1)是二维张量\n",
    "# reshape的参数输入对应于.shape的结果，他们的意思是一样的\n",
    "\n",
    "\n",
    "# 张量的索引\n",
    "# 1. 与数组的索引相同，但要注意的是，索引找到的最基本的元素都是0维张量，不是基本的标量，比如索引结果是tensor(1),而不是1\n",
    "# 2. 切片操作基本与numpy数组的索引相同，但有点区别，如下举例\n",
    "x[1:8:2] # 从第2个元素到第8个元素隔2步索引一次\n",
    "x[1::2] # 从第2个到尾，隔2步索引一次\n",
    "x[::2] # 从头到尾，balabala\n",
    "# 3. x[8:1:-2] 这种操作不行，torch不支持倒着索引\n",
    "# 4. 高维张量索引，与numpy的也没什么\n",
    "# 5. index_select(,,)索引：本质上任何索引都可以x[,,,...]解决，但是当维度很高的时候，有些操作会很麻烦。\n",
    "# 比如我想对一个有100个维度的张量做操作：取出第50个维中的前两行，正常写法是x[:,:,:...,[0,1],:,:,...],过于麻烦，但使用index_select就很方便\n",
    "# torch.index_select用于取出某一个特定维度的某几个列数据\n",
    "x = torch.arange(24).reshape(2,3,4)\n",
    "torch.index_select(x, 2, torch.tensor(0)) # 用法：x是目标张量，第二个参数是第几个维度，第三个参数是索引值，可以是多个，但必须是tensor型，不能是列表或数组型\n",
    "torch.index_select(x, 1, torch.tensor([0])) # 只索引一个单位时，可以用0维或者1维张量\n",
    "torch.index_select(x, 0, torch.tensor([0,1])) # 索引多个单位\n",
    "# 注：torch.tensor([0:1])是不对的！，不能写成切片，第三个参数本质是一个tensor类型的数据！\n",
    "\n",
    "\n",
    "# 张量的切分\n",
    "# 1. view()方法，张量的视图\n",
    "x = torch.arange(12).reshape(3,4)\n",
    "y = x.view(4,3) # 此处view的作用类似于reshape。看上去y和x是不同的对象，但其实指向同一个存储空间，x和y可以看做一个数据的不同表现形式，改变x后Y也会改变，但仍然是不同形状\n",
    "\n",
    "# 2. torch.chunk(t, 4, dim = 0)方法：将张量进行切分的函数。t是要被切分的张量，4是等切分成4份，dim是从哪个维度上切，0是默认最高维，以此类推\n",
    "x = torch.arange(12).reshape(3,4)\n",
    "y = torch.chunk(x, 4, dim = 1) # y是将x按列切分四等份的结果\n",
    "# 注：关于输出结果：是一个元组，其元素是切分后的各个单位；切分后的个体也是tensor对象；切分后每个个体仍然保持没切分的维度，不会降维；\n",
    "# 返回的结果也是视图，不是新对象；默认是等分，但如果不能等分也不会报错，会寻找近似等分，或者不等分\n",
    "\n",
    "# 3. torch.split(t, [a1,a2,a3...], dim = ) 与chunk相似，但有区别，第二个参数可以输入一个列表，表示自定义切分分配方案，要求总和等于对应维度的分量个数\n",
    "x = torch.arange(12).reshape(3,4)\n",
    "y = torch.split(x, 4, dim = 1) # 此时的split与chunk效果相同\n",
    "z = torch.split(x, [1,1,2], dim = 1) # 按照列进行1、1、2份数分配方案的切分\n",
    "\n",
    "\n",
    "# 张量的合并\n",
    "# 1. torch.cat([x,y], dim = 1)，张量拼接操作，类似于numpy的vstack和hstack，以及pandas的concat；dim是选择按哪个维度拼接，默认是0\n",
    "x = torch.zeros((4,4))\n",
    "y = torch.zeros((4,4))\n",
    "torch.cat([x,y], dim = 1) # 按列拼接（横着拼）\n",
    "\n",
    "# 2. torch.stack([x,y]) 张量堆叠函数，并不是拼接，而是把相同形状的多个张量堆到一起，整体升高一个维度（类似于list的append）\n",
    "# 注：x和y的shape必须完全相同才行，否则会报错\n",
    "\n",
    "\n",
    "# 张量的广播\n",
    "# 1. 相同维度相同形状的两个张量广播，对应位置进行计算即可\n",
    "# 2. 相同维度不同形状的两个张量广播，要求在导致形状不同的每个维度上，两个张量中至少有一个在该维度的分量数只有1才行\n",
    "# 3. 不同维度的两个张量广播，低纬度的张量先升维到与高纬度张量相同维度，然后再按照同纬度张量广播规则判定即可\n",
    "# 不同维度广播运算举例：x.shape = (2,1)， y.shape = (3,2,4)。x根据情况选择升维成(1,2,1)，然后(1,2,1)与(3,2,4)是课广播的\n",
    "\n",
    "\n",
    "# 基本并行运算（axis与dim都可以使用，效果相同)\n",
    "x = torch.arange(12).reshape(3,4)\n",
    "x.sum() # 对所有元素求和\n",
    "x.sum(dim = 0) # 对所有行求和，返回长度为3的向量\n",
    "x.sum(dim = 1) # 对所有列求和，返回长度为4的向量\n",
    "x,sum(dim = [0, 1]) # 对于多维数组，对其中某几维进行求和，这里只有2维所以结果跟sum()一样\n",
    "# 排序运算\n",
    "x = torch.randn(12).reshape(3,4) \n",
    "y = x.sort(dim = 1, descending = False).values # 将x按照某个维度升序（默认）排序，这个维度的每个分量都会独立进行排序（打乱对应关系），默认是0；返回的内容比较复杂，.values后是值\n",
    "\n",
    "\n",
    "\n",
    "# 矩阵的基本运算\n",
    "t1 = torch.arange(1,7).reshape(2,3).float()\n",
    "t2 = torch.arange(1,10).reshape(3,3).float()\n",
    "t = torch.arange(3).float()\n",
    "torch.t(t1) # 矩阵转置 \n",
    "torch.eye(3) # 单位阵创建\n",
    "torch.diag(t1.reshape(6)) # 将1维张量变成对角阵元素\n",
    "torch.dot(t,t) # 向量内积运算，dot只支持向量运算\n",
    "torch.mm(t1,t2) # 矩阵乘法，t1 * t2是对应元素相乘\n",
    "torch.mv(t1,t) ### 注意：特殊的矩阵与向量相乘的方法，并非是矩阵运算，而是把矩阵的每行当成一个向量分别与某个向量进行内积，返回各自内积结果组成的1维向量\n",
    "torch.mv(t2, t.reshape(3,1)) # 会报错，不能竖着来\n",
    "torch.mm(t1, t.reshape(3,1)) # 作为矩阵运算可以\n",
    "\n",
    "# 矩阵线性代数运算（不举例了，懒）\n",
    "torch.trace() # 矩阵的迹\n",
    "torch.matrix_rank() # 矩阵的秩\n",
    "torch.det() # 矩阵行列式计算\n",
    "torch.inverse() # 矩阵求逆\n",
    "torch.lstsq(y, X) # 最小二乘结果，y是因变量，X是带1列向量的变量矩阵\n",
    "torch.linalg.norm(t) # 求张量内所有元素平方和开根号（必须是float类型）\n",
    "torch.linalg.norm(t,1) # 求张量内所有元素绝对值和（必须是float类型）\n",
    "\n",
    "\n",
    "\n",
    "# 微分运算\n",
    "a = torch.tensor(1., requires_grad = True) # 创建一个张量的时候可以设置这个张量是否可以进行微分运算，默认是False（必须针对浮点型）\n",
    "b = torch.tensor(1., requires_grad = True)\n",
    "a.requires_grad # 用于查看这个张量是否可以微分\n",
    "# a.requires_grad = False # 可以设置为False\n",
    "sse = torch.pow((2-a-b),2) + torch.pow((4-3*a-b),2)\n",
    "torch.autograd.grad(sse, [a, b]) # 这个是对sse这个函数进行偏导运算，a和b就是变量，但是要先赋值，然后返回的是sse在a和b两个方向的导数在他们各自取值下的导数值\n",
    "\n",
    "x = torch.tensor(1., requires_grad = True)\n",
    "y = x ** 2\n",
    "y.retain_grad() # 加上这个后，z对x进行求导时候也能顺带保存z对y求导的中间结果，没有这句，不能y.grad\n",
    "z = y ** 2\n",
    "y.requires_grad # True\n",
    "z.requires_grad # True\n",
    "z.backward() # 反向传播，就是z对最底层的x求导，结果再代入x的值的最终结果，这个本身不返回什么\n",
    "x.grad # 紧接着可以这样来查看刚才计算的结果，这个是用来看x的求导结果的\n",
    "y.grad # 作为中间变量的y，在z对x求导的过程中，只有在y.retain_grad()被设置才可以\n",
    "# y.backward() # y也是可以对x求梯度的\n",
    "# 切记，一个计算图只能backward一次，比如z.backward()之后，就不能再y.backward()了，如下：\n",
    "x = torch.arange(4., requires_grad = True)\n",
    "y = torch.dot(x, x)\n",
    "z = y ** 2 # xyz属于一个计算图\n",
    "y.backward() # y先进行梯度下降\n",
    "print(x.grad) \n",
    "z.backward() # 这时候会报错，因为这个计算图已经计算过梯度了\n",
    "\n",
    "# 改成下面这样可避免问题\n",
    "x = torch.arange(4., requires_grad = True)\n",
    "y = torch.dot(x, x)\n",
    "z = x.sum() # xy和xz属于两个不同的计算图\n",
    "y.backward() # y先进行梯度下降\n",
    "print(x.grad) \n",
    "z.backward() # 正常计算\n",
    "\n",
    "# 继续引出手动梯度清零：上面结果中的第二个x.grad的输出值不是z对x求导代入后的值，而是这个值加上了之前y对x求导得到的值，x梯度会默认累加\n",
    "x.grad.zero_() # 将过往累加的梯度清零，重新开始"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 学习实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12288])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "# 例子1\n",
    "m = nn.Linear(20, 30) # 规定输入向量大小为20，输出是30\n",
    "input = torch.randn(1,20) # 定义一个二维张量，第二个参数的值必须是20，这代表输入维度，第一个参数是样本数，可随意调整大小，但是必须两个维度都有\n",
    "output = m(input) # 调用全连接层\n",
    "output.size()\n",
    "\n",
    "# 例子2\n",
    "connected_layer = nn.Linear(in_features = 64*64*3, out_features = 1) # in_features由输入张量的形状决定，out_features则决定了输出张量的形状 \n",
    "input = t.randn(64,64,3) # 假定输入的图像形状为[64,64,3]\n",
    "input = input.view(1, 64*64*3) # 将四维张量转换为二维张量之后，才能作为全连接层的输入；前面的1可认为是样本数，不能缺少\n",
    "print(input.shape)\n",
    "output = connected_layer(input) # 调用全连接层\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN经典网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0510,  0.1954,  0.2974, -0.0540, -0.4502,  0.2356, -0.1149,  0.0008,\n",
       "         -0.1557,  0.3028],\n",
       "        [ 0.0510,  0.1953,  0.2975, -0.0539, -0.4503,  0.2357, -0.1148,  0.0008,\n",
       "         -0.1557,  0.3030],\n",
       "        [ 0.0510,  0.1953,  0.2973, -0.0539, -0.4502,  0.2356, -0.1147,  0.0008,\n",
       "         -0.1558,  0.3029],\n",
       "        [ 0.0510,  0.1953,  0.2974, -0.0539, -0.4503,  0.2357, -0.1148,  0.0008,\n",
       "         -0.1558,  0.3029],\n",
       "        [ 0.0510,  0.1954,  0.2975, -0.0540, -0.4502,  0.2357, -0.1148,  0.0008,\n",
       "         -0.1558,  0.3028]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import d2lzh_pytorch as d2l\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size = 5, stride = 1, padding = 2), nn.Sigmoid()\n",
    "            ,nn.AvgPool2d(kernel_size = 2, stride = 2)\n",
    "            ,nn.Conv2d(6, 16, kernel_size = 5), nn.Sigmoid()\n",
    "            ,nn.AvgPool2d(kernel_size = 2, stride = 2), nn.Flatten()\n",
    "            ,nn.Linear(16*5*5, 120), nn.Sigmoid()\n",
    "            ,nn.Linear(120, 84), nn.Sigmoid()\n",
    "            ,nn.Linear(84, 10))\n",
    "    def forward(self, x):\n",
    "        return self.net(x.view(-1, 1, 28, 28)) # -1表示由其他维度计算后得出\n",
    "x = torch.randn(5, 1, 28, 28) # 创建数据\n",
    "lenet = LeNet()\n",
    "lenet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 28, 28])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5, 1, 28, 28)\n",
    "x.view(-1, 1, 28, 28).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 1  loss: 0.67483\n",
      "echo: 2  loss: 0.67027\n",
      "echo: 3  loss: 0.66555\n",
      "echo: 4  loss: 0.66068\n",
      "echo: 5  loss: 0.65568\n",
      "echo: 6  loss: 0.65056\n",
      "echo: 7  loss: 0.64534\n",
      "echo: 8  loss: 0.64004\n",
      "echo: 9  loss: 0.63467\n",
      "echo: 10  loss: 0.62923\n",
      "echo: 11  loss: 0.62375\n",
      "echo: 12  loss: 0.61822\n",
      "echo: 13  loss: 0.61268\n",
      "echo: 14  loss: 0.60712\n",
      "echo: 15  loss: 0.60158\n",
      "echo: 16  loss: 0.59608\n",
      "echo: 17  loss: 0.59063\n",
      "echo: 18  loss: 0.58526\n",
      "echo: 19  loss: 0.57998\n",
      "echo: 20  loss: 0.57483\n",
      "echo: 21  loss: 0.5698\n",
      "echo: 22  loss: 0.56492\n",
      "echo: 23  loss: 0.56019\n",
      "echo: 24  loss: 0.55562\n",
      "echo: 25  loss: 0.55123\n",
      "echo: 26  loss: 0.54701\n",
      "echo: 27  loss: 0.54297\n",
      "echo: 28  loss: 0.53911\n",
      "echo: 29  loss: 0.53544\n",
      "echo: 30  loss: 0.53194\n",
      "echo: 31  loss: 0.52863\n",
      "echo: 32  loss: 0.52549\n",
      "echo: 33  loss: 0.52251\n",
      "echo: 34  loss: 0.5197\n",
      "echo: 35  loss: 0.51704\n",
      "echo: 36  loss: 0.51452\n",
      "echo: 37  loss: 0.51214\n",
      "echo: 38  loss: 0.50989\n",
      "echo: 39  loss: 0.50776\n",
      "echo: 40  loss: 0.50574\n",
      "echo: 41  loss: 0.50383\n",
      "echo: 42  loss: 0.50201\n",
      "echo: 43  loss: 0.50028\n",
      "echo: 44  loss: 0.49864\n",
      "echo: 45  loss: 0.49706\n",
      "echo: 46  loss: 0.49556\n",
      "echo: 47  loss: 0.49413\n",
      "echo: 48  loss: 0.49275\n",
      "echo: 49  loss: 0.49142\n",
      "echo: 50  loss: 0.49015\n",
      "echo: 51  loss: 0.48892\n",
      "echo: 52  loss: 0.48773\n",
      "echo: 53  loss: 0.48658\n",
      "echo: 54  loss: 0.48547\n",
      "echo: 55  loss: 0.48439\n",
      "echo: 56  loss: 0.48335\n",
      "echo: 57  loss: 0.48234\n",
      "echo: 58  loss: 0.48135\n",
      "echo: 59  loss: 0.48039\n",
      "echo: 60  loss: 0.47946\n",
      "echo: 61  loss: 0.47855\n",
      "echo: 62  loss: 0.47767\n",
      "echo: 63  loss: 0.47681\n",
      "echo: 64  loss: 0.47597\n",
      "echo: 65  loss: 0.47516\n",
      "echo: 66  loss: 0.47437\n",
      "echo: 67  loss: 0.4736\n",
      "echo: 68  loss: 0.47285\n",
      "echo: 69  loss: 0.47212\n",
      "echo: 70  loss: 0.47141\n",
      "echo: 71  loss: 0.47072\n",
      "echo: 72  loss: 0.47005\n",
      "echo: 73  loss: 0.4694\n",
      "echo: 74  loss: 0.46877\n",
      "echo: 75  loss: 0.46816\n",
      "echo: 76  loss: 0.46757\n",
      "echo: 77  loss: 0.467\n",
      "echo: 78  loss: 0.46644\n",
      "echo: 79  loss: 0.4659\n",
      "echo: 80  loss: 0.46538\n",
      "echo: 81  loss: 0.46488\n",
      "echo: 82  loss: 0.46439\n",
      "echo: 83  loss: 0.46392\n",
      "echo: 84  loss: 0.46346\n",
      "echo: 85  loss: 0.46302\n",
      "echo: 86  loss: 0.46259\n",
      "echo: 87  loss: 0.46218\n",
      "echo: 88  loss: 0.46178\n",
      "echo: 89  loss: 0.46139\n",
      "echo: 90  loss: 0.46102\n",
      "echo: 91  loss: 0.46065\n",
      "echo: 92  loss: 0.4603\n",
      "echo: 93  loss: 0.45996\n",
      "echo: 94  loss: 0.45963\n",
      "echo: 95  loss: 0.45931\n",
      "echo: 96  loss: 0.459\n",
      "echo: 97  loss: 0.4587\n",
      "echo: 98  loss: 0.4584\n",
      "echo: 99  loss: 0.45812\n",
      "echo: 100  loss: 0.45784\n",
      "echo: 101  loss: 0.45757\n",
      "echo: 102  loss: 0.45731\n",
      "echo: 103  loss: 0.45706\n",
      "echo: 104  loss: 0.45681\n",
      "echo: 105  loss: 0.45657\n",
      "echo: 106  loss: 0.45634\n",
      "echo: 107  loss: 0.45611\n",
      "echo: 108  loss: 0.45588\n",
      "echo: 109  loss: 0.45567\n",
      "echo: 110  loss: 0.45546\n",
      "echo: 111  loss: 0.45525\n",
      "echo: 112  loss: 0.45505\n",
      "echo: 113  loss: 0.45485\n",
      "echo: 114  loss: 0.45466\n",
      "echo: 115  loss: 0.45448\n",
      "echo: 116  loss: 0.4543\n",
      "echo: 117  loss: 0.45412\n",
      "echo: 118  loss: 0.45395\n",
      "echo: 119  loss: 0.45378\n",
      "echo: 120  loss: 0.45361\n",
      "echo: 121  loss: 0.45345\n",
      "echo: 122  loss: 0.4533\n",
      "echo: 123  loss: 0.45314\n",
      "echo: 124  loss: 0.45299\n",
      "echo: 125  loss: 0.45285\n",
      "echo: 126  loss: 0.4527\n",
      "echo: 127  loss: 0.45256\n",
      "echo: 128  loss: 0.45243\n",
      "echo: 129  loss: 0.45229\n",
      "echo: 130  loss: 0.45216\n",
      "echo: 131  loss: 0.45203\n",
      "echo: 132  loss: 0.45191\n",
      "echo: 133  loss: 0.45179\n",
      "echo: 134  loss: 0.45167\n",
      "echo: 135  loss: 0.45155\n",
      "echo: 136  loss: 0.45144\n",
      "echo: 137  loss: 0.45133\n",
      "echo: 138  loss: 0.45122\n",
      "echo: 139  loss: 0.45111\n",
      "echo: 140  loss: 0.451\n",
      "echo: 141  loss: 0.4509\n",
      "echo: 142  loss: 0.4508\n",
      "echo: 143  loss: 0.4507\n",
      "echo: 144  loss: 0.4506\n",
      "echo: 145  loss: 0.45051\n",
      "echo: 146  loss: 0.45042\n",
      "echo: 147  loss: 0.45033\n",
      "echo: 148  loss: 0.45024\n",
      "echo: 149  loss: 0.45015\n",
      "echo: 150  loss: 0.45006\n",
      "echo: 151  loss: 0.44998\n",
      "echo: 152  loss: 0.4499\n",
      "echo: 153  loss: 0.44982\n",
      "echo: 154  loss: 0.44974\n",
      "echo: 155  loss: 0.44966\n",
      "echo: 156  loss: 0.44958\n",
      "echo: 157  loss: 0.44951\n",
      "echo: 158  loss: 0.44943\n",
      "echo: 159  loss: 0.44936\n",
      "echo: 160  loss: 0.44929\n",
      "echo: 161  loss: 0.44922\n",
      "echo: 162  loss: 0.44915\n",
      "echo: 163  loss: 0.44909\n",
      "echo: 164  loss: 0.44902\n",
      "echo: 165  loss: 0.44896\n",
      "echo: 166  loss: 0.44889\n",
      "echo: 167  loss: 0.44883\n",
      "echo: 168  loss: 0.44877\n",
      "echo: 169  loss: 0.44871\n",
      "echo: 170  loss: 0.44865\n",
      "echo: 171  loss: 0.44859\n",
      "echo: 172  loss: 0.44854\n",
      "echo: 173  loss: 0.44848\n",
      "echo: 174  loss: 0.44843\n",
      "echo: 175  loss: 0.44837\n",
      "echo: 176  loss: 0.44832\n",
      "echo: 177  loss: 0.44827\n",
      "echo: 178  loss: 0.44822\n",
      "echo: 179  loss: 0.44817\n",
      "echo: 180  loss: 0.44812\n",
      "echo: 181  loss: 0.44807\n",
      "echo: 182  loss: 0.44802\n",
      "echo: 183  loss: 0.44797\n",
      "echo: 184  loss: 0.44793\n",
      "echo: 185  loss: 0.44788\n",
      "echo: 186  loss: 0.44784\n",
      "echo: 187  loss: 0.44779\n",
      "echo: 188  loss: 0.44775\n",
      "echo: 189  loss: 0.44771\n",
      "echo: 190  loss: 0.44767\n",
      "echo: 191  loss: 0.44762\n",
      "echo: 192  loss: 0.44758\n",
      "echo: 193  loss: 0.44754\n",
      "echo: 194  loss: 0.4475\n",
      "echo: 195  loss: 0.44747\n",
      "echo: 196  loss: 0.44743\n",
      "echo: 197  loss: 0.44739\n",
      "echo: 198  loss: 0.44735\n",
      "echo: 199  loss: 0.44732\n",
      "echo: 200  loss: 0.44728\n",
      "echo: 201  loss: 0.44725\n",
      "echo: 202  loss: 0.44721\n",
      "echo: 203  loss: 0.44718\n",
      "echo: 204  loss: 0.44715\n",
      "echo: 205  loss: 0.44711\n",
      "echo: 206  loss: 0.44708\n",
      "echo: 207  loss: 0.44705\n",
      "echo: 208  loss: 0.44702\n",
      "echo: 209  loss: 0.44699\n",
      "echo: 210  loss: 0.44696\n",
      "echo: 211  loss: 0.44693\n",
      "echo: 212  loss: 0.4469\n",
      "echo: 213  loss: 0.44687\n",
      "echo: 214  loss: 0.44684\n",
      "echo: 215  loss: 0.44681\n",
      "echo: 216  loss: 0.44678\n",
      "echo: 217  loss: 0.44675\n",
      "echo: 218  loss: 0.44673\n",
      "echo: 219  loss: 0.4467\n",
      "echo: 220  loss: 0.44667\n",
      "echo: 221  loss: 0.44665\n",
      "echo: 222  loss: 0.44662\n",
      "echo: 223  loss: 0.4466\n",
      "echo: 224  loss: 0.44657\n",
      "echo: 225  loss: 0.44655\n",
      "echo: 226  loss: 0.44653\n",
      "echo: 227  loss: 0.4465\n",
      "echo: 228  loss: 0.44648\n",
      "echo: 229  loss: 0.44646\n",
      "echo: 230  loss: 0.44643\n",
      "echo: 231  loss: 0.44641\n",
      "echo: 232  loss: 0.44639\n",
      "echo: 233  loss: 0.44637\n",
      "echo: 234  loss: 0.44635\n",
      "echo: 235  loss: 0.44632\n",
      "echo: 236  loss: 0.4463\n",
      "echo: 237  loss: 0.44628\n",
      "echo: 238  loss: 0.44626\n",
      "echo: 239  loss: 0.44624\n",
      "echo: 240  loss: 0.44622\n",
      "echo: 241  loss: 0.4462\n",
      "echo: 242  loss: 0.44619\n",
      "echo: 243  loss: 0.44617\n",
      "echo: 244  loss: 0.44615\n",
      "echo: 245  loss: 0.44613\n",
      "echo: 246  loss: 0.44611\n",
      "echo: 247  loss: 0.44609\n",
      "echo: 248  loss: 0.44608\n",
      "echo: 249  loss: 0.44606\n",
      "echo: 250  loss: 0.44604\n",
      "echo: 251  loss: 0.44602\n",
      "echo: 252  loss: 0.44601\n",
      "echo: 253  loss: 0.44599\n",
      "echo: 254  loss: 0.44597\n",
      "echo: 255  loss: 0.44596\n",
      "echo: 256  loss: 0.44594\n",
      "echo: 257  loss: 0.44593\n",
      "echo: 258  loss: 0.44591\n",
      "echo: 259  loss: 0.4459\n",
      "echo: 260  loss: 0.44588\n",
      "echo: 261  loss: 0.44587\n",
      "echo: 262  loss: 0.44585\n",
      "echo: 263  loss: 0.44584\n",
      "echo: 264  loss: 0.44582\n",
      "echo: 265  loss: 0.44581\n",
      "echo: 266  loss: 0.4458\n",
      "echo: 267  loss: 0.44578\n",
      "echo: 268  loss: 0.44577\n",
      "echo: 269  loss: 0.44575\n",
      "echo: 270  loss: 0.44574\n",
      "echo: 271  loss: 0.44573\n",
      "echo: 272  loss: 0.44572\n",
      "echo: 273  loss: 0.4457\n",
      "echo: 274  loss: 0.44569\n",
      "echo: 275  loss: 0.44568\n",
      "echo: 276  loss: 0.44566\n",
      "echo: 277  loss: 0.44565\n",
      "echo: 278  loss: 0.44564\n",
      "echo: 279  loss: 0.44563\n",
      "echo: 280  loss: 0.44562\n",
      "echo: 281  loss: 0.4456\n",
      "echo: 282  loss: 0.44559\n",
      "echo: 283  loss: 0.44558\n",
      "echo: 284  loss: 0.44557\n",
      "echo: 285  loss: 0.44556\n",
      "echo: 286  loss: 0.44555\n",
      "echo: 287  loss: 0.44554\n",
      "echo: 288  loss: 0.44553\n",
      "echo: 289  loss: 0.44552\n",
      "echo: 290  loss: 0.44551\n",
      "echo: 291  loss: 0.44549\n",
      "echo: 292  loss: 0.44548\n",
      "echo: 293  loss: 0.44547\n",
      "echo: 294  loss: 0.44546\n",
      "echo: 295  loss: 0.44545\n",
      "echo: 296  loss: 0.44544\n",
      "echo: 297  loss: 0.44543\n",
      "echo: 298  loss: 0.44542\n",
      "echo: 299  loss: 0.44541\n",
      "echo: 300  loss: 0.4454\n",
      "echo: 301  loss: 0.44539\n",
      "echo: 302  loss: 0.44539\n",
      "echo: 303  loss: 0.44538\n",
      "echo: 304  loss: 0.44537\n",
      "echo: 305  loss: 0.44536\n",
      "echo: 306  loss: 0.44535\n",
      "echo: 307  loss: 0.44534\n",
      "echo: 308  loss: 0.44533\n",
      "echo: 309  loss: 0.44532\n",
      "echo: 310  loss: 0.44531\n",
      "echo: 311  loss: 0.4453\n",
      "echo: 312  loss: 0.44529\n",
      "echo: 313  loss: 0.44529\n",
      "echo: 314  loss: 0.44528\n",
      "echo: 315  loss: 0.44527\n",
      "echo: 316  loss: 0.44526\n",
      "echo: 317  loss: 0.44525\n",
      "echo: 318  loss: 0.44524\n",
      "echo: 319  loss: 0.44523\n",
      "echo: 320  loss: 0.44523\n",
      "echo: 321  loss: 0.44522\n",
      "echo: 322  loss: 0.44521\n",
      "echo: 323  loss: 0.4452\n",
      "echo: 324  loss: 0.44519\n",
      "echo: 325  loss: 0.44518\n",
      "echo: 326  loss: 0.44518\n",
      "echo: 327  loss: 0.44517\n",
      "echo: 328  loss: 0.44516\n",
      "echo: 329  loss: 0.44515\n",
      "echo: 330  loss: 0.44514\n",
      "echo: 331  loss: 0.44513\n",
      "echo: 332  loss: 0.44513\n",
      "echo: 333  loss: 0.44512\n",
      "echo: 334  loss: 0.44511\n",
      "echo: 335  loss: 0.4451\n",
      "echo: 336  loss: 0.44509\n",
      "echo: 337  loss: 0.44508\n",
      "echo: 338  loss: 0.44508\n",
      "echo: 339  loss: 0.44507\n",
      "echo: 340  loss: 0.44506\n",
      "echo: 341  loss: 0.44505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 342  loss: 0.44504\n",
      "echo: 343  loss: 0.44503\n",
      "echo: 344  loss: 0.44503\n",
      "echo: 345  loss: 0.44502\n",
      "echo: 346  loss: 0.44501\n",
      "echo: 347  loss: 0.445\n",
      "echo: 348  loss: 0.44499\n",
      "echo: 349  loss: 0.44498\n",
      "echo: 350  loss: 0.44498\n",
      "echo: 351  loss: 0.44497\n",
      "echo: 352  loss: 0.44496\n",
      "echo: 353  loss: 0.44495\n",
      "echo: 354  loss: 0.44494\n",
      "echo: 355  loss: 0.44493\n",
      "echo: 356  loss: 0.44492\n",
      "echo: 357  loss: 0.44491\n",
      "echo: 358  loss: 0.44491\n",
      "echo: 359  loss: 0.4449\n",
      "echo: 360  loss: 0.44489\n",
      "echo: 361  loss: 0.44488\n",
      "echo: 362  loss: 0.44487\n",
      "echo: 363  loss: 0.44486\n",
      "echo: 364  loss: 0.44485\n",
      "echo: 365  loss: 0.44484\n",
      "echo: 366  loss: 0.44483\n",
      "echo: 367  loss: 0.44482\n",
      "echo: 368  loss: 0.44481\n",
      "echo: 369  loss: 0.4448\n",
      "echo: 370  loss: 0.44479\n",
      "echo: 371  loss: 0.44478\n",
      "echo: 372  loss: 0.44477\n",
      "echo: 373  loss: 0.44476\n",
      "echo: 374  loss: 0.44475\n",
      "echo: 375  loss: 0.44474\n",
      "echo: 376  loss: 0.44473\n",
      "echo: 377  loss: 0.44472\n",
      "echo: 378  loss: 0.44471\n",
      "echo: 379  loss: 0.4447\n",
      "echo: 380  loss: 0.44468\n",
      "echo: 381  loss: 0.44467\n",
      "echo: 382  loss: 0.44466\n",
      "echo: 383  loss: 0.44465\n",
      "echo: 384  loss: 0.44464\n",
      "echo: 385  loss: 0.44463\n",
      "echo: 386  loss: 0.44462\n",
      "echo: 387  loss: 0.4446\n",
      "echo: 388  loss: 0.44459\n",
      "echo: 389  loss: 0.44458\n",
      "echo: 390  loss: 0.44457\n",
      "echo: 391  loss: 0.44455\n",
      "echo: 392  loss: 0.44454\n",
      "echo: 393  loss: 0.44453\n",
      "echo: 394  loss: 0.44451\n",
      "echo: 395  loss: 0.4445\n",
      "echo: 396  loss: 0.44449\n",
      "echo: 397  loss: 0.44447\n",
      "echo: 398  loss: 0.44446\n",
      "echo: 399  loss: 0.44445\n",
      "echo: 400  loss: 0.44443\n",
      "echo: 401  loss: 0.44442\n",
      "echo: 402  loss: 0.4444\n",
      "echo: 403  loss: 0.44439\n",
      "echo: 404  loss: 0.44437\n",
      "echo: 405  loss: 0.44436\n",
      "echo: 406  loss: 0.44434\n",
      "echo: 407  loss: 0.44433\n",
      "echo: 408  loss: 0.44431\n",
      "echo: 409  loss: 0.4443\n",
      "echo: 410  loss: 0.44428\n",
      "echo: 411  loss: 0.44426\n",
      "echo: 412  loss: 0.44425\n",
      "echo: 413  loss: 0.44423\n",
      "echo: 414  loss: 0.44421\n",
      "echo: 415  loss: 0.44419\n",
      "echo: 416  loss: 0.44418\n",
      "echo: 417  loss: 0.44416\n",
      "echo: 418  loss: 0.44414\n",
      "echo: 419  loss: 0.44412\n",
      "echo: 420  loss: 0.4441\n",
      "echo: 421  loss: 0.44408\n",
      "echo: 422  loss: 0.44406\n",
      "echo: 423  loss: 0.44404\n",
      "echo: 424  loss: 0.44402\n",
      "echo: 425  loss: 0.444\n",
      "echo: 426  loss: 0.44398\n",
      "echo: 427  loss: 0.44396\n",
      "echo: 428  loss: 0.44393\n",
      "echo: 429  loss: 0.44391\n",
      "echo: 430  loss: 0.44389\n",
      "echo: 431  loss: 0.44386\n",
      "echo: 432  loss: 0.44384\n",
      "echo: 433  loss: 0.44381\n",
      "echo: 434  loss: 0.44378\n",
      "echo: 435  loss: 0.44376\n",
      "echo: 436  loss: 0.44373\n",
      "echo: 437  loss: 0.4437\n",
      "echo: 438  loss: 0.44367\n",
      "echo: 439  loss: 0.44364\n",
      "echo: 440  loss: 0.44361\n",
      "echo: 441  loss: 0.44358\n",
      "echo: 442  loss: 0.44354\n",
      "echo: 443  loss: 0.44351\n",
      "echo: 444  loss: 0.44347\n",
      "echo: 445  loss: 0.44344\n",
      "echo: 446  loss: 0.4434\n",
      "echo: 447  loss: 0.44336\n",
      "echo: 448  loss: 0.44332\n",
      "echo: 449  loss: 0.44327\n",
      "echo: 450  loss: 0.44323\n",
      "echo: 451  loss: 0.44319\n",
      "echo: 452  loss: 0.44314\n",
      "echo: 453  loss: 0.44309\n",
      "echo: 454  loss: 0.44304\n",
      "echo: 455  loss: 0.44298\n",
      "echo: 456  loss: 0.44293\n",
      "echo: 457  loss: 0.44287\n",
      "echo: 458  loss: 0.44281\n",
      "echo: 459  loss: 0.44275\n",
      "echo: 460  loss: 0.44268\n",
      "echo: 461  loss: 0.44262\n",
      "echo: 462  loss: 0.44254\n",
      "echo: 463  loss: 0.44247\n",
      "echo: 464  loss: 0.44239\n",
      "echo: 465  loss: 0.44231\n",
      "echo: 466  loss: 0.44223\n",
      "echo: 467  loss: 0.44214\n",
      "echo: 468  loss: 0.44205\n",
      "echo: 469  loss: 0.44195\n",
      "echo: 470  loss: 0.44185\n",
      "echo: 471  loss: 0.44175\n",
      "echo: 472  loss: 0.44164\n",
      "echo: 473  loss: 0.44153\n",
      "echo: 474  loss: 0.44141\n",
      "echo: 475  loss: 0.44129\n",
      "echo: 476  loss: 0.44116\n",
      "echo: 477  loss: 0.44103\n",
      "echo: 478  loss: 0.44089\n",
      "echo: 479  loss: 0.44075\n",
      "echo: 480  loss: 0.4406\n",
      "echo: 481  loss: 0.44045\n",
      "echo: 482  loss: 0.44029\n",
      "echo: 483  loss: 0.44012\n",
      "echo: 484  loss: 0.43995\n",
      "echo: 485  loss: 0.43977\n",
      "echo: 486  loss: 0.43959\n",
      "echo: 487  loss: 0.4394\n",
      "echo: 488  loss: 0.43921\n",
      "echo: 489  loss: 0.439\n",
      "echo: 490  loss: 0.4388\n",
      "echo: 491  loss: 0.43859\n",
      "echo: 492  loss: 0.43837\n",
      "echo: 493  loss: 0.43815\n",
      "echo: 494  loss: 0.43792\n",
      "echo: 495  loss: 0.43768\n",
      "echo: 496  loss: 0.43744\n",
      "echo: 497  loss: 0.43719\n",
      "echo: 498  loss: 0.43694\n",
      "echo: 499  loss: 0.43668\n",
      "echo: 500  loss: 0.43642\n",
      "echo: 501  loss: 0.43615\n",
      "echo: 502  loss: 0.43588\n",
      "echo: 503  loss: 0.4356\n",
      "echo: 504  loss: 0.43532\n",
      "echo: 505  loss: 0.43503\n",
      "echo: 506  loss: 0.43473\n",
      "echo: 507  loss: 0.43443\n",
      "echo: 508  loss: 0.43412\n",
      "echo: 509  loss: 0.43381\n",
      "echo: 510  loss: 0.43349\n",
      "echo: 511  loss: 0.43317\n",
      "echo: 512  loss: 0.43284\n",
      "echo: 513  loss: 0.43251\n",
      "echo: 514  loss: 0.43217\n",
      "echo: 515  loss: 0.43182\n",
      "echo: 516  loss: 0.43147\n",
      "echo: 517  loss: 0.43111\n",
      "echo: 518  loss: 0.43075\n",
      "echo: 519  loss: 0.43038\n",
      "echo: 520  loss: 0.43\n",
      "echo: 521  loss: 0.42962\n",
      "echo: 522  loss: 0.42923\n",
      "echo: 523  loss: 0.42884\n",
      "echo: 524  loss: 0.42844\n",
      "echo: 525  loss: 0.42803\n",
      "echo: 526  loss: 0.42761\n",
      "echo: 527  loss: 0.42719\n",
      "echo: 528  loss: 0.42676\n",
      "echo: 529  loss: 0.42632\n",
      "echo: 530  loss: 0.42588\n",
      "echo: 531  loss: 0.42543\n",
      "echo: 532  loss: 0.42497\n",
      "echo: 533  loss: 0.4245\n",
      "echo: 534  loss: 0.42403\n",
      "echo: 535  loss: 0.42355\n",
      "echo: 536  loss: 0.42306\n",
      "echo: 537  loss: 0.42257\n",
      "echo: 538  loss: 0.42207\n",
      "echo: 539  loss: 0.42156\n",
      "echo: 540  loss: 0.42104\n",
      "echo: 541  loss: 0.42052\n",
      "echo: 542  loss: 0.42\n",
      "echo: 543  loss: 0.41946\n",
      "echo: 544  loss: 0.41892\n",
      "echo: 545  loss: 0.41838\n",
      "echo: 546  loss: 0.41783\n",
      "echo: 547  loss: 0.41728\n",
      "echo: 548  loss: 0.41672\n",
      "echo: 549  loss: 0.41615\n",
      "echo: 550  loss: 0.41558\n",
      "echo: 551  loss: 0.41501\n",
      "echo: 552  loss: 0.41443\n",
      "echo: 553  loss: 0.41385\n",
      "echo: 554  loss: 0.41327\n",
      "echo: 555  loss: 0.41268\n",
      "echo: 556  loss: 0.41209\n",
      "echo: 557  loss: 0.4115\n",
      "echo: 558  loss: 0.41091\n",
      "echo: 559  loss: 0.41031\n",
      "echo: 560  loss: 0.40971\n",
      "echo: 561  loss: 0.40911\n",
      "echo: 562  loss: 0.40851\n",
      "echo: 563  loss: 0.40791\n",
      "echo: 564  loss: 0.40731\n",
      "echo: 565  loss: 0.40671\n",
      "echo: 566  loss: 0.40611\n",
      "echo: 567  loss: 0.40551\n",
      "echo: 568  loss: 0.40491\n",
      "echo: 569  loss: 0.40432\n",
      "echo: 570  loss: 0.40372\n",
      "echo: 571  loss: 0.40312\n",
      "echo: 572  loss: 0.40253\n",
      "echo: 573  loss: 0.40194\n",
      "echo: 574  loss: 0.40135\n",
      "echo: 575  loss: 0.40077\n",
      "echo: 576  loss: 0.40018\n",
      "echo: 577  loss: 0.3996\n",
      "echo: 578  loss: 0.39902\n",
      "echo: 579  loss: 0.39845\n",
      "echo: 580  loss: 0.39788\n",
      "echo: 581  loss: 0.39731\n",
      "echo: 582  loss: 0.39674\n",
      "echo: 583  loss: 0.39618\n",
      "echo: 584  loss: 0.39563\n",
      "echo: 585  loss: 0.39507\n",
      "echo: 586  loss: 0.39452\n",
      "echo: 587  loss: 0.39397\n",
      "echo: 588  loss: 0.39343\n",
      "echo: 589  loss: 0.3929\n",
      "echo: 590  loss: 0.39236\n",
      "echo: 591  loss: 0.39183\n",
      "echo: 592  loss: 0.39131\n",
      "echo: 593  loss: 0.39079\n",
      "echo: 594  loss: 0.39028\n",
      "echo: 595  loss: 0.38977\n",
      "echo: 596  loss: 0.38926\n",
      "echo: 597  loss: 0.38876\n",
      "echo: 598  loss: 0.38827\n",
      "echo: 599  loss: 0.38778\n",
      "echo: 600  loss: 0.3873\n",
      "echo: 601  loss: 0.38682\n",
      "echo: 602  loss: 0.38635\n",
      "echo: 603  loss: 0.38589\n",
      "echo: 604  loss: 0.38543\n",
      "echo: 605  loss: 0.38497\n",
      "echo: 606  loss: 0.38452\n",
      "echo: 607  loss: 0.38408\n",
      "echo: 608  loss: 0.38364\n",
      "echo: 609  loss: 0.38321\n",
      "echo: 610  loss: 0.38278\n",
      "echo: 611  loss: 0.38236\n",
      "echo: 612  loss: 0.38195\n",
      "echo: 613  loss: 0.38154\n",
      "echo: 614  loss: 0.38114\n",
      "echo: 615  loss: 0.38074\n",
      "echo: 616  loss: 0.38035\n",
      "echo: 617  loss: 0.37997\n",
      "echo: 618  loss: 0.37959\n",
      "echo: 619  loss: 0.37921\n",
      "echo: 620  loss: 0.37884\n",
      "echo: 621  loss: 0.37848\n",
      "echo: 622  loss: 0.37812\n",
      "echo: 623  loss: 0.37777\n",
      "echo: 624  loss: 0.37742\n",
      "echo: 625  loss: 0.37708\n",
      "echo: 626  loss: 0.37674\n",
      "echo: 627  loss: 0.37641\n",
      "echo: 628  loss: 0.37608\n",
      "echo: 629  loss: 0.37576\n",
      "echo: 630  loss: 0.37544\n",
      "echo: 631  loss: 0.37512\n",
      "echo: 632  loss: 0.37482\n",
      "echo: 633  loss: 0.37451\n",
      "echo: 634  loss: 0.37421\n",
      "echo: 635  loss: 0.37392\n",
      "echo: 636  loss: 0.37362\n",
      "echo: 637  loss: 0.37334\n",
      "echo: 638  loss: 0.37305\n",
      "echo: 639  loss: 0.37278\n",
      "echo: 640  loss: 0.3725\n",
      "echo: 641  loss: 0.37223\n",
      "echo: 642  loss: 0.37196\n",
      "echo: 643  loss: 0.3717\n",
      "echo: 644  loss: 0.37144\n",
      "echo: 645  loss: 0.37118\n",
      "echo: 646  loss: 0.37093\n",
      "echo: 647  loss: 0.37068\n",
      "echo: 648  loss: 0.37043\n",
      "echo: 649  loss: 0.37019\n",
      "echo: 650  loss: 0.36995\n",
      "echo: 651  loss: 0.36972\n",
      "echo: 652  loss: 0.36948\n",
      "echo: 653  loss: 0.36925\n",
      "echo: 654  loss: 0.36903\n",
      "echo: 655  loss: 0.3688\n",
      "echo: 656  loss: 0.36858\n",
      "echo: 657  loss: 0.36836\n",
      "echo: 658  loss: 0.36815\n",
      "echo: 659  loss: 0.36793\n",
      "echo: 660  loss: 0.36772\n",
      "echo: 661  loss: 0.36752\n",
      "echo: 662  loss: 0.36731\n",
      "echo: 663  loss: 0.36711\n",
      "echo: 664  loss: 0.36691\n",
      "echo: 665  loss: 0.36671\n",
      "echo: 666  loss: 0.36652\n",
      "echo: 667  loss: 0.36633\n",
      "echo: 668  loss: 0.36614\n",
      "echo: 669  loss: 0.36595\n",
      "echo: 670  loss: 0.36576\n",
      "echo: 671  loss: 0.36558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 672  loss: 0.3654\n",
      "echo: 673  loss: 0.36522\n",
      "echo: 674  loss: 0.36504\n",
      "echo: 675  loss: 0.36486\n",
      "echo: 676  loss: 0.36469\n",
      "echo: 677  loss: 0.36452\n",
      "echo: 678  loss: 0.36435\n",
      "echo: 679  loss: 0.36418\n",
      "echo: 680  loss: 0.36402\n",
      "echo: 681  loss: 0.36386\n",
      "echo: 682  loss: 0.36369\n",
      "echo: 683  loss: 0.36353\n",
      "echo: 684  loss: 0.36338\n",
      "echo: 685  loss: 0.36322\n",
      "echo: 686  loss: 0.36306\n",
      "echo: 687  loss: 0.36291\n",
      "echo: 688  loss: 0.36276\n",
      "echo: 689  loss: 0.36261\n",
      "echo: 690  loss: 0.36246\n",
      "echo: 691  loss: 0.36232\n",
      "echo: 692  loss: 0.36217\n",
      "echo: 693  loss: 0.36203\n",
      "echo: 694  loss: 0.36189\n",
      "echo: 695  loss: 0.36175\n",
      "echo: 696  loss: 0.36161\n",
      "echo: 697  loss: 0.36147\n",
      "echo: 698  loss: 0.36133\n",
      "echo: 699  loss: 0.3612\n",
      "echo: 700  loss: 0.36106\n",
      "echo: 701  loss: 0.36093\n",
      "echo: 702  loss: 0.3608\n",
      "echo: 703  loss: 0.36067\n",
      "echo: 704  loss: 0.36054\n",
      "echo: 705  loss: 0.36042\n",
      "echo: 706  loss: 0.36029\n",
      "echo: 707  loss: 0.36017\n",
      "echo: 708  loss: 0.36004\n",
      "echo: 709  loss: 0.35992\n",
      "echo: 710  loss: 0.3598\n",
      "echo: 711  loss: 0.35968\n",
      "echo: 712  loss: 0.35956\n",
      "echo: 713  loss: 0.35945\n",
      "echo: 714  loss: 0.35933\n",
      "echo: 715  loss: 0.35922\n",
      "echo: 716  loss: 0.3591\n",
      "echo: 717  loss: 0.35899\n",
      "echo: 718  loss: 0.35888\n",
      "echo: 719  loss: 0.35877\n",
      "echo: 720  loss: 0.35866\n",
      "echo: 721  loss: 0.35855\n",
      "echo: 722  loss: 0.35844\n",
      "echo: 723  loss: 0.35833\n",
      "echo: 724  loss: 0.35823\n",
      "echo: 725  loss: 0.35812\n",
      "echo: 726  loss: 0.35802\n",
      "echo: 727  loss: 0.35792\n",
      "echo: 728  loss: 0.35781\n",
      "echo: 729  loss: 0.35771\n",
      "echo: 730  loss: 0.35761\n",
      "echo: 731  loss: 0.35751\n",
      "echo: 732  loss: 0.35741\n",
      "echo: 733  loss: 0.35732\n",
      "echo: 734  loss: 0.35722\n",
      "echo: 735  loss: 0.35712\n",
      "echo: 736  loss: 0.35703\n",
      "echo: 737  loss: 0.35694\n",
      "echo: 738  loss: 0.35684\n",
      "echo: 739  loss: 0.35675\n",
      "echo: 740  loss: 0.35666\n",
      "echo: 741  loss: 0.35657\n",
      "echo: 742  loss: 0.35648\n",
      "echo: 743  loss: 0.35639\n",
      "echo: 744  loss: 0.3563\n",
      "echo: 745  loss: 0.35621\n",
      "echo: 746  loss: 0.35612\n",
      "echo: 747  loss: 0.35604\n",
      "echo: 748  loss: 0.35595\n",
      "echo: 749  loss: 0.35586\n",
      "echo: 750  loss: 0.35578\n",
      "echo: 751  loss: 0.3557\n",
      "echo: 752  loss: 0.35561\n",
      "echo: 753  loss: 0.35553\n",
      "echo: 754  loss: 0.35545\n",
      "echo: 755  loss: 0.35537\n",
      "echo: 756  loss: 0.35529\n",
      "echo: 757  loss: 0.35521\n",
      "echo: 758  loss: 0.35513\n",
      "echo: 759  loss: 0.35505\n",
      "echo: 760  loss: 0.35497\n",
      "echo: 761  loss: 0.35489\n",
      "echo: 762  loss: 0.35481\n",
      "echo: 763  loss: 0.35474\n",
      "echo: 764  loss: 0.35466\n",
      "echo: 765  loss: 0.35459\n",
      "echo: 766  loss: 0.35451\n",
      "echo: 767  loss: 0.35444\n",
      "echo: 768  loss: 0.35437\n",
      "echo: 769  loss: 0.35429\n",
      "echo: 770  loss: 0.35422\n",
      "echo: 771  loss: 0.35415\n",
      "echo: 772  loss: 0.35408\n",
      "echo: 773  loss: 0.35401\n",
      "echo: 774  loss: 0.35393\n",
      "echo: 775  loss: 0.35387\n",
      "echo: 776  loss: 0.3538\n",
      "echo: 777  loss: 0.35373\n",
      "echo: 778  loss: 0.35366\n",
      "echo: 779  loss: 0.35359\n",
      "echo: 780  loss: 0.35352\n",
      "echo: 781  loss: 0.35346\n",
      "echo: 782  loss: 0.35339\n",
      "echo: 783  loss: 0.35332\n",
      "echo: 784  loss: 0.35326\n",
      "echo: 785  loss: 0.35319\n",
      "echo: 786  loss: 0.35313\n",
      "echo: 787  loss: 0.35306\n",
      "echo: 788  loss: 0.353\n",
      "echo: 789  loss: 0.35294\n",
      "echo: 790  loss: 0.35287\n",
      "echo: 791  loss: 0.35281\n",
      "echo: 792  loss: 0.35275\n",
      "echo: 793  loss: 0.35269\n",
      "echo: 794  loss: 0.35262\n",
      "echo: 795  loss: 0.35256\n",
      "echo: 796  loss: 0.3525\n",
      "echo: 797  loss: 0.35244\n",
      "echo: 798  loss: 0.35238\n",
      "echo: 799  loss: 0.35232\n",
      "echo: 800  loss: 0.35226\n",
      "echo: 801  loss: 0.35221\n",
      "echo: 802  loss: 0.35215\n",
      "echo: 803  loss: 0.35209\n",
      "echo: 804  loss: 0.35203\n",
      "echo: 805  loss: 0.35197\n",
      "echo: 806  loss: 0.35192\n",
      "echo: 807  loss: 0.35186\n",
      "echo: 808  loss: 0.3518\n",
      "echo: 809  loss: 0.35175\n",
      "echo: 810  loss: 0.35169\n",
      "echo: 811  loss: 0.35164\n",
      "echo: 812  loss: 0.35158\n",
      "echo: 813  loss: 0.35153\n",
      "echo: 814  loss: 0.35147\n",
      "echo: 815  loss: 0.35142\n",
      "echo: 816  loss: 0.35136\n",
      "echo: 817  loss: 0.35131\n",
      "echo: 818  loss: 0.35126\n",
      "echo: 819  loss: 0.3512\n",
      "echo: 820  loss: 0.35115\n",
      "echo: 821  loss: 0.3511\n",
      "echo: 822  loss: 0.35105\n",
      "echo: 823  loss: 0.35099\n",
      "echo: 824  loss: 0.35094\n",
      "echo: 825  loss: 0.35089\n",
      "echo: 826  loss: 0.35084\n",
      "echo: 827  loss: 0.35079\n",
      "echo: 828  loss: 0.35074\n",
      "echo: 829  loss: 0.35069\n",
      "echo: 830  loss: 0.35064\n",
      "echo: 831  loss: 0.35059\n",
      "echo: 832  loss: 0.35054\n",
      "echo: 833  loss: 0.35049\n",
      "echo: 834  loss: 0.35044\n",
      "echo: 835  loss: 0.35039\n",
      "echo: 836  loss: 0.35034\n",
      "echo: 837  loss: 0.35029\n",
      "echo: 838  loss: 0.35024\n",
      "echo: 839  loss: 0.3502\n",
      "echo: 840  loss: 0.35015\n",
      "echo: 841  loss: 0.3501\n",
      "echo: 842  loss: 0.35005\n",
      "echo: 843  loss: 0.35001\n",
      "echo: 844  loss: 0.34996\n",
      "echo: 845  loss: 0.34991\n",
      "echo: 846  loss: 0.34986\n",
      "echo: 847  loss: 0.34982\n",
      "echo: 848  loss: 0.34977\n",
      "echo: 849  loss: 0.34972\n",
      "echo: 850  loss: 0.34968\n",
      "echo: 851  loss: 0.34963\n",
      "echo: 852  loss: 0.34959\n",
      "echo: 853  loss: 0.34954\n",
      "echo: 854  loss: 0.34949\n",
      "echo: 855  loss: 0.34945\n",
      "echo: 856  loss: 0.3494\n",
      "echo: 857  loss: 0.34936\n",
      "echo: 858  loss: 0.34931\n",
      "echo: 859  loss: 0.34927\n",
      "echo: 860  loss: 0.34922\n",
      "echo: 861  loss: 0.34918\n",
      "echo: 862  loss: 0.34913\n",
      "echo: 863  loss: 0.34909\n",
      "echo: 864  loss: 0.34904\n",
      "echo: 865  loss: 0.349\n",
      "echo: 866  loss: 0.34895\n",
      "echo: 867  loss: 0.34891\n",
      "echo: 868  loss: 0.34886\n",
      "echo: 869  loss: 0.34882\n",
      "echo: 870  loss: 0.34878\n",
      "echo: 871  loss: 0.34873\n",
      "echo: 872  loss: 0.34869\n",
      "echo: 873  loss: 0.34864\n",
      "echo: 874  loss: 0.3486\n",
      "echo: 875  loss: 0.34855\n",
      "echo: 876  loss: 0.34851\n",
      "echo: 877  loss: 0.34847\n",
      "echo: 878  loss: 0.34842\n",
      "echo: 879  loss: 0.34838\n",
      "echo: 880  loss: 0.34833\n",
      "echo: 881  loss: 0.34829\n",
      "echo: 882  loss: 0.34824\n",
      "echo: 883  loss: 0.3482\n",
      "echo: 884  loss: 0.34816\n",
      "echo: 885  loss: 0.34811\n",
      "echo: 886  loss: 0.34807\n",
      "echo: 887  loss: 0.34802\n",
      "echo: 888  loss: 0.34798\n",
      "echo: 889  loss: 0.34793\n",
      "echo: 890  loss: 0.34789\n",
      "echo: 891  loss: 0.34784\n",
      "echo: 892  loss: 0.3478\n",
      "echo: 893  loss: 0.34775\n",
      "echo: 894  loss: 0.34771\n",
      "echo: 895  loss: 0.34766\n",
      "echo: 896  loss: 0.34762\n",
      "echo: 897  loss: 0.34757\n",
      "echo: 898  loss: 0.34753\n",
      "echo: 899  loss: 0.34748\n",
      "echo: 900  loss: 0.34743\n",
      "echo: 901  loss: 0.34739\n",
      "echo: 902  loss: 0.34734\n",
      "echo: 903  loss: 0.3473\n",
      "echo: 904  loss: 0.34725\n",
      "echo: 905  loss: 0.3472\n",
      "echo: 906  loss: 0.34716\n",
      "echo: 907  loss: 0.34711\n",
      "echo: 908  loss: 0.34706\n",
      "echo: 909  loss: 0.34702\n",
      "echo: 910  loss: 0.34697\n",
      "echo: 911  loss: 0.34692\n",
      "echo: 912  loss: 0.34687\n",
      "echo: 913  loss: 0.34682\n",
      "echo: 914  loss: 0.34678\n",
      "echo: 915  loss: 0.34673\n",
      "echo: 916  loss: 0.34668\n",
      "echo: 917  loss: 0.34663\n",
      "echo: 918  loss: 0.34658\n",
      "echo: 919  loss: 0.34653\n",
      "echo: 920  loss: 0.34649\n",
      "echo: 921  loss: 0.34644\n",
      "echo: 922  loss: 0.34639\n",
      "echo: 923  loss: 0.34634\n",
      "echo: 924  loss: 0.34629\n",
      "echo: 925  loss: 0.34624\n",
      "echo: 926  loss: 0.34619\n",
      "echo: 927  loss: 0.34614\n",
      "echo: 928  loss: 0.34609\n",
      "echo: 929  loss: 0.34604\n",
      "echo: 930  loss: 0.34599\n",
      "echo: 931  loss: 0.34594\n",
      "echo: 932  loss: 0.3459\n",
      "echo: 933  loss: 0.34585\n",
      "echo: 934  loss: 0.3458\n",
      "echo: 935  loss: 0.34575\n",
      "echo: 936  loss: 0.3457\n",
      "echo: 937  loss: 0.34565\n",
      "echo: 938  loss: 0.3456\n",
      "echo: 939  loss: 0.34555\n",
      "echo: 940  loss: 0.3455\n",
      "echo: 941  loss: 0.34545\n",
      "echo: 942  loss: 0.3454\n",
      "echo: 943  loss: 0.34536\n",
      "echo: 944  loss: 0.34531\n",
      "echo: 945  loss: 0.34526\n",
      "echo: 946  loss: 0.34521\n",
      "echo: 947  loss: 0.34516\n",
      "echo: 948  loss: 0.34512\n",
      "echo: 949  loss: 0.34507\n",
      "echo: 950  loss: 0.34502\n",
      "echo: 951  loss: 0.34497\n",
      "echo: 952  loss: 0.34493\n",
      "echo: 953  loss: 0.34488\n",
      "echo: 954  loss: 0.34483\n",
      "echo: 955  loss: 0.34479\n",
      "echo: 956  loss: 0.34474\n",
      "echo: 957  loss: 0.3447\n",
      "echo: 958  loss: 0.34465\n",
      "echo: 959  loss: 0.3446\n",
      "echo: 960  loss: 0.34456\n",
      "echo: 961  loss: 0.34451\n",
      "echo: 962  loss: 0.34447\n",
      "echo: 963  loss: 0.34443\n",
      "echo: 964  loss: 0.34438\n",
      "echo: 965  loss: 0.34434\n",
      "echo: 966  loss: 0.3443\n",
      "echo: 967  loss: 0.34425\n",
      "echo: 968  loss: 0.34421\n",
      "echo: 969  loss: 0.34417\n",
      "echo: 970  loss: 0.34412\n",
      "echo: 971  loss: 0.34408\n",
      "echo: 972  loss: 0.34404\n",
      "echo: 973  loss: 0.344\n",
      "echo: 974  loss: 0.34396\n",
      "echo: 975  loss: 0.34392\n",
      "echo: 976  loss: 0.34388\n",
      "echo: 977  loss: 0.34384\n",
      "echo: 978  loss: 0.3438\n",
      "echo: 979  loss: 0.34376\n",
      "echo: 980  loss: 0.34372\n",
      "echo: 981  loss: 0.34368\n",
      "echo: 982  loss: 0.34364\n",
      "echo: 983  loss: 0.3436\n",
      "echo: 984  loss: 0.34356\n",
      "echo: 985  loss: 0.34352\n",
      "echo: 986  loss: 0.34349\n",
      "echo: 987  loss: 0.34345\n",
      "echo: 988  loss: 0.34341\n",
      "echo: 989  loss: 0.34337\n",
      "echo: 990  loss: 0.34334\n",
      "echo: 991  loss: 0.3433\n",
      "echo: 992  loss: 0.34326\n",
      "echo: 993  loss: 0.34323\n",
      "echo: 994  loss: 0.34319\n",
      "echo: 995  loss: 0.34316\n",
      "echo: 996  loss: 0.34312\n",
      "echo: 997  loss: 0.34308\n",
      "echo: 998  loss: 0.34305\n",
      "echo: 999  loss: 0.34302\n",
      "echo: 1000  loss: 0.34298\n",
      "echo: 1001  loss: 0.34295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 1002  loss: 0.34291\n",
      "echo: 1003  loss: 0.34288\n",
      "echo: 1004  loss: 0.34285\n",
      "echo: 1005  loss: 0.34281\n",
      "echo: 1006  loss: 0.34278\n",
      "echo: 1007  loss: 0.34275\n",
      "echo: 1008  loss: 0.34271\n",
      "echo: 1009  loss: 0.34268\n",
      "echo: 1010  loss: 0.34265\n",
      "echo: 1011  loss: 0.34262\n",
      "echo: 1012  loss: 0.34259\n",
      "echo: 1013  loss: 0.34255\n",
      "echo: 1014  loss: 0.34252\n",
      "echo: 1015  loss: 0.34249\n",
      "echo: 1016  loss: 0.34246\n",
      "echo: 1017  loss: 0.34243\n",
      "echo: 1018  loss: 0.3424\n",
      "echo: 1019  loss: 0.34237\n",
      "echo: 1020  loss: 0.34234\n",
      "echo: 1021  loss: 0.34231\n",
      "echo: 1022  loss: 0.34228\n",
      "echo: 1023  loss: 0.34225\n",
      "echo: 1024  loss: 0.34222\n",
      "echo: 1025  loss: 0.34219\n",
      "echo: 1026  loss: 0.34216\n",
      "echo: 1027  loss: 0.34213\n",
      "echo: 1028  loss: 0.3421\n",
      "echo: 1029  loss: 0.34207\n",
      "echo: 1030  loss: 0.34205\n",
      "echo: 1031  loss: 0.34202\n",
      "echo: 1032  loss: 0.34199\n",
      "echo: 1033  loss: 0.34196\n",
      "echo: 1034  loss: 0.34193\n",
      "echo: 1035  loss: 0.34191\n",
      "echo: 1036  loss: 0.34188\n",
      "echo: 1037  loss: 0.34185\n",
      "echo: 1038  loss: 0.34182\n",
      "echo: 1039  loss: 0.3418\n",
      "echo: 1040  loss: 0.34177\n",
      "echo: 1041  loss: 0.34174\n",
      "echo: 1042  loss: 0.34172\n",
      "echo: 1043  loss: 0.34169\n",
      "echo: 1044  loss: 0.34166\n",
      "echo: 1045  loss: 0.34164\n",
      "echo: 1046  loss: 0.34161\n",
      "echo: 1047  loss: 0.34159\n",
      "echo: 1048  loss: 0.34156\n",
      "echo: 1049  loss: 0.34154\n",
      "echo: 1050  loss: 0.34151\n",
      "echo: 1051  loss: 0.34148\n",
      "echo: 1052  loss: 0.34146\n",
      "echo: 1053  loss: 0.34143\n",
      "echo: 1054  loss: 0.34141\n",
      "echo: 1055  loss: 0.34138\n",
      "echo: 1056  loss: 0.34136\n",
      "echo: 1057  loss: 0.34134\n",
      "echo: 1058  loss: 0.34131\n",
      "echo: 1059  loss: 0.34129\n",
      "echo: 1060  loss: 0.34126\n",
      "echo: 1061  loss: 0.34124\n",
      "echo: 1062  loss: 0.34121\n",
      "echo: 1063  loss: 0.34119\n",
      "echo: 1064  loss: 0.34117\n",
      "echo: 1065  loss: 0.34114\n",
      "echo: 1066  loss: 0.34112\n",
      "echo: 1067  loss: 0.3411\n",
      "echo: 1068  loss: 0.34107\n",
      "echo: 1069  loss: 0.34105\n",
      "echo: 1070  loss: 0.34103\n",
      "echo: 1071  loss: 0.341\n",
      "echo: 1072  loss: 0.34098\n",
      "echo: 1073  loss: 0.34096\n",
      "echo: 1074  loss: 0.34094\n",
      "echo: 1075  loss: 0.34091\n",
      "echo: 1076  loss: 0.34089\n",
      "echo: 1077  loss: 0.34087\n",
      "echo: 1078  loss: 0.34085\n",
      "echo: 1079  loss: 0.34082\n",
      "echo: 1080  loss: 0.3408\n",
      "echo: 1081  loss: 0.34078\n",
      "echo: 1082  loss: 0.34076\n",
      "echo: 1083  loss: 0.34074\n",
      "echo: 1084  loss: 0.34072\n",
      "echo: 1085  loss: 0.34069\n",
      "echo: 1086  loss: 0.34067\n",
      "echo: 1087  loss: 0.34065\n",
      "echo: 1088  loss: 0.34063\n",
      "echo: 1089  loss: 0.34061\n",
      "echo: 1090  loss: 0.34059\n",
      "echo: 1091  loss: 0.34057\n",
      "echo: 1092  loss: 0.34055\n",
      "echo: 1093  loss: 0.34053\n",
      "echo: 1094  loss: 0.34051\n",
      "echo: 1095  loss: 0.34048\n",
      "echo: 1096  loss: 0.34046\n",
      "echo: 1097  loss: 0.34044\n",
      "echo: 1098  loss: 0.34042\n",
      "echo: 1099  loss: 0.3404\n",
      "echo: 1100  loss: 0.34038\n",
      "echo: 1101  loss: 0.34036\n",
      "echo: 1102  loss: 0.34034\n",
      "echo: 1103  loss: 0.34032\n",
      "echo: 1104  loss: 0.3403\n",
      "echo: 1105  loss: 0.34028\n",
      "echo: 1106  loss: 0.34026\n",
      "echo: 1107  loss: 0.34024\n",
      "echo: 1108  loss: 0.34022\n",
      "echo: 1109  loss: 0.3402\n",
      "echo: 1110  loss: 0.34019\n",
      "echo: 1111  loss: 0.34017\n",
      "echo: 1112  loss: 0.34015\n",
      "echo: 1113  loss: 0.34013\n",
      "echo: 1114  loss: 0.34011\n",
      "echo: 1115  loss: 0.34009\n",
      "echo: 1116  loss: 0.34007\n",
      "echo: 1117  loss: 0.34005\n",
      "echo: 1118  loss: 0.34003\n",
      "echo: 1119  loss: 0.34001\n",
      "echo: 1120  loss: 0.34\n",
      "echo: 1121  loss: 0.33998\n",
      "echo: 1122  loss: 0.33996\n",
      "echo: 1123  loss: 0.33994\n",
      "echo: 1124  loss: 0.33992\n",
      "echo: 1125  loss: 0.3399\n",
      "echo: 1126  loss: 0.33988\n",
      "echo: 1127  loss: 0.33987\n",
      "echo: 1128  loss: 0.33985\n",
      "echo: 1129  loss: 0.33983\n",
      "echo: 1130  loss: 0.33981\n",
      "echo: 1131  loss: 0.33979\n",
      "echo: 1132  loss: 0.33978\n",
      "echo: 1133  loss: 0.33976\n",
      "echo: 1134  loss: 0.33974\n",
      "echo: 1135  loss: 0.33972\n",
      "echo: 1136  loss: 0.33971\n",
      "echo: 1137  loss: 0.33969\n",
      "echo: 1138  loss: 0.33967\n",
      "echo: 1139  loss: 0.33965\n",
      "echo: 1140  loss: 0.33964\n",
      "echo: 1141  loss: 0.33962\n",
      "echo: 1142  loss: 0.3396\n",
      "echo: 1143  loss: 0.33958\n",
      "echo: 1144  loss: 0.33957\n",
      "echo: 1145  loss: 0.33955\n",
      "echo: 1146  loss: 0.33953\n",
      "echo: 1147  loss: 0.33952\n",
      "echo: 1148  loss: 0.3395\n",
      "echo: 1149  loss: 0.33948\n",
      "echo: 1150  loss: 0.33946\n",
      "echo: 1151  loss: 0.33945\n",
      "echo: 1152  loss: 0.33943\n",
      "echo: 1153  loss: 0.33941\n",
      "echo: 1154  loss: 0.3394\n",
      "echo: 1155  loss: 0.33938\n",
      "echo: 1156  loss: 0.33937\n",
      "echo: 1157  loss: 0.33935\n",
      "echo: 1158  loss: 0.33933\n",
      "echo: 1159  loss: 0.33932\n",
      "echo: 1160  loss: 0.3393\n",
      "echo: 1161  loss: 0.33928\n",
      "echo: 1162  loss: 0.33927\n",
      "echo: 1163  loss: 0.33925\n",
      "echo: 1164  loss: 0.33924\n",
      "echo: 1165  loss: 0.33922\n",
      "echo: 1166  loss: 0.3392\n",
      "echo: 1167  loss: 0.33919\n",
      "echo: 1168  loss: 0.33917\n",
      "echo: 1169  loss: 0.33916\n",
      "echo: 1170  loss: 0.33914\n",
      "echo: 1171  loss: 0.33912\n",
      "echo: 1172  loss: 0.33911\n",
      "echo: 1173  loss: 0.33909\n",
      "echo: 1174  loss: 0.33908\n",
      "echo: 1175  loss: 0.33906\n",
      "echo: 1176  loss: 0.33905\n",
      "echo: 1177  loss: 0.33903\n",
      "echo: 1178  loss: 0.33902\n",
      "echo: 1179  loss: 0.339\n",
      "echo: 1180  loss: 0.33899\n",
      "echo: 1181  loss: 0.33897\n",
      "echo: 1182  loss: 0.33895\n",
      "echo: 1183  loss: 0.33894\n",
      "echo: 1184  loss: 0.33892\n",
      "echo: 1185  loss: 0.33891\n",
      "echo: 1186  loss: 0.33889\n",
      "echo: 1187  loss: 0.33888\n",
      "echo: 1188  loss: 0.33886\n",
      "echo: 1189  loss: 0.33885\n",
      "echo: 1190  loss: 0.33884\n",
      "echo: 1191  loss: 0.33882\n",
      "echo: 1192  loss: 0.33881\n",
      "echo: 1193  loss: 0.33879\n",
      "echo: 1194  loss: 0.33878\n",
      "echo: 1195  loss: 0.33876\n",
      "echo: 1196  loss: 0.33875\n",
      "echo: 1197  loss: 0.33873\n",
      "echo: 1198  loss: 0.33872\n",
      "echo: 1199  loss: 0.3387\n",
      "echo: 1200  loss: 0.33869\n",
      "echo: 1201  loss: 0.33867\n",
      "echo: 1202  loss: 0.33866\n",
      "echo: 1203  loss: 0.33865\n",
      "echo: 1204  loss: 0.33863\n",
      "echo: 1205  loss: 0.33862\n",
      "echo: 1206  loss: 0.3386\n",
      "echo: 1207  loss: 0.33859\n",
      "echo: 1208  loss: 0.33858\n",
      "echo: 1209  loss: 0.33856\n",
      "echo: 1210  loss: 0.33855\n",
      "echo: 1211  loss: 0.33853\n",
      "echo: 1212  loss: 0.33852\n",
      "echo: 1213  loss: 0.33851\n",
      "echo: 1214  loss: 0.33849\n",
      "echo: 1215  loss: 0.33848\n",
      "echo: 1216  loss: 0.33846\n",
      "echo: 1217  loss: 0.33845\n",
      "echo: 1218  loss: 0.33844\n",
      "echo: 1219  loss: 0.33842\n",
      "echo: 1220  loss: 0.33841\n",
      "echo: 1221  loss: 0.3384\n",
      "echo: 1222  loss: 0.33838\n",
      "echo: 1223  loss: 0.33837\n",
      "echo: 1224  loss: 0.33836\n",
      "echo: 1225  loss: 0.33834\n",
      "echo: 1226  loss: 0.33833\n",
      "echo: 1227  loss: 0.33832\n",
      "echo: 1228  loss: 0.3383\n",
      "echo: 1229  loss: 0.33829\n",
      "echo: 1230  loss: 0.33828\n",
      "echo: 1231  loss: 0.33826\n",
      "echo: 1232  loss: 0.33825\n",
      "echo: 1233  loss: 0.33824\n",
      "echo: 1234  loss: 0.33822\n",
      "echo: 1235  loss: 0.33821\n",
      "echo: 1236  loss: 0.3382\n",
      "echo: 1237  loss: 0.33818\n",
      "echo: 1238  loss: 0.33817\n",
      "echo: 1239  loss: 0.33816\n",
      "echo: 1240  loss: 0.33815\n",
      "echo: 1241  loss: 0.33813\n",
      "echo: 1242  loss: 0.33812\n",
      "echo: 1243  loss: 0.33811\n",
      "echo: 1244  loss: 0.33809\n",
      "echo: 1245  loss: 0.33808\n",
      "echo: 1246  loss: 0.33807\n",
      "echo: 1247  loss: 0.33806\n",
      "echo: 1248  loss: 0.33804\n",
      "echo: 1249  loss: 0.33803\n",
      "echo: 1250  loss: 0.33802\n",
      "echo: 1251  loss: 0.33801\n",
      "echo: 1252  loss: 0.33799\n",
      "echo: 1253  loss: 0.33798\n",
      "echo: 1254  loss: 0.33797\n",
      "echo: 1255  loss: 0.33796\n",
      "echo: 1256  loss: 0.33794\n",
      "echo: 1257  loss: 0.33793\n",
      "echo: 1258  loss: 0.33792\n",
      "echo: 1259  loss: 0.33791\n",
      "echo: 1260  loss: 0.3379\n",
      "echo: 1261  loss: 0.33788\n",
      "echo: 1262  loss: 0.33787\n",
      "echo: 1263  loss: 0.33786\n",
      "echo: 1264  loss: 0.33785\n",
      "echo: 1265  loss: 0.33783\n",
      "echo: 1266  loss: 0.33782\n",
      "echo: 1267  loss: 0.33781\n",
      "echo: 1268  loss: 0.3378\n",
      "echo: 1269  loss: 0.33779\n",
      "echo: 1270  loss: 0.33777\n",
      "echo: 1271  loss: 0.33776\n",
      "echo: 1272  loss: 0.33775\n",
      "echo: 1273  loss: 0.33774\n",
      "echo: 1274  loss: 0.33773\n",
      "echo: 1275  loss: 0.33772\n",
      "echo: 1276  loss: 0.3377\n",
      "echo: 1277  loss: 0.33769\n",
      "echo: 1278  loss: 0.33768\n",
      "echo: 1279  loss: 0.33767\n",
      "echo: 1280  loss: 0.33766\n",
      "echo: 1281  loss: 0.33765\n",
      "echo: 1282  loss: 0.33763\n",
      "echo: 1283  loss: 0.33762\n",
      "echo: 1284  loss: 0.33761\n",
      "echo: 1285  loss: 0.3376\n",
      "echo: 1286  loss: 0.33759\n",
      "echo: 1287  loss: 0.33758\n",
      "echo: 1288  loss: 0.33756\n",
      "echo: 1289  loss: 0.33755\n",
      "echo: 1290  loss: 0.33754\n",
      "echo: 1291  loss: 0.33753\n",
      "echo: 1292  loss: 0.33752\n",
      "echo: 1293  loss: 0.33751\n",
      "echo: 1294  loss: 0.3375\n",
      "echo: 1295  loss: 0.33749\n",
      "echo: 1296  loss: 0.33747\n",
      "echo: 1297  loss: 0.33746\n",
      "echo: 1298  loss: 0.33745\n",
      "echo: 1299  loss: 0.33744\n",
      "echo: 1300  loss: 0.33743\n",
      "echo: 1301  loss: 0.33742\n",
      "echo: 1302  loss: 0.33741\n",
      "echo: 1303  loss: 0.3374\n",
      "echo: 1304  loss: 0.33739\n",
      "echo: 1305  loss: 0.33737\n",
      "echo: 1306  loss: 0.33736\n",
      "echo: 1307  loss: 0.33735\n",
      "echo: 1308  loss: 0.33734\n",
      "echo: 1309  loss: 0.33733\n",
      "echo: 1310  loss: 0.33732\n",
      "echo: 1311  loss: 0.33731\n",
      "echo: 1312  loss: 0.3373\n",
      "echo: 1313  loss: 0.33729\n",
      "echo: 1314  loss: 0.33728\n",
      "echo: 1315  loss: 0.33727\n",
      "echo: 1316  loss: 0.33726\n",
      "echo: 1317  loss: 0.33725\n",
      "echo: 1318  loss: 0.33723\n",
      "echo: 1319  loss: 0.33722\n",
      "echo: 1320  loss: 0.33721\n",
      "echo: 1321  loss: 0.3372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 1322  loss: 0.33719\n",
      "echo: 1323  loss: 0.33718\n",
      "echo: 1324  loss: 0.33717\n",
      "echo: 1325  loss: 0.33716\n",
      "echo: 1326  loss: 0.33715\n",
      "echo: 1327  loss: 0.33714\n",
      "echo: 1328  loss: 0.33713\n",
      "echo: 1329  loss: 0.33712\n",
      "echo: 1330  loss: 0.33711\n",
      "echo: 1331  loss: 0.3371\n",
      "echo: 1332  loss: 0.33709\n",
      "echo: 1333  loss: 0.33708\n",
      "echo: 1334  loss: 0.33707\n",
      "echo: 1335  loss: 0.33706\n",
      "echo: 1336  loss: 0.33705\n",
      "echo: 1337  loss: 0.33704\n",
      "echo: 1338  loss: 0.33703\n",
      "echo: 1339  loss: 0.33702\n",
      "echo: 1340  loss: 0.33701\n",
      "echo: 1341  loss: 0.337\n",
      "echo: 1342  loss: 0.33699\n",
      "echo: 1343  loss: 0.33698\n",
      "echo: 1344  loss: 0.33697\n",
      "echo: 1345  loss: 0.33696\n",
      "echo: 1346  loss: 0.33695\n",
      "echo: 1347  loss: 0.33694\n",
      "echo: 1348  loss: 0.33693\n",
      "echo: 1349  loss: 0.33692\n",
      "echo: 1350  loss: 0.33691\n",
      "echo: 1351  loss: 0.3369\n",
      "echo: 1352  loss: 0.33689\n",
      "echo: 1353  loss: 0.33688\n",
      "echo: 1354  loss: 0.33687\n",
      "echo: 1355  loss: 0.33686\n",
      "echo: 1356  loss: 0.33685\n",
      "echo: 1357  loss: 0.33684\n",
      "echo: 1358  loss: 0.33683\n",
      "echo: 1359  loss: 0.33682\n",
      "echo: 1360  loss: 0.33681\n",
      "echo: 1361  loss: 0.3368\n",
      "echo: 1362  loss: 0.33679\n",
      "echo: 1363  loss: 0.33678\n",
      "echo: 1364  loss: 0.33677\n",
      "echo: 1365  loss: 0.33676\n",
      "echo: 1366  loss: 0.33675\n",
      "echo: 1367  loss: 0.33674\n",
      "echo: 1368  loss: 0.33673\n",
      "echo: 1369  loss: 0.33672\n",
      "echo: 1370  loss: 0.33671\n",
      "echo: 1371  loss: 0.3367\n",
      "echo: 1372  loss: 0.33669\n",
      "echo: 1373  loss: 0.33668\n",
      "echo: 1374  loss: 0.33668\n",
      "echo: 1375  loss: 0.33667\n",
      "echo: 1376  loss: 0.33666\n",
      "echo: 1377  loss: 0.33665\n",
      "echo: 1378  loss: 0.33664\n",
      "echo: 1379  loss: 0.33663\n",
      "echo: 1380  loss: 0.33662\n",
      "echo: 1381  loss: 0.33661\n",
      "echo: 1382  loss: 0.3366\n",
      "echo: 1383  loss: 0.33659\n",
      "echo: 1384  loss: 0.33658\n",
      "echo: 1385  loss: 0.33657\n",
      "echo: 1386  loss: 0.33656\n",
      "echo: 1387  loss: 0.33655\n",
      "echo: 1388  loss: 0.33655\n",
      "echo: 1389  loss: 0.33654\n",
      "echo: 1390  loss: 0.33653\n",
      "echo: 1391  loss: 0.33652\n",
      "echo: 1392  loss: 0.33651\n",
      "echo: 1393  loss: 0.3365\n",
      "echo: 1394  loss: 0.33649\n",
      "echo: 1395  loss: 0.33648\n",
      "echo: 1396  loss: 0.33647\n",
      "echo: 1397  loss: 0.33646\n",
      "echo: 1398  loss: 0.33646\n",
      "echo: 1399  loss: 0.33645\n",
      "echo: 1400  loss: 0.33644\n",
      "echo: 1401  loss: 0.33643\n",
      "echo: 1402  loss: 0.33642\n",
      "echo: 1403  loss: 0.33641\n",
      "echo: 1404  loss: 0.3364\n",
      "echo: 1405  loss: 0.33639\n",
      "echo: 1406  loss: 0.33638\n",
      "echo: 1407  loss: 0.33638\n",
      "echo: 1408  loss: 0.33637\n",
      "echo: 1409  loss: 0.33636\n",
      "echo: 1410  loss: 0.33635\n",
      "echo: 1411  loss: 0.33634\n",
      "echo: 1412  loss: 0.33633\n",
      "echo: 1413  loss: 0.33632\n",
      "echo: 1414  loss: 0.33631\n",
      "echo: 1415  loss: 0.33631\n",
      "echo: 1416  loss: 0.3363\n",
      "echo: 1417  loss: 0.33629\n",
      "echo: 1418  loss: 0.33628\n",
      "echo: 1419  loss: 0.33627\n",
      "echo: 1420  loss: 0.33626\n",
      "echo: 1421  loss: 0.33625\n",
      "echo: 1422  loss: 0.33625\n",
      "echo: 1423  loss: 0.33624\n",
      "echo: 1424  loss: 0.33623\n",
      "echo: 1425  loss: 0.33622\n",
      "echo: 1426  loss: 0.33621\n",
      "echo: 1427  loss: 0.3362\n",
      "echo: 1428  loss: 0.33619\n",
      "echo: 1429  loss: 0.33619\n",
      "echo: 1430  loss: 0.33618\n",
      "echo: 1431  loss: 0.33617\n",
      "echo: 1432  loss: 0.33616\n",
      "echo: 1433  loss: 0.33615\n",
      "echo: 1434  loss: 0.33614\n",
      "echo: 1435  loss: 0.33614\n",
      "echo: 1436  loss: 0.33613\n",
      "echo: 1437  loss: 0.33612\n",
      "echo: 1438  loss: 0.33611\n",
      "echo: 1439  loss: 0.3361\n",
      "echo: 1440  loss: 0.33609\n",
      "echo: 1441  loss: 0.33609\n",
      "echo: 1442  loss: 0.33608\n",
      "echo: 1443  loss: 0.33607\n",
      "echo: 1444  loss: 0.33606\n",
      "echo: 1445  loss: 0.33605\n",
      "echo: 1446  loss: 0.33605\n",
      "echo: 1447  loss: 0.33604\n",
      "echo: 1448  loss: 0.33603\n",
      "echo: 1449  loss: 0.33602\n",
      "echo: 1450  loss: 0.33601\n",
      "echo: 1451  loss: 0.33601\n",
      "echo: 1452  loss: 0.336\n",
      "echo: 1453  loss: 0.33599\n",
      "echo: 1454  loss: 0.33598\n",
      "echo: 1455  loss: 0.33597\n",
      "echo: 1456  loss: 0.33597\n",
      "echo: 1457  loss: 0.33596\n",
      "echo: 1458  loss: 0.33595\n",
      "echo: 1459  loss: 0.33594\n",
      "echo: 1460  loss: 0.33593\n",
      "echo: 1461  loss: 0.33593\n",
      "echo: 1462  loss: 0.33592\n",
      "echo: 1463  loss: 0.33591\n",
      "echo: 1464  loss: 0.3359\n",
      "echo: 1465  loss: 0.33589\n",
      "echo: 1466  loss: 0.33589\n",
      "echo: 1467  loss: 0.33588\n",
      "echo: 1468  loss: 0.33587\n",
      "echo: 1469  loss: 0.33586\n",
      "echo: 1470  loss: 0.33585\n",
      "echo: 1471  loss: 0.33585\n",
      "echo: 1472  loss: 0.33584\n",
      "echo: 1473  loss: 0.33583\n",
      "echo: 1474  loss: 0.33582\n",
      "echo: 1475  loss: 0.33582\n",
      "echo: 1476  loss: 0.33581\n",
      "echo: 1477  loss: 0.3358\n",
      "echo: 1478  loss: 0.33579\n",
      "echo: 1479  loss: 0.33579\n",
      "echo: 1480  loss: 0.33578\n",
      "echo: 1481  loss: 0.33577\n",
      "echo: 1482  loss: 0.33576\n",
      "echo: 1483  loss: 0.33575\n",
      "echo: 1484  loss: 0.33575\n",
      "echo: 1485  loss: 0.33574\n",
      "echo: 1486  loss: 0.33573\n",
      "echo: 1487  loss: 0.33572\n",
      "echo: 1488  loss: 0.33572\n",
      "echo: 1489  loss: 0.33571\n",
      "echo: 1490  loss: 0.3357\n",
      "echo: 1491  loss: 0.33569\n",
      "echo: 1492  loss: 0.33569\n",
      "echo: 1493  loss: 0.33568\n",
      "echo: 1494  loss: 0.33567\n",
      "echo: 1495  loss: 0.33566\n",
      "echo: 1496  loss: 0.33566\n",
      "echo: 1497  loss: 0.33565\n",
      "echo: 1498  loss: 0.33564\n",
      "echo: 1499  loss: 0.33564\n",
      "echo: 1500  loss: 0.33563\n",
      "echo: 1501  loss: 0.33562\n",
      "echo: 1502  loss: 0.33561\n",
      "echo: 1503  loss: 0.33561\n",
      "echo: 1504  loss: 0.3356\n",
      "echo: 1505  loss: 0.33559\n",
      "echo: 1506  loss: 0.33558\n",
      "echo: 1507  loss: 0.33558\n",
      "echo: 1508  loss: 0.33557\n",
      "echo: 1509  loss: 0.33556\n",
      "echo: 1510  loss: 0.33555\n",
      "echo: 1511  loss: 0.33555\n",
      "echo: 1512  loss: 0.33554\n",
      "echo: 1513  loss: 0.33553\n",
      "echo: 1514  loss: 0.33553\n",
      "echo: 1515  loss: 0.33552\n",
      "echo: 1516  loss: 0.33551\n",
      "echo: 1517  loss: 0.3355\n",
      "echo: 1518  loss: 0.3355\n",
      "echo: 1519  loss: 0.33549\n",
      "echo: 1520  loss: 0.33548\n",
      "echo: 1521  loss: 0.33548\n",
      "echo: 1522  loss: 0.33547\n",
      "echo: 1523  loss: 0.33546\n",
      "echo: 1524  loss: 0.33545\n",
      "echo: 1525  loss: 0.33545\n",
      "echo: 1526  loss: 0.33544\n",
      "echo: 1527  loss: 0.33543\n",
      "echo: 1528  loss: 0.33543\n",
      "echo: 1529  loss: 0.33542\n",
      "echo: 1530  loss: 0.33541\n",
      "echo: 1531  loss: 0.33541\n",
      "echo: 1532  loss: 0.3354\n",
      "echo: 1533  loss: 0.33539\n",
      "echo: 1534  loss: 0.33538\n",
      "echo: 1535  loss: 0.33538\n",
      "echo: 1536  loss: 0.33537\n",
      "echo: 1537  loss: 0.33536\n",
      "echo: 1538  loss: 0.33536\n",
      "echo: 1539  loss: 0.33535\n",
      "echo: 1540  loss: 0.33534\n",
      "echo: 1541  loss: 0.33534\n",
      "echo: 1542  loss: 0.33533\n",
      "echo: 1543  loss: 0.33532\n",
      "echo: 1544  loss: 0.33532\n",
      "echo: 1545  loss: 0.33531\n",
      "echo: 1546  loss: 0.3353\n",
      "echo: 1547  loss: 0.3353\n",
      "echo: 1548  loss: 0.33529\n",
      "echo: 1549  loss: 0.33528\n",
      "echo: 1550  loss: 0.33528\n",
      "echo: 1551  loss: 0.33527\n",
      "echo: 1552  loss: 0.33526\n",
      "echo: 1553  loss: 0.33525\n",
      "echo: 1554  loss: 0.33525\n",
      "echo: 1555  loss: 0.33524\n",
      "echo: 1556  loss: 0.33523\n",
      "echo: 1557  loss: 0.33523\n",
      "echo: 1558  loss: 0.33522\n",
      "echo: 1559  loss: 0.33521\n",
      "echo: 1560  loss: 0.33521\n",
      "echo: 1561  loss: 0.3352\n",
      "echo: 1562  loss: 0.33519\n",
      "echo: 1563  loss: 0.33519\n",
      "echo: 1564  loss: 0.33518\n",
      "echo: 1565  loss: 0.33518\n",
      "echo: 1566  loss: 0.33517\n",
      "echo: 1567  loss: 0.33516\n",
      "echo: 1568  loss: 0.33516\n",
      "echo: 1569  loss: 0.33515\n",
      "echo: 1570  loss: 0.33514\n",
      "echo: 1571  loss: 0.33514\n",
      "echo: 1572  loss: 0.33513\n",
      "echo: 1573  loss: 0.33512\n",
      "echo: 1574  loss: 0.33512\n",
      "echo: 1575  loss: 0.33511\n",
      "echo: 1576  loss: 0.3351\n",
      "echo: 1577  loss: 0.3351\n",
      "echo: 1578  loss: 0.33509\n",
      "echo: 1579  loss: 0.33508\n",
      "echo: 1580  loss: 0.33508\n",
      "echo: 1581  loss: 0.33507\n",
      "echo: 1582  loss: 0.33506\n",
      "echo: 1583  loss: 0.33506\n",
      "echo: 1584  loss: 0.33505\n",
      "echo: 1585  loss: 0.33505\n",
      "echo: 1586  loss: 0.33504\n",
      "echo: 1587  loss: 0.33503\n",
      "echo: 1588  loss: 0.33503\n",
      "echo: 1589  loss: 0.33502\n",
      "echo: 1590  loss: 0.33501\n",
      "echo: 1591  loss: 0.33501\n",
      "echo: 1592  loss: 0.335\n",
      "echo: 1593  loss: 0.33499\n",
      "echo: 1594  loss: 0.33499\n",
      "echo: 1595  loss: 0.33498\n",
      "echo: 1596  loss: 0.33498\n",
      "echo: 1597  loss: 0.33497\n",
      "echo: 1598  loss: 0.33496\n",
      "echo: 1599  loss: 0.33496\n",
      "echo: 1600  loss: 0.33495\n",
      "echo: 1601  loss: 0.33495\n",
      "echo: 1602  loss: 0.33494\n",
      "echo: 1603  loss: 0.33493\n",
      "echo: 1604  loss: 0.33493\n",
      "echo: 1605  loss: 0.33492\n",
      "echo: 1606  loss: 0.33491\n",
      "echo: 1607  loss: 0.33491\n",
      "echo: 1608  loss: 0.3349\n",
      "echo: 1609  loss: 0.3349\n",
      "echo: 1610  loss: 0.33489\n",
      "echo: 1611  loss: 0.33488\n",
      "echo: 1612  loss: 0.33488\n",
      "echo: 1613  loss: 0.33487\n",
      "echo: 1614  loss: 0.33487\n",
      "echo: 1615  loss: 0.33486\n",
      "echo: 1616  loss: 0.33485\n",
      "echo: 1617  loss: 0.33485\n",
      "echo: 1618  loss: 0.33484\n",
      "echo: 1619  loss: 0.33483\n",
      "echo: 1620  loss: 0.33483\n",
      "echo: 1621  loss: 0.33482\n",
      "echo: 1622  loss: 0.33482\n",
      "echo: 1623  loss: 0.33481\n",
      "echo: 1624  loss: 0.3348\n",
      "echo: 1625  loss: 0.3348\n",
      "echo: 1626  loss: 0.33479\n",
      "echo: 1627  loss: 0.33479\n",
      "echo: 1628  loss: 0.33478\n",
      "echo: 1629  loss: 0.33478\n",
      "echo: 1630  loss: 0.33477\n",
      "echo: 1631  loss: 0.33476\n",
      "echo: 1632  loss: 0.33476\n",
      "echo: 1633  loss: 0.33475\n",
      "echo: 1634  loss: 0.33475\n",
      "echo: 1635  loss: 0.33474\n",
      "echo: 1636  loss: 0.33473\n",
      "echo: 1637  loss: 0.33473\n",
      "echo: 1638  loss: 0.33472\n",
      "echo: 1639  loss: 0.33472\n",
      "echo: 1640  loss: 0.33471\n",
      "echo: 1641  loss: 0.3347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 1642  loss: 0.3347\n",
      "echo: 1643  loss: 0.33469\n",
      "echo: 1644  loss: 0.33469\n",
      "echo: 1645  loss: 0.33468\n",
      "echo: 1646  loss: 0.33468\n",
      "echo: 1647  loss: 0.33467\n",
      "echo: 1648  loss: 0.33466\n",
      "echo: 1649  loss: 0.33466\n",
      "echo: 1650  loss: 0.33465\n",
      "echo: 1651  loss: 0.33465\n",
      "echo: 1652  loss: 0.33464\n",
      "echo: 1653  loss: 0.33464\n",
      "echo: 1654  loss: 0.33463\n",
      "echo: 1655  loss: 0.33462\n",
      "echo: 1656  loss: 0.33462\n",
      "echo: 1657  loss: 0.33461\n",
      "echo: 1658  loss: 0.33461\n",
      "echo: 1659  loss: 0.3346\n",
      "echo: 1660  loss: 0.3346\n",
      "echo: 1661  loss: 0.33459\n",
      "echo: 1662  loss: 0.33458\n",
      "echo: 1663  loss: 0.33458\n",
      "echo: 1664  loss: 0.33457\n",
      "echo: 1665  loss: 0.33457\n",
      "echo: 1666  loss: 0.33456\n",
      "echo: 1667  loss: 0.33456\n",
      "echo: 1668  loss: 0.33455\n",
      "echo: 1669  loss: 0.33454\n",
      "echo: 1670  loss: 0.33454\n",
      "echo: 1671  loss: 0.33453\n",
      "echo: 1672  loss: 0.33453\n",
      "echo: 1673  loss: 0.33452\n",
      "echo: 1674  loss: 0.33452\n",
      "echo: 1675  loss: 0.33451\n",
      "echo: 1676  loss: 0.33451\n",
      "echo: 1677  loss: 0.3345\n",
      "echo: 1678  loss: 0.33449\n",
      "echo: 1679  loss: 0.33449\n",
      "echo: 1680  loss: 0.33448\n",
      "echo: 1681  loss: 0.33448\n",
      "echo: 1682  loss: 0.33447\n",
      "echo: 1683  loss: 0.33447\n",
      "echo: 1684  loss: 0.33446\n",
      "echo: 1685  loss: 0.33446\n",
      "echo: 1686  loss: 0.33445\n",
      "echo: 1687  loss: 0.33445\n",
      "echo: 1688  loss: 0.33444\n",
      "echo: 1689  loss: 0.33443\n",
      "echo: 1690  loss: 0.33443\n",
      "echo: 1691  loss: 0.33442\n",
      "echo: 1692  loss: 0.33442\n",
      "echo: 1693  loss: 0.33441\n",
      "echo: 1694  loss: 0.33441\n",
      "echo: 1695  loss: 0.3344\n",
      "echo: 1696  loss: 0.3344\n",
      "echo: 1697  loss: 0.33439\n",
      "echo: 1698  loss: 0.33439\n",
      "echo: 1699  loss: 0.33438\n",
      "echo: 1700  loss: 0.33438\n",
      "echo: 1701  loss: 0.33437\n",
      "echo: 1702  loss: 0.33436\n",
      "echo: 1703  loss: 0.33436\n",
      "echo: 1704  loss: 0.33435\n",
      "echo: 1705  loss: 0.33435\n",
      "echo: 1706  loss: 0.33434\n",
      "echo: 1707  loss: 0.33434\n",
      "echo: 1708  loss: 0.33433\n",
      "echo: 1709  loss: 0.33433\n",
      "echo: 1710  loss: 0.33432\n",
      "echo: 1711  loss: 0.33432\n",
      "echo: 1712  loss: 0.33431\n",
      "echo: 1713  loss: 0.33431\n",
      "echo: 1714  loss: 0.3343\n",
      "echo: 1715  loss: 0.3343\n",
      "echo: 1716  loss: 0.33429\n",
      "echo: 1717  loss: 0.33429\n",
      "echo: 1718  loss: 0.33428\n",
      "echo: 1719  loss: 0.33428\n",
      "echo: 1720  loss: 0.33427\n",
      "echo: 1721  loss: 0.33427\n",
      "echo: 1722  loss: 0.33426\n",
      "echo: 1723  loss: 0.33425\n",
      "echo: 1724  loss: 0.33425\n",
      "echo: 1725  loss: 0.33424\n",
      "echo: 1726  loss: 0.33424\n",
      "echo: 1727  loss: 0.33423\n",
      "echo: 1728  loss: 0.33423\n",
      "echo: 1729  loss: 0.33422\n",
      "echo: 1730  loss: 0.33422\n",
      "echo: 1731  loss: 0.33421\n",
      "echo: 1732  loss: 0.33421\n",
      "echo: 1733  loss: 0.3342\n",
      "echo: 1734  loss: 0.3342\n",
      "echo: 1735  loss: 0.33419\n",
      "echo: 1736  loss: 0.33419\n",
      "echo: 1737  loss: 0.33418\n",
      "echo: 1738  loss: 0.33418\n",
      "echo: 1739  loss: 0.33417\n",
      "echo: 1740  loss: 0.33417\n",
      "echo: 1741  loss: 0.33416\n",
      "echo: 1742  loss: 0.33416\n",
      "echo: 1743  loss: 0.33415\n",
      "echo: 1744  loss: 0.33415\n",
      "echo: 1745  loss: 0.33414\n",
      "echo: 1746  loss: 0.33414\n",
      "echo: 1747  loss: 0.33413\n",
      "echo: 1748  loss: 0.33413\n",
      "echo: 1749  loss: 0.33412\n",
      "echo: 1750  loss: 0.33412\n",
      "echo: 1751  loss: 0.33411\n",
      "echo: 1752  loss: 0.33411\n",
      "echo: 1753  loss: 0.3341\n",
      "echo: 1754  loss: 0.3341\n",
      "echo: 1755  loss: 0.33409\n",
      "echo: 1756  loss: 0.33409\n",
      "echo: 1757  loss: 0.33408\n",
      "echo: 1758  loss: 0.33408\n",
      "echo: 1759  loss: 0.33407\n",
      "echo: 1760  loss: 0.33407\n",
      "echo: 1761  loss: 0.33406\n",
      "echo: 1762  loss: 0.33406\n",
      "echo: 1763  loss: 0.33405\n",
      "echo: 1764  loss: 0.33405\n",
      "echo: 1765  loss: 0.33404\n",
      "echo: 1766  loss: 0.33404\n",
      "echo: 1767  loss: 0.33403\n",
      "echo: 1768  loss: 0.33403\n",
      "echo: 1769  loss: 0.33403\n",
      "echo: 1770  loss: 0.33402\n",
      "echo: 1771  loss: 0.33402\n",
      "echo: 1772  loss: 0.33401\n",
      "echo: 1773  loss: 0.33401\n",
      "echo: 1774  loss: 0.334\n",
      "echo: 1775  loss: 0.334\n",
      "echo: 1776  loss: 0.33399\n",
      "echo: 1777  loss: 0.33399\n",
      "echo: 1778  loss: 0.33398\n",
      "echo: 1779  loss: 0.33398\n",
      "echo: 1780  loss: 0.33397\n",
      "echo: 1781  loss: 0.33397\n",
      "echo: 1782  loss: 0.33396\n",
      "echo: 1783  loss: 0.33396\n",
      "echo: 1784  loss: 0.33395\n",
      "echo: 1785  loss: 0.33395\n",
      "echo: 1786  loss: 0.33394\n",
      "echo: 1787  loss: 0.33394\n",
      "echo: 1788  loss: 0.33393\n",
      "echo: 1789  loss: 0.33393\n",
      "echo: 1790  loss: 0.33393\n",
      "echo: 1791  loss: 0.33392\n",
      "echo: 1792  loss: 0.33392\n",
      "echo: 1793  loss: 0.33391\n",
      "echo: 1794  loss: 0.33391\n",
      "echo: 1795  loss: 0.3339\n",
      "echo: 1796  loss: 0.3339\n",
      "echo: 1797  loss: 0.33389\n",
      "echo: 1798  loss: 0.33389\n",
      "echo: 1799  loss: 0.33388\n",
      "echo: 1800  loss: 0.33388\n",
      "echo: 1801  loss: 0.33387\n",
      "echo: 1802  loss: 0.33387\n",
      "echo: 1803  loss: 0.33386\n",
      "echo: 1804  loss: 0.33386\n",
      "echo: 1805  loss: 0.33386\n",
      "echo: 1806  loss: 0.33385\n",
      "echo: 1807  loss: 0.33385\n",
      "echo: 1808  loss: 0.33384\n",
      "echo: 1809  loss: 0.33384\n",
      "echo: 1810  loss: 0.33383\n",
      "echo: 1811  loss: 0.33383\n",
      "echo: 1812  loss: 0.33382\n",
      "echo: 1813  loss: 0.33382\n",
      "echo: 1814  loss: 0.33381\n",
      "echo: 1815  loss: 0.33381\n",
      "echo: 1816  loss: 0.33381\n",
      "echo: 1817  loss: 0.3338\n",
      "echo: 1818  loss: 0.3338\n",
      "echo: 1819  loss: 0.33379\n",
      "echo: 1820  loss: 0.33379\n",
      "echo: 1821  loss: 0.33378\n",
      "echo: 1822  loss: 0.33378\n",
      "echo: 1823  loss: 0.33377\n",
      "echo: 1824  loss: 0.33377\n",
      "echo: 1825  loss: 0.33376\n",
      "echo: 1826  loss: 0.33376\n",
      "echo: 1827  loss: 0.33376\n",
      "echo: 1828  loss: 0.33375\n",
      "echo: 1829  loss: 0.33375\n",
      "echo: 1830  loss: 0.33374\n",
      "echo: 1831  loss: 0.33374\n",
      "echo: 1832  loss: 0.33373\n",
      "echo: 1833  loss: 0.33373\n",
      "echo: 1834  loss: 0.33372\n",
      "echo: 1835  loss: 0.33372\n",
      "echo: 1836  loss: 0.33372\n",
      "echo: 1837  loss: 0.33371\n",
      "echo: 1838  loss: 0.33371\n",
      "echo: 1839  loss: 0.3337\n",
      "echo: 1840  loss: 0.3337\n",
      "echo: 1841  loss: 0.33369\n",
      "echo: 1842  loss: 0.33369\n",
      "echo: 1843  loss: 0.33368\n",
      "echo: 1844  loss: 0.33368\n",
      "echo: 1845  loss: 0.33368\n",
      "echo: 1846  loss: 0.33367\n",
      "echo: 1847  loss: 0.33367\n",
      "echo: 1848  loss: 0.33366\n",
      "echo: 1849  loss: 0.33366\n",
      "echo: 1850  loss: 0.33365\n",
      "echo: 1851  loss: 0.33365\n",
      "echo: 1852  loss: 0.33365\n",
      "echo: 1853  loss: 0.33364\n",
      "echo: 1854  loss: 0.33364\n",
      "echo: 1855  loss: 0.33363\n",
      "echo: 1856  loss: 0.33363\n",
      "echo: 1857  loss: 0.33362\n",
      "echo: 1858  loss: 0.33362\n",
      "echo: 1859  loss: 0.33362\n",
      "echo: 1860  loss: 0.33361\n",
      "echo: 1861  loss: 0.33361\n",
      "echo: 1862  loss: 0.3336\n",
      "echo: 1863  loss: 0.3336\n",
      "echo: 1864  loss: 0.33359\n",
      "echo: 1865  loss: 0.33359\n",
      "echo: 1866  loss: 0.33359\n",
      "echo: 1867  loss: 0.33358\n",
      "echo: 1868  loss: 0.33358\n",
      "echo: 1869  loss: 0.33357\n",
      "echo: 1870  loss: 0.33357\n",
      "echo: 1871  loss: 0.33356\n",
      "echo: 1872  loss: 0.33356\n",
      "echo: 1873  loss: 0.33356\n",
      "echo: 1874  loss: 0.33355\n",
      "echo: 1875  loss: 0.33355\n",
      "echo: 1876  loss: 0.33354\n",
      "echo: 1877  loss: 0.33354\n",
      "echo: 1878  loss: 0.33353\n",
      "echo: 1879  loss: 0.33353\n",
      "echo: 1880  loss: 0.33353\n",
      "echo: 1881  loss: 0.33352\n",
      "echo: 1882  loss: 0.33352\n",
      "echo: 1883  loss: 0.33351\n",
      "echo: 1884  loss: 0.33351\n",
      "echo: 1885  loss: 0.33351\n",
      "echo: 1886  loss: 0.3335\n",
      "echo: 1887  loss: 0.3335\n",
      "echo: 1888  loss: 0.33349\n",
      "echo: 1889  loss: 0.33349\n",
      "echo: 1890  loss: 0.33348\n",
      "echo: 1891  loss: 0.33348\n",
      "echo: 1892  loss: 0.33348\n",
      "echo: 1893  loss: 0.33347\n",
      "echo: 1894  loss: 0.33347\n",
      "echo: 1895  loss: 0.33346\n",
      "echo: 1896  loss: 0.33346\n",
      "echo: 1897  loss: 0.33346\n",
      "echo: 1898  loss: 0.33345\n",
      "echo: 1899  loss: 0.33345\n",
      "echo: 1900  loss: 0.33344\n",
      "echo: 1901  loss: 0.33344\n",
      "echo: 1902  loss: 0.33344\n",
      "echo: 1903  loss: 0.33343\n",
      "echo: 1904  loss: 0.33343\n",
      "echo: 1905  loss: 0.33342\n",
      "echo: 1906  loss: 0.33342\n",
      "echo: 1907  loss: 0.33342\n",
      "echo: 1908  loss: 0.33341\n",
      "echo: 1909  loss: 0.33341\n",
      "echo: 1910  loss: 0.3334\n",
      "echo: 1911  loss: 0.3334\n",
      "echo: 1912  loss: 0.3334\n",
      "echo: 1913  loss: 0.33339\n",
      "echo: 1914  loss: 0.33339\n",
      "echo: 1915  loss: 0.33338\n",
      "echo: 1916  loss: 0.33338\n",
      "echo: 1917  loss: 0.33338\n",
      "echo: 1918  loss: 0.33337\n",
      "echo: 1919  loss: 0.33337\n",
      "echo: 1920  loss: 0.33336\n",
      "echo: 1921  loss: 0.33336\n",
      "echo: 1922  loss: 0.33336\n",
      "echo: 1923  loss: 0.33335\n",
      "echo: 1924  loss: 0.33335\n",
      "echo: 1925  loss: 0.33334\n",
      "echo: 1926  loss: 0.33334\n",
      "echo: 1927  loss: 0.33334\n",
      "echo: 1928  loss: 0.33333\n",
      "echo: 1929  loss: 0.33333\n",
      "echo: 1930  loss: 0.33332\n",
      "echo: 1931  loss: 0.33332\n",
      "echo: 1932  loss: 0.33332\n",
      "echo: 1933  loss: 0.33331\n",
      "echo: 1934  loss: 0.33331\n",
      "echo: 1935  loss: 0.3333\n",
      "echo: 1936  loss: 0.3333\n",
      "echo: 1937  loss: 0.3333\n",
      "echo: 1938  loss: 0.33329\n",
      "echo: 1939  loss: 0.33329\n",
      "echo: 1940  loss: 0.33328\n",
      "echo: 1941  loss: 0.33328\n",
      "echo: 1942  loss: 0.33328\n",
      "echo: 1943  loss: 0.33327\n",
      "echo: 1944  loss: 0.33327\n",
      "echo: 1945  loss: 0.33327\n",
      "echo: 1946  loss: 0.33326\n",
      "echo: 1947  loss: 0.33326\n",
      "echo: 1948  loss: 0.33325\n",
      "echo: 1949  loss: 0.33325\n",
      "echo: 1950  loss: 0.33325\n",
      "echo: 1951  loss: 0.33324\n",
      "echo: 1952  loss: 0.33324\n",
      "echo: 1953  loss: 0.33323\n",
      "echo: 1954  loss: 0.33323\n",
      "echo: 1955  loss: 0.33323\n",
      "echo: 1956  loss: 0.33322\n",
      "echo: 1957  loss: 0.33322\n",
      "echo: 1958  loss: 0.33322\n",
      "echo: 1959  loss: 0.33321\n",
      "echo: 1960  loss: 0.33321\n",
      "echo: 1961  loss: 0.3332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 1962  loss: 0.3332\n",
      "echo: 1963  loss: 0.3332\n",
      "echo: 1964  loss: 0.33319\n",
      "echo: 1965  loss: 0.33319\n",
      "echo: 1966  loss: 0.33319\n",
      "echo: 1967  loss: 0.33318\n",
      "echo: 1968  loss: 0.33318\n",
      "echo: 1969  loss: 0.33317\n",
      "echo: 1970  loss: 0.33317\n",
      "echo: 1971  loss: 0.33317\n",
      "echo: 1972  loss: 0.33316\n",
      "echo: 1973  loss: 0.33316\n",
      "echo: 1974  loss: 0.33316\n",
      "echo: 1975  loss: 0.33315\n",
      "echo: 1976  loss: 0.33315\n",
      "echo: 1977  loss: 0.33314\n",
      "echo: 1978  loss: 0.33314\n",
      "echo: 1979  loss: 0.33314\n",
      "echo: 1980  loss: 0.33313\n",
      "echo: 1981  loss: 0.33313\n",
      "echo: 1982  loss: 0.33313\n",
      "echo: 1983  loss: 0.33312\n",
      "echo: 1984  loss: 0.33312\n",
      "echo: 1985  loss: 0.33311\n",
      "echo: 1986  loss: 0.33311\n",
      "echo: 1987  loss: 0.33311\n",
      "echo: 1988  loss: 0.3331\n",
      "echo: 1989  loss: 0.3331\n",
      "echo: 1990  loss: 0.3331\n",
      "echo: 1991  loss: 0.33309\n",
      "echo: 1992  loss: 0.33309\n",
      "echo: 1993  loss: 0.33309\n",
      "echo: 1994  loss: 0.33308\n",
      "echo: 1995  loss: 0.33308\n",
      "echo: 1996  loss: 0.33307\n",
      "echo: 1997  loss: 0.33307\n",
      "echo: 1998  loss: 0.33307\n",
      "echo: 1999  loss: 0.33306\n",
      "echo: 2000  loss: 0.33306\n",
      "echo: 2001  loss: 0.33306\n",
      "echo: 2002  loss: 0.33305\n",
      "echo: 2003  loss: 0.33305\n",
      "echo: 2004  loss: 0.33305\n",
      "echo: 2005  loss: 0.33304\n",
      "echo: 2006  loss: 0.33304\n",
      "echo: 2007  loss: 0.33303\n",
      "echo: 2008  loss: 0.33303\n",
      "echo: 2009  loss: 0.33303\n",
      "echo: 2010  loss: 0.33302\n",
      "echo: 2011  loss: 0.33302\n",
      "echo: 2012  loss: 0.33302\n",
      "echo: 2013  loss: 0.33301\n",
      "echo: 2014  loss: 0.33301\n",
      "echo: 2015  loss: 0.33301\n",
      "echo: 2016  loss: 0.333\n",
      "echo: 2017  loss: 0.333\n",
      "echo: 2018  loss: 0.333\n",
      "echo: 2019  loss: 0.33299\n",
      "echo: 2020  loss: 0.33299\n",
      "echo: 2021  loss: 0.33298\n",
      "echo: 2022  loss: 0.33298\n",
      "echo: 2023  loss: 0.33298\n",
      "echo: 2024  loss: 0.33297\n",
      "echo: 2025  loss: 0.33297\n",
      "echo: 2026  loss: 0.33297\n",
      "echo: 2027  loss: 0.33296\n",
      "echo: 2028  loss: 0.33296\n",
      "echo: 2029  loss: 0.33296\n",
      "echo: 2030  loss: 0.33295\n",
      "echo: 2031  loss: 0.33295\n",
      "echo: 2032  loss: 0.33295\n",
      "echo: 2033  loss: 0.33294\n",
      "echo: 2034  loss: 0.33294\n",
      "echo: 2035  loss: 0.33294\n",
      "echo: 2036  loss: 0.33293\n",
      "echo: 2037  loss: 0.33293\n",
      "echo: 2038  loss: 0.33293\n",
      "echo: 2039  loss: 0.33292\n",
      "echo: 2040  loss: 0.33292\n",
      "echo: 2041  loss: 0.33291\n",
      "echo: 2042  loss: 0.33291\n",
      "echo: 2043  loss: 0.33291\n",
      "echo: 2044  loss: 0.3329\n",
      "echo: 2045  loss: 0.3329\n",
      "echo: 2046  loss: 0.3329\n",
      "echo: 2047  loss: 0.33289\n",
      "echo: 2048  loss: 0.33289\n",
      "echo: 2049  loss: 0.33289\n",
      "echo: 2050  loss: 0.33288\n",
      "echo: 2051  loss: 0.33288\n",
      "echo: 2052  loss: 0.33288\n",
      "echo: 2053  loss: 0.33287\n",
      "echo: 2054  loss: 0.33287\n",
      "echo: 2055  loss: 0.33287\n",
      "echo: 2056  loss: 0.33286\n",
      "echo: 2057  loss: 0.33286\n",
      "echo: 2058  loss: 0.33286\n",
      "echo: 2059  loss: 0.33285\n",
      "echo: 2060  loss: 0.33285\n",
      "echo: 2061  loss: 0.33285\n",
      "echo: 2062  loss: 0.33284\n",
      "echo: 2063  loss: 0.33284\n",
      "echo: 2064  loss: 0.33284\n",
      "echo: 2065  loss: 0.33283\n",
      "echo: 2066  loss: 0.33283\n",
      "echo: 2067  loss: 0.33283\n",
      "echo: 2068  loss: 0.33282\n",
      "echo: 2069  loss: 0.33282\n",
      "echo: 2070  loss: 0.33282\n",
      "echo: 2071  loss: 0.33281\n",
      "echo: 2072  loss: 0.33281\n",
      "echo: 2073  loss: 0.33281\n",
      "echo: 2074  loss: 0.3328\n",
      "echo: 2075  loss: 0.3328\n",
      "echo: 2076  loss: 0.3328\n",
      "echo: 2077  loss: 0.33279\n",
      "echo: 2078  loss: 0.33279\n",
      "echo: 2079  loss: 0.33279\n",
      "echo: 2080  loss: 0.33278\n",
      "echo: 2081  loss: 0.33278\n",
      "echo: 2082  loss: 0.33278\n",
      "echo: 2083  loss: 0.33277\n",
      "echo: 2084  loss: 0.33277\n",
      "echo: 2085  loss: 0.33277\n",
      "echo: 2086  loss: 0.33276\n",
      "echo: 2087  loss: 0.33276\n",
      "echo: 2088  loss: 0.33276\n",
      "echo: 2089  loss: 0.33275\n",
      "echo: 2090  loss: 0.33275\n",
      "echo: 2091  loss: 0.33275\n",
      "echo: 2092  loss: 0.33274\n",
      "echo: 2093  loss: 0.33274\n",
      "echo: 2094  loss: 0.33274\n",
      "echo: 2095  loss: 0.33273\n",
      "echo: 2096  loss: 0.33273\n",
      "echo: 2097  loss: 0.33273\n",
      "echo: 2098  loss: 0.33272\n",
      "echo: 2099  loss: 0.33272\n",
      "echo: 2100  loss: 0.33272\n",
      "echo: 2101  loss: 0.33271\n",
      "echo: 2102  loss: 0.33271\n",
      "echo: 2103  loss: 0.33271\n",
      "echo: 2104  loss: 0.3327\n",
      "echo: 2105  loss: 0.3327\n",
      "echo: 2106  loss: 0.3327\n",
      "echo: 2107  loss: 0.3327\n",
      "echo: 2108  loss: 0.33269\n",
      "echo: 2109  loss: 0.33269\n",
      "echo: 2110  loss: 0.33269\n",
      "echo: 2111  loss: 0.33268\n",
      "echo: 2112  loss: 0.33268\n",
      "echo: 2113  loss: 0.33268\n",
      "echo: 2114  loss: 0.33267\n",
      "echo: 2115  loss: 0.33267\n",
      "echo: 2116  loss: 0.33267\n",
      "echo: 2117  loss: 0.33266\n",
      "echo: 2118  loss: 0.33266\n",
      "echo: 2119  loss: 0.33266\n",
      "echo: 2120  loss: 0.33265\n",
      "echo: 2121  loss: 0.33265\n",
      "echo: 2122  loss: 0.33265\n",
      "echo: 2123  loss: 0.33264\n",
      "echo: 2124  loss: 0.33264\n",
      "echo: 2125  loss: 0.33264\n",
      "echo: 2126  loss: 0.33263\n",
      "echo: 2127  loss: 0.33263\n",
      "echo: 2128  loss: 0.33263\n",
      "echo: 2129  loss: 0.33263\n",
      "echo: 2130  loss: 0.33262\n",
      "echo: 2131  loss: 0.33262\n",
      "echo: 2132  loss: 0.33262\n",
      "echo: 2133  loss: 0.33261\n",
      "echo: 2134  loss: 0.33261\n",
      "echo: 2135  loss: 0.33261\n",
      "echo: 2136  loss: 0.3326\n",
      "echo: 2137  loss: 0.3326\n",
      "echo: 2138  loss: 0.3326\n",
      "echo: 2139  loss: 0.33259\n",
      "echo: 2140  loss: 0.33259\n",
      "echo: 2141  loss: 0.33259\n",
      "echo: 2142  loss: 0.33258\n",
      "echo: 2143  loss: 0.33258\n",
      "echo: 2144  loss: 0.33258\n",
      "echo: 2145  loss: 0.33258\n",
      "echo: 2146  loss: 0.33257\n",
      "echo: 2147  loss: 0.33257\n",
      "echo: 2148  loss: 0.33257\n",
      "echo: 2149  loss: 0.33256\n",
      "echo: 2150  loss: 0.33256\n",
      "echo: 2151  loss: 0.33256\n",
      "echo: 2152  loss: 0.33255\n",
      "echo: 2153  loss: 0.33255\n",
      "echo: 2154  loss: 0.33255\n",
      "echo: 2155  loss: 0.33254\n",
      "echo: 2156  loss: 0.33254\n",
      "echo: 2157  loss: 0.33254\n",
      "echo: 2158  loss: 0.33254\n",
      "echo: 2159  loss: 0.33253\n",
      "echo: 2160  loss: 0.33253\n",
      "echo: 2161  loss: 0.33253\n",
      "echo: 2162  loss: 0.33252\n",
      "echo: 2163  loss: 0.33252\n",
      "echo: 2164  loss: 0.33252\n",
      "echo: 2165  loss: 0.33251\n",
      "echo: 2166  loss: 0.33251\n",
      "echo: 2167  loss: 0.33251\n",
      "echo: 2168  loss: 0.3325\n",
      "echo: 2169  loss: 0.3325\n",
      "echo: 2170  loss: 0.3325\n",
      "echo: 2171  loss: 0.3325\n",
      "echo: 2172  loss: 0.33249\n",
      "echo: 2173  loss: 0.33249\n",
      "echo: 2174  loss: 0.33249\n",
      "echo: 2175  loss: 0.33248\n",
      "echo: 2176  loss: 0.33248\n",
      "echo: 2177  loss: 0.33248\n",
      "echo: 2178  loss: 0.33247\n",
      "echo: 2179  loss: 0.33247\n",
      "echo: 2180  loss: 0.33247\n",
      "echo: 2181  loss: 0.33247\n",
      "echo: 2182  loss: 0.33246\n",
      "echo: 2183  loss: 0.33246\n",
      "echo: 2184  loss: 0.33246\n",
      "echo: 2185  loss: 0.33245\n",
      "echo: 2186  loss: 0.33245\n",
      "echo: 2187  loss: 0.33245\n",
      "echo: 2188  loss: 0.33245\n",
      "echo: 2189  loss: 0.33244\n",
      "echo: 2190  loss: 0.33244\n",
      "echo: 2191  loss: 0.33244\n",
      "echo: 2192  loss: 0.33243\n",
      "echo: 2193  loss: 0.33243\n",
      "echo: 2194  loss: 0.33243\n",
      "echo: 2195  loss: 0.33242\n",
      "echo: 2196  loss: 0.33242\n",
      "echo: 2197  loss: 0.33242\n",
      "echo: 2198  loss: 0.33242\n",
      "echo: 2199  loss: 0.33241\n",
      "echo: 2200  loss: 0.33241\n",
      "echo: 2201  loss: 0.33241\n",
      "echo: 2202  loss: 0.3324\n",
      "echo: 2203  loss: 0.3324\n",
      "echo: 2204  loss: 0.3324\n",
      "echo: 2205  loss: 0.3324\n",
      "echo: 2206  loss: 0.33239\n",
      "echo: 2207  loss: 0.33239\n",
      "echo: 2208  loss: 0.33239\n",
      "echo: 2209  loss: 0.33238\n",
      "echo: 2210  loss: 0.33238\n",
      "echo: 2211  loss: 0.33238\n",
      "echo: 2212  loss: 0.33238\n",
      "echo: 2213  loss: 0.33237\n",
      "echo: 2214  loss: 0.33237\n",
      "echo: 2215  loss: 0.33237\n",
      "echo: 2216  loss: 0.33236\n",
      "echo: 2217  loss: 0.33236\n",
      "echo: 2218  loss: 0.33236\n",
      "echo: 2219  loss: 0.33235\n",
      "echo: 2220  loss: 0.33235\n",
      "echo: 2221  loss: 0.33235\n",
      "echo: 2222  loss: 0.33235\n",
      "echo: 2223  loss: 0.33234\n",
      "echo: 2224  loss: 0.33234\n",
      "echo: 2225  loss: 0.33234\n",
      "echo: 2226  loss: 0.33233\n",
      "echo: 2227  loss: 0.33233\n",
      "echo: 2228  loss: 0.33233\n",
      "echo: 2229  loss: 0.33233\n",
      "echo: 2230  loss: 0.33232\n",
      "echo: 2231  loss: 0.33232\n",
      "echo: 2232  loss: 0.33232\n",
      "echo: 2233  loss: 0.33231\n",
      "echo: 2234  loss: 0.33231\n",
      "echo: 2235  loss: 0.33231\n",
      "echo: 2236  loss: 0.33231\n",
      "echo: 2237  loss: 0.3323\n",
      "echo: 2238  loss: 0.3323\n",
      "echo: 2239  loss: 0.3323\n",
      "echo: 2240  loss: 0.3323\n",
      "echo: 2241  loss: 0.33229\n",
      "echo: 2242  loss: 0.33229\n",
      "echo: 2243  loss: 0.33229\n",
      "echo: 2244  loss: 0.33228\n",
      "echo: 2245  loss: 0.33228\n",
      "echo: 2246  loss: 0.33228\n",
      "echo: 2247  loss: 0.33228\n",
      "echo: 2248  loss: 0.33227\n",
      "echo: 2249  loss: 0.33227\n",
      "echo: 2250  loss: 0.33227\n",
      "echo: 2251  loss: 0.33226\n",
      "echo: 2252  loss: 0.33226\n",
      "echo: 2253  loss: 0.33226\n",
      "echo: 2254  loss: 0.33226\n",
      "echo: 2255  loss: 0.33225\n",
      "echo: 2256  loss: 0.33225\n",
      "echo: 2257  loss: 0.33225\n",
      "echo: 2258  loss: 0.33225\n",
      "echo: 2259  loss: 0.33224\n",
      "echo: 2260  loss: 0.33224\n",
      "echo: 2261  loss: 0.33224\n",
      "echo: 2262  loss: 0.33223\n",
      "echo: 2263  loss: 0.33223\n",
      "echo: 2264  loss: 0.33223\n",
      "echo: 2265  loss: 0.33223\n",
      "echo: 2266  loss: 0.33222\n",
      "echo: 2267  loss: 0.33222\n",
      "echo: 2268  loss: 0.33222\n",
      "echo: 2269  loss: 0.33221\n",
      "echo: 2270  loss: 0.33221\n",
      "echo: 2271  loss: 0.33221\n",
      "echo: 2272  loss: 0.33221\n",
      "echo: 2273  loss: 0.3322\n",
      "echo: 2274  loss: 0.3322\n",
      "echo: 2275  loss: 0.3322\n",
      "echo: 2276  loss: 0.3322\n",
      "echo: 2277  loss: 0.33219\n",
      "echo: 2278  loss: 0.33219\n",
      "echo: 2279  loss: 0.33219\n",
      "echo: 2280  loss: 0.33218\n",
      "echo: 2281  loss: 0.33218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 2282  loss: 0.33218\n",
      "echo: 2283  loss: 0.33218\n",
      "echo: 2284  loss: 0.33217\n",
      "echo: 2285  loss: 0.33217\n",
      "echo: 2286  loss: 0.33217\n",
      "echo: 2287  loss: 0.33217\n",
      "echo: 2288  loss: 0.33216\n",
      "echo: 2289  loss: 0.33216\n",
      "echo: 2290  loss: 0.33216\n",
      "echo: 2291  loss: 0.33216\n",
      "echo: 2292  loss: 0.33215\n",
      "echo: 2293  loss: 0.33215\n",
      "echo: 2294  loss: 0.33215\n",
      "echo: 2295  loss: 0.33214\n",
      "echo: 2296  loss: 0.33214\n",
      "echo: 2297  loss: 0.33214\n",
      "echo: 2298  loss: 0.33214\n",
      "echo: 2299  loss: 0.33213\n",
      "echo: 2300  loss: 0.33213\n",
      "echo: 2301  loss: 0.33213\n",
      "echo: 2302  loss: 0.33213\n",
      "echo: 2303  loss: 0.33212\n",
      "echo: 2304  loss: 0.33212\n",
      "echo: 2305  loss: 0.33212\n",
      "echo: 2306  loss: 0.33212\n",
      "echo: 2307  loss: 0.33211\n",
      "echo: 2308  loss: 0.33211\n",
      "echo: 2309  loss: 0.33211\n",
      "echo: 2310  loss: 0.3321\n",
      "echo: 2311  loss: 0.3321\n",
      "echo: 2312  loss: 0.3321\n",
      "echo: 2313  loss: 0.3321\n",
      "echo: 2314  loss: 0.33209\n",
      "echo: 2315  loss: 0.33209\n",
      "echo: 2316  loss: 0.33209\n",
      "echo: 2317  loss: 0.33209\n",
      "echo: 2318  loss: 0.33208\n",
      "echo: 2319  loss: 0.33208\n",
      "echo: 2320  loss: 0.33208\n",
      "echo: 2321  loss: 0.33208\n",
      "echo: 2322  loss: 0.33207\n",
      "echo: 2323  loss: 0.33207\n",
      "echo: 2324  loss: 0.33207\n",
      "echo: 2325  loss: 0.33207\n",
      "echo: 2326  loss: 0.33206\n",
      "echo: 2327  loss: 0.33206\n",
      "echo: 2328  loss: 0.33206\n",
      "echo: 2329  loss: 0.33206\n",
      "echo: 2330  loss: 0.33205\n",
      "echo: 2331  loss: 0.33205\n",
      "echo: 2332  loss: 0.33205\n",
      "echo: 2333  loss: 0.33204\n",
      "echo: 2334  loss: 0.33204\n",
      "echo: 2335  loss: 0.33204\n",
      "echo: 2336  loss: 0.33204\n",
      "echo: 2337  loss: 0.33203\n",
      "echo: 2338  loss: 0.33203\n",
      "echo: 2339  loss: 0.33203\n",
      "echo: 2340  loss: 0.33203\n",
      "echo: 2341  loss: 0.33202\n",
      "echo: 2342  loss: 0.33202\n",
      "echo: 2343  loss: 0.33202\n",
      "echo: 2344  loss: 0.33202\n",
      "echo: 2345  loss: 0.33201\n",
      "echo: 2346  loss: 0.33201\n",
      "echo: 2347  loss: 0.33201\n",
      "echo: 2348  loss: 0.33201\n",
      "echo: 2349  loss: 0.332\n",
      "echo: 2350  loss: 0.332\n",
      "echo: 2351  loss: 0.332\n",
      "echo: 2352  loss: 0.332\n",
      "echo: 2353  loss: 0.33199\n",
      "echo: 2354  loss: 0.33199\n",
      "echo: 2355  loss: 0.33199\n",
      "echo: 2356  loss: 0.33199\n",
      "echo: 2357  loss: 0.33198\n",
      "echo: 2358  loss: 0.33198\n",
      "echo: 2359  loss: 0.33198\n",
      "echo: 2360  loss: 0.33198\n",
      "echo: 2361  loss: 0.33197\n",
      "echo: 2362  loss: 0.33197\n",
      "echo: 2363  loss: 0.33197\n",
      "echo: 2364  loss: 0.33197\n",
      "echo: 2365  loss: 0.33196\n",
      "echo: 2366  loss: 0.33196\n",
      "echo: 2367  loss: 0.33196\n",
      "echo: 2368  loss: 0.33196\n",
      "echo: 2369  loss: 0.33195\n",
      "echo: 2370  loss: 0.33195\n",
      "echo: 2371  loss: 0.33195\n",
      "echo: 2372  loss: 0.33195\n",
      "echo: 2373  loss: 0.33194\n",
      "echo: 2374  loss: 0.33194\n",
      "echo: 2375  loss: 0.33194\n",
      "echo: 2376  loss: 0.33194\n",
      "echo: 2377  loss: 0.33193\n",
      "echo: 2378  loss: 0.33193\n",
      "echo: 2379  loss: 0.33193\n",
      "echo: 2380  loss: 0.33193\n",
      "echo: 2381  loss: 0.33192\n",
      "echo: 2382  loss: 0.33192\n",
      "echo: 2383  loss: 0.33192\n",
      "echo: 2384  loss: 0.33192\n",
      "echo: 2385  loss: 0.33191\n",
      "echo: 2386  loss: 0.33191\n",
      "echo: 2387  loss: 0.33191\n",
      "echo: 2388  loss: 0.33191\n",
      "echo: 2389  loss: 0.3319\n",
      "echo: 2390  loss: 0.3319\n",
      "echo: 2391  loss: 0.3319\n",
      "echo: 2392  loss: 0.3319\n",
      "echo: 2393  loss: 0.33189\n",
      "echo: 2394  loss: 0.33189\n",
      "echo: 2395  loss: 0.33189\n",
      "echo: 2396  loss: 0.33189\n",
      "echo: 2397  loss: 0.33188\n",
      "echo: 2398  loss: 0.33188\n",
      "echo: 2399  loss: 0.33188\n",
      "echo: 2400  loss: 0.33188\n",
      "echo: 2401  loss: 0.33187\n",
      "echo: 2402  loss: 0.33187\n",
      "echo: 2403  loss: 0.33187\n",
      "echo: 2404  loss: 0.33187\n",
      "echo: 2405  loss: 0.33187\n",
      "echo: 2406  loss: 0.33186\n",
      "echo: 2407  loss: 0.33186\n",
      "echo: 2408  loss: 0.33186\n",
      "echo: 2409  loss: 0.33186\n",
      "echo: 2410  loss: 0.33185\n",
      "echo: 2411  loss: 0.33185\n",
      "echo: 2412  loss: 0.33185\n",
      "echo: 2413  loss: 0.33185\n",
      "echo: 2414  loss: 0.33184\n",
      "echo: 2415  loss: 0.33184\n",
      "echo: 2416  loss: 0.33184\n",
      "echo: 2417  loss: 0.33184\n",
      "echo: 2418  loss: 0.33183\n",
      "echo: 2419  loss: 0.33183\n",
      "echo: 2420  loss: 0.33183\n",
      "echo: 2421  loss: 0.33183\n",
      "echo: 2422  loss: 0.33182\n",
      "echo: 2423  loss: 0.33182\n",
      "echo: 2424  loss: 0.33182\n",
      "echo: 2425  loss: 0.33182\n",
      "echo: 2426  loss: 0.33181\n",
      "echo: 2427  loss: 0.33181\n",
      "echo: 2428  loss: 0.33181\n",
      "echo: 2429  loss: 0.33181\n",
      "echo: 2430  loss: 0.33181\n",
      "echo: 2431  loss: 0.3318\n",
      "echo: 2432  loss: 0.3318\n",
      "echo: 2433  loss: 0.3318\n",
      "echo: 2434  loss: 0.3318\n",
      "echo: 2435  loss: 0.33179\n",
      "echo: 2436  loss: 0.33179\n",
      "echo: 2437  loss: 0.33179\n",
      "echo: 2438  loss: 0.33179\n",
      "echo: 2439  loss: 0.33178\n",
      "echo: 2440  loss: 0.33178\n",
      "echo: 2441  loss: 0.33178\n",
      "echo: 2442  loss: 0.33178\n",
      "echo: 2443  loss: 0.33177\n",
      "echo: 2444  loss: 0.33177\n",
      "echo: 2445  loss: 0.33177\n",
      "echo: 2446  loss: 0.33177\n",
      "echo: 2447  loss: 0.33177\n",
      "echo: 2448  loss: 0.33176\n",
      "echo: 2449  loss: 0.33176\n",
      "echo: 2450  loss: 0.33176\n",
      "echo: 2451  loss: 0.33176\n",
      "echo: 2452  loss: 0.33175\n",
      "echo: 2453  loss: 0.33175\n",
      "echo: 2454  loss: 0.33175\n",
      "echo: 2455  loss: 0.33175\n",
      "echo: 2456  loss: 0.33174\n",
      "echo: 2457  loss: 0.33174\n",
      "echo: 2458  loss: 0.33174\n",
      "echo: 2459  loss: 0.33174\n",
      "echo: 2460  loss: 0.33174\n",
      "echo: 2461  loss: 0.33173\n",
      "echo: 2462  loss: 0.33173\n",
      "echo: 2463  loss: 0.33173\n",
      "echo: 2464  loss: 0.33173\n",
      "echo: 2465  loss: 0.33172\n",
      "echo: 2466  loss: 0.33172\n",
      "echo: 2467  loss: 0.33172\n",
      "echo: 2468  loss: 0.33172\n",
      "echo: 2469  loss: 0.33171\n",
      "echo: 2470  loss: 0.33171\n",
      "echo: 2471  loss: 0.33171\n",
      "echo: 2472  loss: 0.33171\n",
      "echo: 2473  loss: 0.33171\n",
      "echo: 2474  loss: 0.3317\n",
      "echo: 2475  loss: 0.3317\n",
      "echo: 2476  loss: 0.3317\n",
      "echo: 2477  loss: 0.3317\n",
      "echo: 2478  loss: 0.33169\n",
      "echo: 2479  loss: 0.33169\n",
      "echo: 2480  loss: 0.33169\n",
      "echo: 2481  loss: 0.33169\n",
      "echo: 2482  loss: 0.33168\n",
      "echo: 2483  loss: 0.33168\n",
      "echo: 2484  loss: 0.33168\n",
      "echo: 2485  loss: 0.33168\n",
      "echo: 2486  loss: 0.33168\n",
      "echo: 2487  loss: 0.33167\n",
      "echo: 2488  loss: 0.33167\n",
      "echo: 2489  loss: 0.33167\n",
      "echo: 2490  loss: 0.33167\n",
      "echo: 2491  loss: 0.33166\n",
      "echo: 2492  loss: 0.33166\n",
      "echo: 2493  loss: 0.33166\n",
      "echo: 2494  loss: 0.33166\n",
      "echo: 2495  loss: 0.33166\n",
      "echo: 2496  loss: 0.33165\n",
      "echo: 2497  loss: 0.33165\n",
      "echo: 2498  loss: 0.33165\n",
      "echo: 2499  loss: 0.33165\n",
      "echo: 2500  loss: 0.33164\n",
      "echo: 2501  loss: 0.33164\n",
      "echo: 2502  loss: 0.33164\n",
      "echo: 2503  loss: 0.33164\n",
      "echo: 2504  loss: 0.33164\n",
      "echo: 2505  loss: 0.33163\n",
      "echo: 2506  loss: 0.33163\n",
      "echo: 2507  loss: 0.33163\n",
      "echo: 2508  loss: 0.33163\n",
      "echo: 2509  loss: 0.33162\n",
      "echo: 2510  loss: 0.33162\n",
      "echo: 2511  loss: 0.33162\n",
      "echo: 2512  loss: 0.33162\n",
      "echo: 2513  loss: 0.33162\n",
      "echo: 2514  loss: 0.33161\n",
      "echo: 2515  loss: 0.33161\n",
      "echo: 2516  loss: 0.33161\n",
      "echo: 2517  loss: 0.33161\n",
      "echo: 2518  loss: 0.3316\n",
      "echo: 2519  loss: 0.3316\n",
      "echo: 2520  loss: 0.3316\n",
      "echo: 2521  loss: 0.3316\n",
      "echo: 2522  loss: 0.3316\n",
      "echo: 2523  loss: 0.33159\n",
      "echo: 2524  loss: 0.33159\n",
      "echo: 2525  loss: 0.33159\n",
      "echo: 2526  loss: 0.33159\n",
      "echo: 2527  loss: 0.33158\n",
      "echo: 2528  loss: 0.33158\n",
      "echo: 2529  loss: 0.33158\n",
      "echo: 2530  loss: 0.33158\n",
      "echo: 2531  loss: 0.33158\n",
      "echo: 2532  loss: 0.33157\n",
      "echo: 2533  loss: 0.33157\n",
      "echo: 2534  loss: 0.33157\n",
      "echo: 2535  loss: 0.33157\n",
      "echo: 2536  loss: 0.33157\n",
      "echo: 2537  loss: 0.33156\n",
      "echo: 2538  loss: 0.33156\n",
      "echo: 2539  loss: 0.33156\n",
      "echo: 2540  loss: 0.33156\n",
      "echo: 2541  loss: 0.33155\n",
      "echo: 2542  loss: 0.33155\n",
      "echo: 2543  loss: 0.33155\n",
      "echo: 2544  loss: 0.33155\n",
      "echo: 2545  loss: 0.33155\n",
      "echo: 2546  loss: 0.33154\n",
      "echo: 2547  loss: 0.33154\n",
      "echo: 2548  loss: 0.33154\n",
      "echo: 2549  loss: 0.33154\n",
      "echo: 2550  loss: 0.33154\n",
      "echo: 2551  loss: 0.33153\n",
      "echo: 2552  loss: 0.33153\n",
      "echo: 2553  loss: 0.33153\n",
      "echo: 2554  loss: 0.33153\n",
      "echo: 2555  loss: 0.33152\n",
      "echo: 2556  loss: 0.33152\n",
      "echo: 2557  loss: 0.33152\n",
      "echo: 2558  loss: 0.33152\n",
      "echo: 2559  loss: 0.33152\n",
      "echo: 2560  loss: 0.33151\n",
      "echo: 2561  loss: 0.33151\n",
      "echo: 2562  loss: 0.33151\n",
      "echo: 2563  loss: 0.33151\n",
      "echo: 2564  loss: 0.33151\n",
      "echo: 2565  loss: 0.3315\n",
      "echo: 2566  loss: 0.3315\n",
      "echo: 2567  loss: 0.3315\n",
      "echo: 2568  loss: 0.3315\n",
      "echo: 2569  loss: 0.33149\n",
      "echo: 2570  loss: 0.33149\n",
      "echo: 2571  loss: 0.33149\n",
      "echo: 2572  loss: 0.33149\n",
      "echo: 2573  loss: 0.33149\n",
      "echo: 2574  loss: 0.33148\n",
      "echo: 2575  loss: 0.33148\n",
      "echo: 2576  loss: 0.33148\n",
      "echo: 2577  loss: 0.33148\n",
      "echo: 2578  loss: 0.33148\n",
      "echo: 2579  loss: 0.33147\n",
      "echo: 2580  loss: 0.33147\n",
      "echo: 2581  loss: 0.33147\n",
      "echo: 2582  loss: 0.33147\n",
      "echo: 2583  loss: 0.33147\n",
      "echo: 2584  loss: 0.33146\n",
      "echo: 2585  loss: 0.33146\n",
      "echo: 2586  loss: 0.33146\n",
      "echo: 2587  loss: 0.33146\n",
      "echo: 2588  loss: 0.33145\n",
      "echo: 2589  loss: 0.33145\n",
      "echo: 2590  loss: 0.33145\n",
      "echo: 2591  loss: 0.33145\n",
      "echo: 2592  loss: 0.33145\n",
      "echo: 2593  loss: 0.33144\n",
      "echo: 2594  loss: 0.33144\n",
      "echo: 2595  loss: 0.33144\n",
      "echo: 2596  loss: 0.33144\n",
      "echo: 2597  loss: 0.33144\n",
      "echo: 2598  loss: 0.33143\n",
      "echo: 2599  loss: 0.33143\n",
      "echo: 2600  loss: 0.33143\n",
      "echo: 2601  loss: 0.33143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 2602  loss: 0.33143\n",
      "echo: 2603  loss: 0.33142\n",
      "echo: 2604  loss: 0.33142\n",
      "echo: 2605  loss: 0.33142\n",
      "echo: 2606  loss: 0.33142\n",
      "echo: 2607  loss: 0.33142\n",
      "echo: 2608  loss: 0.33141\n",
      "echo: 2609  loss: 0.33141\n",
      "echo: 2610  loss: 0.33141\n",
      "echo: 2611  loss: 0.33141\n",
      "echo: 2612  loss: 0.33141\n",
      "echo: 2613  loss: 0.3314\n",
      "echo: 2614  loss: 0.3314\n",
      "echo: 2615  loss: 0.3314\n",
      "echo: 2616  loss: 0.3314\n",
      "echo: 2617  loss: 0.3314\n",
      "echo: 2618  loss: 0.33139\n",
      "echo: 2619  loss: 0.33139\n",
      "echo: 2620  loss: 0.33139\n",
      "echo: 2621  loss: 0.33139\n",
      "echo: 2622  loss: 0.33139\n",
      "echo: 2623  loss: 0.33138\n",
      "echo: 2624  loss: 0.33138\n",
      "echo: 2625  loss: 0.33138\n",
      "echo: 2626  loss: 0.33138\n",
      "echo: 2627  loss: 0.33137\n",
      "echo: 2628  loss: 0.33137\n",
      "echo: 2629  loss: 0.33137\n",
      "echo: 2630  loss: 0.33137\n",
      "echo: 2631  loss: 0.33137\n",
      "echo: 2632  loss: 0.33136\n",
      "echo: 2633  loss: 0.33136\n",
      "echo: 2634  loss: 0.33136\n",
      "echo: 2635  loss: 0.33136\n",
      "echo: 2636  loss: 0.33136\n",
      "echo: 2637  loss: 0.33135\n",
      "echo: 2638  loss: 0.33135\n",
      "echo: 2639  loss: 0.33135\n",
      "echo: 2640  loss: 0.33135\n",
      "echo: 2641  loss: 0.33135\n",
      "echo: 2642  loss: 0.33134\n",
      "echo: 2643  loss: 0.33134\n",
      "echo: 2644  loss: 0.33134\n",
      "echo: 2645  loss: 0.33134\n",
      "echo: 2646  loss: 0.33134\n",
      "echo: 2647  loss: 0.33133\n",
      "echo: 2648  loss: 0.33133\n",
      "echo: 2649  loss: 0.33133\n",
      "echo: 2650  loss: 0.33133\n",
      "echo: 2651  loss: 0.33133\n",
      "echo: 2652  loss: 0.33132\n",
      "echo: 2653  loss: 0.33132\n",
      "echo: 2654  loss: 0.33132\n",
      "echo: 2655  loss: 0.33132\n",
      "echo: 2656  loss: 0.33132\n",
      "echo: 2657  loss: 0.33132\n",
      "echo: 2658  loss: 0.33131\n",
      "echo: 2659  loss: 0.33131\n",
      "echo: 2660  loss: 0.33131\n",
      "echo: 2661  loss: 0.33131\n",
      "echo: 2662  loss: 0.33131\n",
      "echo: 2663  loss: 0.3313\n",
      "echo: 2664  loss: 0.3313\n",
      "echo: 2665  loss: 0.3313\n",
      "echo: 2666  loss: 0.3313\n",
      "echo: 2667  loss: 0.3313\n",
      "echo: 2668  loss: 0.33129\n",
      "echo: 2669  loss: 0.33129\n",
      "echo: 2670  loss: 0.33129\n",
      "echo: 2671  loss: 0.33129\n",
      "echo: 2672  loss: 0.33129\n",
      "echo: 2673  loss: 0.33128\n",
      "echo: 2674  loss: 0.33128\n",
      "echo: 2675  loss: 0.33128\n",
      "echo: 2676  loss: 0.33128\n",
      "echo: 2677  loss: 0.33128\n",
      "echo: 2678  loss: 0.33127\n",
      "echo: 2679  loss: 0.33127\n",
      "echo: 2680  loss: 0.33127\n",
      "echo: 2681  loss: 0.33127\n",
      "echo: 2682  loss: 0.33127\n",
      "echo: 2683  loss: 0.33126\n",
      "echo: 2684  loss: 0.33126\n",
      "echo: 2685  loss: 0.33126\n",
      "echo: 2686  loss: 0.33126\n",
      "echo: 2687  loss: 0.33126\n",
      "echo: 2688  loss: 0.33125\n",
      "echo: 2689  loss: 0.33125\n",
      "echo: 2690  loss: 0.33125\n",
      "echo: 2691  loss: 0.33125\n",
      "echo: 2692  loss: 0.33125\n",
      "echo: 2693  loss: 0.33125\n",
      "echo: 2694  loss: 0.33124\n",
      "echo: 2695  loss: 0.33124\n",
      "echo: 2696  loss: 0.33124\n",
      "echo: 2697  loss: 0.33124\n",
      "echo: 2698  loss: 0.33124\n",
      "echo: 2699  loss: 0.33123\n",
      "echo: 2700  loss: 0.33123\n",
      "echo: 2701  loss: 0.33123\n",
      "echo: 2702  loss: 0.33123\n",
      "echo: 2703  loss: 0.33123\n",
      "echo: 2704  loss: 0.33122\n",
      "echo: 2705  loss: 0.33122\n",
      "echo: 2706  loss: 0.33122\n",
      "echo: 2707  loss: 0.33122\n",
      "echo: 2708  loss: 0.33122\n",
      "echo: 2709  loss: 0.33121\n",
      "echo: 2710  loss: 0.33121\n",
      "echo: 2711  loss: 0.33121\n",
      "echo: 2712  loss: 0.33121\n",
      "echo: 2713  loss: 0.33121\n",
      "echo: 2714  loss: 0.33121\n",
      "echo: 2715  loss: 0.3312\n",
      "echo: 2716  loss: 0.3312\n",
      "echo: 2717  loss: 0.3312\n",
      "echo: 2718  loss: 0.3312\n",
      "echo: 2719  loss: 0.3312\n",
      "echo: 2720  loss: 0.33119\n",
      "echo: 2721  loss: 0.33119\n",
      "echo: 2722  loss: 0.33119\n",
      "echo: 2723  loss: 0.33119\n",
      "echo: 2724  loss: 0.33119\n",
      "echo: 2725  loss: 0.33118\n",
      "echo: 2726  loss: 0.33118\n",
      "echo: 2727  loss: 0.33118\n",
      "echo: 2728  loss: 0.33118\n",
      "echo: 2729  loss: 0.33118\n",
      "echo: 2730  loss: 0.33118\n",
      "echo: 2731  loss: 0.33117\n",
      "echo: 2732  loss: 0.33117\n",
      "echo: 2733  loss: 0.33117\n",
      "echo: 2734  loss: 0.33117\n",
      "echo: 2735  loss: 0.33117\n",
      "echo: 2736  loss: 0.33116\n",
      "echo: 2737  loss: 0.33116\n",
      "echo: 2738  loss: 0.33116\n",
      "echo: 2739  loss: 0.33116\n",
      "echo: 2740  loss: 0.33116\n",
      "echo: 2741  loss: 0.33115\n",
      "echo: 2742  loss: 0.33115\n",
      "echo: 2743  loss: 0.33115\n",
      "echo: 2744  loss: 0.33115\n",
      "echo: 2745  loss: 0.33115\n",
      "echo: 2746  loss: 0.33115\n",
      "echo: 2747  loss: 0.33114\n",
      "echo: 2748  loss: 0.33114\n",
      "echo: 2749  loss: 0.33114\n",
      "echo: 2750  loss: 0.33114\n",
      "echo: 2751  loss: 0.33114\n",
      "echo: 2752  loss: 0.33113\n",
      "echo: 2753  loss: 0.33113\n",
      "echo: 2754  loss: 0.33113\n",
      "echo: 2755  loss: 0.33113\n",
      "echo: 2756  loss: 0.33113\n",
      "echo: 2757  loss: 0.33113\n",
      "echo: 2758  loss: 0.33112\n",
      "echo: 2759  loss: 0.33112\n",
      "echo: 2760  loss: 0.33112\n",
      "echo: 2761  loss: 0.33112\n",
      "echo: 2762  loss: 0.33112\n",
      "echo: 2763  loss: 0.33111\n",
      "echo: 2764  loss: 0.33111\n",
      "echo: 2765  loss: 0.33111\n",
      "echo: 2766  loss: 0.33111\n",
      "echo: 2767  loss: 0.33111\n",
      "echo: 2768  loss: 0.33111\n",
      "echo: 2769  loss: 0.3311\n",
      "echo: 2770  loss: 0.3311\n",
      "echo: 2771  loss: 0.3311\n",
      "echo: 2772  loss: 0.3311\n",
      "echo: 2773  loss: 0.3311\n",
      "echo: 2774  loss: 0.33109\n",
      "echo: 2775  loss: 0.33109\n",
      "echo: 2776  loss: 0.33109\n",
      "echo: 2777  loss: 0.33109\n",
      "echo: 2778  loss: 0.33109\n",
      "echo: 2779  loss: 0.33109\n",
      "echo: 2780  loss: 0.33108\n",
      "echo: 2781  loss: 0.33108\n",
      "echo: 2782  loss: 0.33108\n",
      "echo: 2783  loss: 0.33108\n",
      "echo: 2784  loss: 0.33108\n",
      "echo: 2785  loss: 0.33107\n",
      "echo: 2786  loss: 0.33107\n",
      "echo: 2787  loss: 0.33107\n",
      "echo: 2788  loss: 0.33107\n",
      "echo: 2789  loss: 0.33107\n",
      "echo: 2790  loss: 0.33107\n",
      "echo: 2791  loss: 0.33106\n",
      "echo: 2792  loss: 0.33106\n",
      "echo: 2793  loss: 0.33106\n",
      "echo: 2794  loss: 0.33106\n",
      "echo: 2795  loss: 0.33106\n",
      "echo: 2796  loss: 0.33105\n",
      "echo: 2797  loss: 0.33105\n",
      "echo: 2798  loss: 0.33105\n",
      "echo: 2799  loss: 0.33105\n",
      "echo: 2800  loss: 0.33105\n",
      "echo: 2801  loss: 0.33105\n",
      "echo: 2802  loss: 0.33104\n",
      "echo: 2803  loss: 0.33104\n",
      "echo: 2804  loss: 0.33104\n",
      "echo: 2805  loss: 0.33104\n",
      "echo: 2806  loss: 0.33104\n",
      "echo: 2807  loss: 0.33104\n",
      "echo: 2808  loss: 0.33103\n",
      "echo: 2809  loss: 0.33103\n",
      "echo: 2810  loss: 0.33103\n",
      "echo: 2811  loss: 0.33103\n",
      "echo: 2812  loss: 0.33103\n",
      "echo: 2813  loss: 0.33102\n",
      "echo: 2814  loss: 0.33102\n",
      "echo: 2815  loss: 0.33102\n",
      "echo: 2816  loss: 0.33102\n",
      "echo: 2817  loss: 0.33102\n",
      "echo: 2818  loss: 0.33102\n",
      "echo: 2819  loss: 0.33101\n",
      "echo: 2820  loss: 0.33101\n",
      "echo: 2821  loss: 0.33101\n",
      "echo: 2822  loss: 0.33101\n",
      "echo: 2823  loss: 0.33101\n",
      "echo: 2824  loss: 0.33101\n",
      "echo: 2825  loss: 0.331\n",
      "echo: 2826  loss: 0.331\n",
      "echo: 2827  loss: 0.331\n",
      "echo: 2828  loss: 0.331\n",
      "echo: 2829  loss: 0.331\n",
      "echo: 2830  loss: 0.331\n",
      "echo: 2831  loss: 0.33099\n",
      "echo: 2832  loss: 0.33099\n",
      "echo: 2833  loss: 0.33099\n",
      "echo: 2834  loss: 0.33099\n",
      "echo: 2835  loss: 0.33099\n",
      "echo: 2836  loss: 0.33098\n",
      "echo: 2837  loss: 0.33098\n",
      "echo: 2838  loss: 0.33098\n",
      "echo: 2839  loss: 0.33098\n",
      "echo: 2840  loss: 0.33098\n",
      "echo: 2841  loss: 0.33098\n",
      "echo: 2842  loss: 0.33097\n",
      "echo: 2843  loss: 0.33097\n",
      "echo: 2844  loss: 0.33097\n",
      "echo: 2845  loss: 0.33097\n",
      "echo: 2846  loss: 0.33097\n",
      "echo: 2847  loss: 0.33097\n",
      "echo: 2848  loss: 0.33096\n",
      "echo: 2849  loss: 0.33096\n",
      "echo: 2850  loss: 0.33096\n",
      "echo: 2851  loss: 0.33096\n",
      "echo: 2852  loss: 0.33096\n",
      "echo: 2853  loss: 0.33096\n",
      "echo: 2854  loss: 0.33095\n",
      "echo: 2855  loss: 0.33095\n",
      "echo: 2856  loss: 0.33095\n",
      "echo: 2857  loss: 0.33095\n",
      "echo: 2858  loss: 0.33095\n",
      "echo: 2859  loss: 0.33095\n",
      "echo: 2860  loss: 0.33094\n",
      "echo: 2861  loss: 0.33094\n",
      "echo: 2862  loss: 0.33094\n",
      "echo: 2863  loss: 0.33094\n",
      "echo: 2864  loss: 0.33094\n",
      "echo: 2865  loss: 0.33093\n",
      "echo: 2866  loss: 0.33093\n",
      "echo: 2867  loss: 0.33093\n",
      "echo: 2868  loss: 0.33093\n",
      "echo: 2869  loss: 0.33093\n",
      "echo: 2870  loss: 0.33093\n",
      "echo: 2871  loss: 0.33092\n",
      "echo: 2872  loss: 0.33092\n",
      "echo: 2873  loss: 0.33092\n",
      "echo: 2874  loss: 0.33092\n",
      "echo: 2875  loss: 0.33092\n",
      "echo: 2876  loss: 0.33092\n",
      "echo: 2877  loss: 0.33091\n",
      "echo: 2878  loss: 0.33091\n",
      "echo: 2879  loss: 0.33091\n",
      "echo: 2880  loss: 0.33091\n",
      "echo: 2881  loss: 0.33091\n",
      "echo: 2882  loss: 0.33091\n",
      "echo: 2883  loss: 0.3309\n",
      "echo: 2884  loss: 0.3309\n",
      "echo: 2885  loss: 0.3309\n",
      "echo: 2886  loss: 0.3309\n",
      "echo: 2887  loss: 0.3309\n",
      "echo: 2888  loss: 0.3309\n",
      "echo: 2889  loss: 0.33089\n",
      "echo: 2890  loss: 0.33089\n",
      "echo: 2891  loss: 0.33089\n",
      "echo: 2892  loss: 0.33089\n",
      "echo: 2893  loss: 0.33089\n",
      "echo: 2894  loss: 0.33089\n",
      "echo: 2895  loss: 0.33088\n",
      "echo: 2896  loss: 0.33088\n",
      "echo: 2897  loss: 0.33088\n",
      "echo: 2898  loss: 0.33088\n",
      "echo: 2899  loss: 0.33088\n",
      "echo: 2900  loss: 0.33088\n",
      "echo: 2901  loss: 0.33087\n",
      "echo: 2902  loss: 0.33087\n",
      "echo: 2903  loss: 0.33087\n",
      "echo: 2904  loss: 0.33087\n",
      "echo: 2905  loss: 0.33087\n",
      "echo: 2906  loss: 0.33087\n",
      "echo: 2907  loss: 0.33086\n",
      "echo: 2908  loss: 0.33086\n",
      "echo: 2909  loss: 0.33086\n",
      "echo: 2910  loss: 0.33086\n",
      "echo: 2911  loss: 0.33086\n",
      "echo: 2912  loss: 0.33086\n",
      "echo: 2913  loss: 0.33085\n",
      "echo: 2914  loss: 0.33085\n",
      "echo: 2915  loss: 0.33085\n",
      "echo: 2916  loss: 0.33085\n",
      "echo: 2917  loss: 0.33085\n",
      "echo: 2918  loss: 0.33085\n",
      "echo: 2919  loss: 0.33085\n",
      "echo: 2920  loss: 0.33084\n",
      "echo: 2921  loss: 0.33084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 2922  loss: 0.33084\n",
      "echo: 2923  loss: 0.33084\n",
      "echo: 2924  loss: 0.33084\n",
      "echo: 2925  loss: 0.33084\n",
      "echo: 2926  loss: 0.33083\n",
      "echo: 2927  loss: 0.33083\n",
      "echo: 2928  loss: 0.33083\n",
      "echo: 2929  loss: 0.33083\n",
      "echo: 2930  loss: 0.33083\n",
      "echo: 2931  loss: 0.33083\n",
      "echo: 2932  loss: 0.33082\n",
      "echo: 2933  loss: 0.33082\n",
      "echo: 2934  loss: 0.33082\n",
      "echo: 2935  loss: 0.33082\n",
      "echo: 2936  loss: 0.33082\n",
      "echo: 2937  loss: 0.33082\n",
      "echo: 2938  loss: 0.33081\n",
      "echo: 2939  loss: 0.33081\n",
      "echo: 2940  loss: 0.33081\n",
      "echo: 2941  loss: 0.33081\n",
      "echo: 2942  loss: 0.33081\n",
      "echo: 2943  loss: 0.33081\n",
      "echo: 2944  loss: 0.3308\n",
      "echo: 2945  loss: 0.3308\n",
      "echo: 2946  loss: 0.3308\n",
      "echo: 2947  loss: 0.3308\n",
      "echo: 2948  loss: 0.3308\n",
      "echo: 2949  loss: 0.3308\n",
      "echo: 2950  loss: 0.3308\n",
      "echo: 2951  loss: 0.33079\n",
      "echo: 2952  loss: 0.33079\n",
      "echo: 2953  loss: 0.33079\n",
      "echo: 2954  loss: 0.33079\n",
      "echo: 2955  loss: 0.33079\n",
      "echo: 2956  loss: 0.33079\n",
      "echo: 2957  loss: 0.33078\n",
      "echo: 2958  loss: 0.33078\n",
      "echo: 2959  loss: 0.33078\n",
      "echo: 2960  loss: 0.33078\n",
      "echo: 2961  loss: 0.33078\n",
      "echo: 2962  loss: 0.33078\n",
      "echo: 2963  loss: 0.33077\n",
      "echo: 2964  loss: 0.33077\n",
      "echo: 2965  loss: 0.33077\n",
      "echo: 2966  loss: 0.33077\n",
      "echo: 2967  loss: 0.33077\n",
      "echo: 2968  loss: 0.33077\n",
      "echo: 2969  loss: 0.33076\n",
      "echo: 2970  loss: 0.33076\n",
      "echo: 2971  loss: 0.33076\n",
      "echo: 2972  loss: 0.33076\n",
      "echo: 2973  loss: 0.33076\n",
      "echo: 2974  loss: 0.33076\n",
      "echo: 2975  loss: 0.33076\n",
      "echo: 2976  loss: 0.33075\n",
      "echo: 2977  loss: 0.33075\n",
      "echo: 2978  loss: 0.33075\n",
      "echo: 2979  loss: 0.33075\n",
      "echo: 2980  loss: 0.33075\n",
      "echo: 2981  loss: 0.33075\n",
      "echo: 2982  loss: 0.33074\n",
      "echo: 2983  loss: 0.33074\n",
      "echo: 2984  loss: 0.33074\n",
      "echo: 2985  loss: 0.33074\n",
      "echo: 2986  loss: 0.33074\n",
      "echo: 2987  loss: 0.33074\n",
      "echo: 2988  loss: 0.33074\n",
      "echo: 2989  loss: 0.33073\n",
      "echo: 2990  loss: 0.33073\n",
      "echo: 2991  loss: 0.33073\n",
      "echo: 2992  loss: 0.33073\n",
      "echo: 2993  loss: 0.33073\n",
      "echo: 2994  loss: 0.33073\n",
      "echo: 2995  loss: 0.33072\n",
      "echo: 2996  loss: 0.33072\n",
      "echo: 2997  loss: 0.33072\n",
      "echo: 2998  loss: 0.33072\n",
      "echo: 2999  loss: 0.33072\n",
      "echo: 3000  loss: 0.33072\n",
      "echo: 3001  loss: 0.33072\n",
      "echo: 3002  loss: 0.33071\n",
      "echo: 3003  loss: 0.33071\n",
      "echo: 3004  loss: 0.33071\n",
      "echo: 3005  loss: 0.33071\n",
      "echo: 3006  loss: 0.33071\n",
      "echo: 3007  loss: 0.33071\n",
      "echo: 3008  loss: 0.3307\n",
      "echo: 3009  loss: 0.3307\n",
      "echo: 3010  loss: 0.3307\n",
      "echo: 3011  loss: 0.3307\n",
      "echo: 3012  loss: 0.3307\n",
      "echo: 3013  loss: 0.3307\n",
      "echo: 3014  loss: 0.3307\n",
      "echo: 3015  loss: 0.33069\n",
      "echo: 3016  loss: 0.33069\n",
      "echo: 3017  loss: 0.33069\n",
      "echo: 3018  loss: 0.33069\n",
      "echo: 3019  loss: 0.33069\n",
      "echo: 3020  loss: 0.33069\n",
      "echo: 3021  loss: 0.33068\n",
      "echo: 3022  loss: 0.33068\n",
      "echo: 3023  loss: 0.33068\n",
      "echo: 3024  loss: 0.33068\n",
      "echo: 3025  loss: 0.33068\n",
      "echo: 3026  loss: 0.33068\n",
      "echo: 3027  loss: 0.33068\n",
      "echo: 3028  loss: 0.33067\n",
      "echo: 3029  loss: 0.33067\n",
      "echo: 3030  loss: 0.33067\n",
      "echo: 3031  loss: 0.33067\n",
      "echo: 3032  loss: 0.33067\n",
      "echo: 3033  loss: 0.33067\n",
      "echo: 3034  loss: 0.33066\n",
      "echo: 3035  loss: 0.33066\n",
      "echo: 3036  loss: 0.33066\n",
      "echo: 3037  loss: 0.33066\n",
      "echo: 3038  loss: 0.33066\n",
      "echo: 3039  loss: 0.33066\n",
      "echo: 3040  loss: 0.33066\n",
      "echo: 3041  loss: 0.33065\n",
      "echo: 3042  loss: 0.33065\n",
      "echo: 3043  loss: 0.33065\n",
      "echo: 3044  loss: 0.33065\n",
      "echo: 3045  loss: 0.33065\n",
      "echo: 3046  loss: 0.33065\n",
      "echo: 3047  loss: 0.33065\n",
      "echo: 3048  loss: 0.33064\n",
      "echo: 3049  loss: 0.33064\n",
      "echo: 3050  loss: 0.33064\n",
      "echo: 3051  loss: 0.33064\n",
      "echo: 3052  loss: 0.33064\n",
      "echo: 3053  loss: 0.33064\n",
      "echo: 3054  loss: 0.33063\n",
      "echo: 3055  loss: 0.33063\n",
      "echo: 3056  loss: 0.33063\n",
      "echo: 3057  loss: 0.33063\n",
      "echo: 3058  loss: 0.33063\n",
      "echo: 3059  loss: 0.33063\n",
      "echo: 3060  loss: 0.33063\n",
      "echo: 3061  loss: 0.33062\n",
      "echo: 3062  loss: 0.33062\n",
      "echo: 3063  loss: 0.33062\n",
      "echo: 3064  loss: 0.33062\n",
      "echo: 3065  loss: 0.33062\n",
      "echo: 3066  loss: 0.33062\n",
      "echo: 3067  loss: 0.33062\n",
      "echo: 3068  loss: 0.33061\n",
      "echo: 3069  loss: 0.33061\n",
      "echo: 3070  loss: 0.33061\n",
      "echo: 3071  loss: 0.33061\n",
      "echo: 3072  loss: 0.33061\n",
      "echo: 3073  loss: 0.33061\n",
      "echo: 3074  loss: 0.33061\n",
      "echo: 3075  loss: 0.3306\n",
      "echo: 3076  loss: 0.3306\n",
      "echo: 3077  loss: 0.3306\n",
      "echo: 3078  loss: 0.3306\n",
      "echo: 3079  loss: 0.3306\n",
      "echo: 3080  loss: 0.3306\n",
      "echo: 3081  loss: 0.33059\n",
      "echo: 3082  loss: 0.33059\n",
      "echo: 3083  loss: 0.33059\n",
      "echo: 3084  loss: 0.33059\n",
      "echo: 3085  loss: 0.33059\n",
      "echo: 3086  loss: 0.33059\n",
      "echo: 3087  loss: 0.33059\n",
      "echo: 3088  loss: 0.33058\n",
      "echo: 3089  loss: 0.33058\n",
      "echo: 3090  loss: 0.33058\n",
      "echo: 3091  loss: 0.33058\n",
      "echo: 3092  loss: 0.33058\n",
      "echo: 3093  loss: 0.33058\n",
      "echo: 3094  loss: 0.33058\n",
      "echo: 3095  loss: 0.33057\n",
      "echo: 3096  loss: 0.33057\n",
      "echo: 3097  loss: 0.33057\n",
      "echo: 3098  loss: 0.33057\n",
      "echo: 3099  loss: 0.33057\n",
      "echo: 3100  loss: 0.33057\n",
      "echo: 3101  loss: 0.33057\n",
      "echo: 3102  loss: 0.33056\n",
      "echo: 3103  loss: 0.33056\n",
      "echo: 3104  loss: 0.33056\n",
      "echo: 3105  loss: 0.33056\n",
      "echo: 3106  loss: 0.33056\n",
      "echo: 3107  loss: 0.33056\n",
      "echo: 3108  loss: 0.33056\n",
      "echo: 3109  loss: 0.33055\n",
      "echo: 3110  loss: 0.33055\n",
      "echo: 3111  loss: 0.33055\n",
      "echo: 3112  loss: 0.33055\n",
      "echo: 3113  loss: 0.33055\n",
      "echo: 3114  loss: 0.33055\n",
      "echo: 3115  loss: 0.33055\n",
      "echo: 3116  loss: 0.33054\n",
      "echo: 3117  loss: 0.33054\n",
      "echo: 3118  loss: 0.33054\n",
      "echo: 3119  loss: 0.33054\n",
      "echo: 3120  loss: 0.33054\n",
      "echo: 3121  loss: 0.33054\n",
      "echo: 3122  loss: 0.33054\n",
      "echo: 3123  loss: 0.33053\n",
      "echo: 3124  loss: 0.33053\n",
      "echo: 3125  loss: 0.33053\n",
      "echo: 3126  loss: 0.33053\n",
      "echo: 3127  loss: 0.33053\n",
      "echo: 3128  loss: 0.33053\n",
      "echo: 3129  loss: 0.33053\n",
      "echo: 3130  loss: 0.33052\n",
      "echo: 3131  loss: 0.33052\n",
      "echo: 3132  loss: 0.33052\n",
      "echo: 3133  loss: 0.33052\n",
      "echo: 3134  loss: 0.33052\n",
      "echo: 3135  loss: 0.33052\n",
      "echo: 3136  loss: 0.33052\n",
      "echo: 3137  loss: 0.33051\n",
      "echo: 3138  loss: 0.33051\n",
      "echo: 3139  loss: 0.33051\n",
      "echo: 3140  loss: 0.33051\n",
      "echo: 3141  loss: 0.33051\n",
      "echo: 3142  loss: 0.33051\n",
      "echo: 3143  loss: 0.33051\n",
      "echo: 3144  loss: 0.3305\n",
      "echo: 3145  loss: 0.3305\n",
      "echo: 3146  loss: 0.3305\n",
      "echo: 3147  loss: 0.3305\n",
      "echo: 3148  loss: 0.3305\n",
      "echo: 3149  loss: 0.3305\n",
      "echo: 3150  loss: 0.3305\n",
      "echo: 3151  loss: 0.3305\n",
      "echo: 3152  loss: 0.33049\n",
      "echo: 3153  loss: 0.33049\n",
      "echo: 3154  loss: 0.33049\n",
      "echo: 3155  loss: 0.33049\n",
      "echo: 3156  loss: 0.33049\n",
      "echo: 3157  loss: 0.33049\n",
      "echo: 3158  loss: 0.33049\n",
      "echo: 3159  loss: 0.33048\n",
      "echo: 3160  loss: 0.33048\n",
      "echo: 3161  loss: 0.33048\n",
      "echo: 3162  loss: 0.33048\n",
      "echo: 3163  loss: 0.33048\n",
      "echo: 3164  loss: 0.33048\n",
      "echo: 3165  loss: 0.33048\n",
      "echo: 3166  loss: 0.33047\n",
      "echo: 3167  loss: 0.33047\n",
      "echo: 3168  loss: 0.33047\n",
      "echo: 3169  loss: 0.33047\n",
      "echo: 3170  loss: 0.33047\n",
      "echo: 3171  loss: 0.33047\n",
      "echo: 3172  loss: 0.33047\n",
      "echo: 3173  loss: 0.33046\n",
      "echo: 3174  loss: 0.33046\n",
      "echo: 3175  loss: 0.33046\n",
      "echo: 3176  loss: 0.33046\n",
      "echo: 3177  loss: 0.33046\n",
      "echo: 3178  loss: 0.33046\n",
      "echo: 3179  loss: 0.33046\n",
      "echo: 3180  loss: 0.33046\n",
      "echo: 3181  loss: 0.33045\n",
      "echo: 3182  loss: 0.33045\n",
      "echo: 3183  loss: 0.33045\n",
      "echo: 3184  loss: 0.33045\n",
      "echo: 3185  loss: 0.33045\n",
      "echo: 3186  loss: 0.33045\n",
      "echo: 3187  loss: 0.33045\n",
      "echo: 3188  loss: 0.33044\n",
      "echo: 3189  loss: 0.33044\n",
      "echo: 3190  loss: 0.33044\n",
      "echo: 3191  loss: 0.33044\n",
      "echo: 3192  loss: 0.33044\n",
      "echo: 3193  loss: 0.33044\n",
      "echo: 3194  loss: 0.33044\n",
      "echo: 3195  loss: 0.33043\n",
      "echo: 3196  loss: 0.33043\n",
      "echo: 3197  loss: 0.33043\n",
      "echo: 3198  loss: 0.33043\n",
      "echo: 3199  loss: 0.33043\n",
      "echo: 3200  loss: 0.33043\n",
      "echo: 3201  loss: 0.33043\n",
      "echo: 3202  loss: 0.33043\n",
      "echo: 3203  loss: 0.33042\n",
      "echo: 3204  loss: 0.33042\n",
      "echo: 3205  loss: 0.33042\n",
      "echo: 3206  loss: 0.33042\n",
      "echo: 3207  loss: 0.33042\n",
      "echo: 3208  loss: 0.33042\n",
      "echo: 3209  loss: 0.33042\n",
      "echo: 3210  loss: 0.33041\n",
      "echo: 3211  loss: 0.33041\n",
      "echo: 3212  loss: 0.33041\n",
      "echo: 3213  loss: 0.33041\n",
      "echo: 3214  loss: 0.33041\n",
      "echo: 3215  loss: 0.33041\n",
      "echo: 3216  loss: 0.33041\n",
      "echo: 3217  loss: 0.33041\n",
      "echo: 3218  loss: 0.3304\n",
      "echo: 3219  loss: 0.3304\n",
      "echo: 3220  loss: 0.3304\n",
      "echo: 3221  loss: 0.3304\n",
      "echo: 3222  loss: 0.3304\n",
      "echo: 3223  loss: 0.3304\n",
      "echo: 3224  loss: 0.3304\n",
      "echo: 3225  loss: 0.33039\n",
      "echo: 3226  loss: 0.33039\n",
      "echo: 3227  loss: 0.33039\n",
      "echo: 3228  loss: 0.33039\n",
      "echo: 3229  loss: 0.33039\n",
      "echo: 3230  loss: 0.33039\n",
      "echo: 3231  loss: 0.33039\n",
      "echo: 3232  loss: 0.33039\n",
      "echo: 3233  loss: 0.33038\n",
      "echo: 3234  loss: 0.33038\n",
      "echo: 3235  loss: 0.33038\n",
      "echo: 3236  loss: 0.33038\n",
      "echo: 3237  loss: 0.33038\n",
      "echo: 3238  loss: 0.33038\n",
      "echo: 3239  loss: 0.33038\n",
      "echo: 3240  loss: 0.33037\n",
      "echo: 3241  loss: 0.33037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 3242  loss: 0.33037\n",
      "echo: 3243  loss: 0.33037\n",
      "echo: 3244  loss: 0.33037\n",
      "echo: 3245  loss: 0.33037\n",
      "echo: 3246  loss: 0.33037\n",
      "echo: 3247  loss: 0.33037\n",
      "echo: 3248  loss: 0.33036\n",
      "echo: 3249  loss: 0.33036\n",
      "echo: 3250  loss: 0.33036\n",
      "echo: 3251  loss: 0.33036\n",
      "echo: 3252  loss: 0.33036\n",
      "echo: 3253  loss: 0.33036\n",
      "echo: 3254  loss: 0.33036\n",
      "echo: 3255  loss: 0.33035\n",
      "echo: 3256  loss: 0.33035\n",
      "echo: 3257  loss: 0.33035\n",
      "echo: 3258  loss: 0.33035\n",
      "echo: 3259  loss: 0.33035\n",
      "echo: 3260  loss: 0.33035\n",
      "echo: 3261  loss: 0.33035\n",
      "echo: 3262  loss: 0.33035\n",
      "echo: 3263  loss: 0.33034\n",
      "echo: 3264  loss: 0.33034\n",
      "echo: 3265  loss: 0.33034\n",
      "echo: 3266  loss: 0.33034\n",
      "echo: 3267  loss: 0.33034\n",
      "echo: 3268  loss: 0.33034\n",
      "echo: 3269  loss: 0.33034\n",
      "echo: 3270  loss: 0.33034\n",
      "echo: 3271  loss: 0.33033\n",
      "echo: 3272  loss: 0.33033\n",
      "echo: 3273  loss: 0.33033\n",
      "echo: 3274  loss: 0.33033\n",
      "echo: 3275  loss: 0.33033\n",
      "echo: 3276  loss: 0.33033\n",
      "echo: 3277  loss: 0.33033\n",
      "echo: 3278  loss: 0.33033\n",
      "echo: 3279  loss: 0.33032\n",
      "echo: 3280  loss: 0.33032\n",
      "echo: 3281  loss: 0.33032\n",
      "echo: 3282  loss: 0.33032\n",
      "echo: 3283  loss: 0.33032\n",
      "echo: 3284  loss: 0.33032\n",
      "echo: 3285  loss: 0.33032\n",
      "echo: 3286  loss: 0.33031\n",
      "echo: 3287  loss: 0.33031\n",
      "echo: 3288  loss: 0.33031\n",
      "echo: 3289  loss: 0.33031\n",
      "echo: 3290  loss: 0.33031\n",
      "echo: 3291  loss: 0.33031\n",
      "echo: 3292  loss: 0.33031\n",
      "echo: 3293  loss: 0.33031\n",
      "echo: 3294  loss: 0.3303\n",
      "echo: 3295  loss: 0.3303\n",
      "echo: 3296  loss: 0.3303\n",
      "echo: 3297  loss: 0.3303\n",
      "echo: 3298  loss: 0.3303\n",
      "echo: 3299  loss: 0.3303\n",
      "echo: 3300  loss: 0.3303\n",
      "echo: 3301  loss: 0.3303\n",
      "echo: 3302  loss: 0.33029\n",
      "echo: 3303  loss: 0.33029\n",
      "echo: 3304  loss: 0.33029\n",
      "echo: 3305  loss: 0.33029\n",
      "echo: 3306  loss: 0.33029\n",
      "echo: 3307  loss: 0.33029\n",
      "echo: 3308  loss: 0.33029\n",
      "echo: 3309  loss: 0.33029\n",
      "echo: 3310  loss: 0.33028\n",
      "echo: 3311  loss: 0.33028\n",
      "echo: 3312  loss: 0.33028\n",
      "echo: 3313  loss: 0.33028\n",
      "echo: 3314  loss: 0.33028\n",
      "echo: 3315  loss: 0.33028\n",
      "echo: 3316  loss: 0.33028\n",
      "echo: 3317  loss: 0.33028\n",
      "echo: 3318  loss: 0.33027\n",
      "echo: 3319  loss: 0.33027\n",
      "echo: 3320  loss: 0.33027\n",
      "echo: 3321  loss: 0.33027\n",
      "echo: 3322  loss: 0.33027\n",
      "echo: 3323  loss: 0.33027\n",
      "echo: 3324  loss: 0.33027\n",
      "echo: 3325  loss: 0.33027\n",
      "echo: 3326  loss: 0.33026\n",
      "echo: 3327  loss: 0.33026\n",
      "echo: 3328  loss: 0.33026\n",
      "echo: 3329  loss: 0.33026\n",
      "echo: 3330  loss: 0.33026\n",
      "echo: 3331  loss: 0.33026\n",
      "echo: 3332  loss: 0.33026\n",
      "echo: 3333  loss: 0.33026\n",
      "echo: 3334  loss: 0.33025\n",
      "echo: 3335  loss: 0.33025\n",
      "echo: 3336  loss: 0.33025\n",
      "echo: 3337  loss: 0.33025\n",
      "echo: 3338  loss: 0.33025\n",
      "echo: 3339  loss: 0.33025\n",
      "echo: 3340  loss: 0.33025\n",
      "echo: 3341  loss: 0.33025\n",
      "echo: 3342  loss: 0.33024\n",
      "echo: 3343  loss: 0.33024\n",
      "echo: 3344  loss: 0.33024\n",
      "echo: 3345  loss: 0.33024\n",
      "echo: 3346  loss: 0.33024\n",
      "echo: 3347  loss: 0.33024\n",
      "echo: 3348  loss: 0.33024\n",
      "echo: 3349  loss: 0.33024\n",
      "echo: 3350  loss: 0.33024\n",
      "echo: 3351  loss: 0.33023\n",
      "echo: 3352  loss: 0.33023\n",
      "echo: 3353  loss: 0.33023\n",
      "echo: 3354  loss: 0.33023\n",
      "echo: 3355  loss: 0.33023\n",
      "echo: 3356  loss: 0.33023\n",
      "echo: 3357  loss: 0.33023\n",
      "echo: 3358  loss: 0.33023\n",
      "echo: 3359  loss: 0.33022\n",
      "echo: 3360  loss: 0.33022\n",
      "echo: 3361  loss: 0.33022\n",
      "echo: 3362  loss: 0.33022\n",
      "echo: 3363  loss: 0.33022\n",
      "echo: 3364  loss: 0.33022\n",
      "echo: 3365  loss: 0.33022\n",
      "echo: 3366  loss: 0.33022\n",
      "echo: 3367  loss: 0.33021\n",
      "echo: 3368  loss: 0.33021\n",
      "echo: 3369  loss: 0.33021\n",
      "echo: 3370  loss: 0.33021\n",
      "echo: 3371  loss: 0.33021\n",
      "echo: 3372  loss: 0.33021\n",
      "echo: 3373  loss: 0.33021\n",
      "echo: 3374  loss: 0.33021\n",
      "echo: 3375  loss: 0.3302\n",
      "echo: 3376  loss: 0.3302\n",
      "echo: 3377  loss: 0.3302\n",
      "echo: 3378  loss: 0.3302\n",
      "echo: 3379  loss: 0.3302\n",
      "echo: 3380  loss: 0.3302\n",
      "echo: 3381  loss: 0.3302\n",
      "echo: 3382  loss: 0.3302\n",
      "echo: 3383  loss: 0.3302\n",
      "echo: 3384  loss: 0.33019\n",
      "echo: 3385  loss: 0.33019\n",
      "echo: 3386  loss: 0.33019\n",
      "echo: 3387  loss: 0.33019\n",
      "echo: 3388  loss: 0.33019\n",
      "echo: 3389  loss: 0.33019\n",
      "echo: 3390  loss: 0.33019\n",
      "echo: 3391  loss: 0.33019\n",
      "echo: 3392  loss: 0.33018\n",
      "echo: 3393  loss: 0.33018\n",
      "echo: 3394  loss: 0.33018\n",
      "echo: 3395  loss: 0.33018\n",
      "echo: 3396  loss: 0.33018\n",
      "echo: 3397  loss: 0.33018\n",
      "echo: 3398  loss: 0.33018\n",
      "echo: 3399  loss: 0.33018\n",
      "echo: 3400  loss: 0.33017\n",
      "echo: 3401  loss: 0.33017\n",
      "echo: 3402  loss: 0.33017\n",
      "echo: 3403  loss: 0.33017\n",
      "echo: 3404  loss: 0.33017\n",
      "echo: 3405  loss: 0.33017\n",
      "echo: 3406  loss: 0.33017\n",
      "echo: 3407  loss: 0.33017\n",
      "echo: 3408  loss: 0.33017\n",
      "echo: 3409  loss: 0.33016\n",
      "echo: 3410  loss: 0.33016\n",
      "echo: 3411  loss: 0.33016\n",
      "echo: 3412  loss: 0.33016\n",
      "echo: 3413  loss: 0.33016\n",
      "echo: 3414  loss: 0.33016\n",
      "echo: 3415  loss: 0.33016\n",
      "echo: 3416  loss: 0.33016\n",
      "echo: 3417  loss: 0.33015\n",
      "echo: 3418  loss: 0.33015\n",
      "echo: 3419  loss: 0.33015\n",
      "echo: 3420  loss: 0.33015\n",
      "echo: 3421  loss: 0.33015\n",
      "echo: 3422  loss: 0.33015\n",
      "echo: 3423  loss: 0.33015\n",
      "echo: 3424  loss: 0.33015\n",
      "echo: 3425  loss: 0.33015\n",
      "echo: 3426  loss: 0.33014\n",
      "echo: 3427  loss: 0.33014\n",
      "echo: 3428  loss: 0.33014\n",
      "echo: 3429  loss: 0.33014\n",
      "echo: 3430  loss: 0.33014\n",
      "echo: 3431  loss: 0.33014\n",
      "echo: 3432  loss: 0.33014\n",
      "echo: 3433  loss: 0.33014\n",
      "echo: 3434  loss: 0.33013\n",
      "echo: 3435  loss: 0.33013\n",
      "echo: 3436  loss: 0.33013\n",
      "echo: 3437  loss: 0.33013\n",
      "echo: 3438  loss: 0.33013\n",
      "echo: 3439  loss: 0.33013\n",
      "echo: 3440  loss: 0.33013\n",
      "echo: 3441  loss: 0.33013\n",
      "echo: 3442  loss: 0.33013\n",
      "echo: 3443  loss: 0.33012\n",
      "echo: 3444  loss: 0.33012\n",
      "echo: 3445  loss: 0.33012\n",
      "echo: 3446  loss: 0.33012\n",
      "echo: 3447  loss: 0.33012\n",
      "echo: 3448  loss: 0.33012\n",
      "echo: 3449  loss: 0.33012\n",
      "echo: 3450  loss: 0.33012\n",
      "echo: 3451  loss: 0.33012\n",
      "echo: 3452  loss: 0.33011\n",
      "echo: 3453  loss: 0.33011\n",
      "echo: 3454  loss: 0.33011\n",
      "echo: 3455  loss: 0.33011\n",
      "echo: 3456  loss: 0.33011\n",
      "echo: 3457  loss: 0.33011\n",
      "echo: 3458  loss: 0.33011\n",
      "echo: 3459  loss: 0.33011\n",
      "echo: 3460  loss: 0.33011\n",
      "echo: 3461  loss: 0.3301\n",
      "echo: 3462  loss: 0.3301\n",
      "echo: 3463  loss: 0.3301\n",
      "echo: 3464  loss: 0.3301\n",
      "echo: 3465  loss: 0.3301\n",
      "echo: 3466  loss: 0.3301\n",
      "echo: 3467  loss: 0.3301\n",
      "echo: 3468  loss: 0.3301\n",
      "echo: 3469  loss: 0.33009\n",
      "echo: 3470  loss: 0.33009\n",
      "echo: 3471  loss: 0.33009\n",
      "echo: 3472  loss: 0.33009\n",
      "echo: 3473  loss: 0.33009\n",
      "echo: 3474  loss: 0.33009\n",
      "echo: 3475  loss: 0.33009\n",
      "echo: 3476  loss: 0.33009\n",
      "echo: 3477  loss: 0.33009\n",
      "echo: 3478  loss: 0.33008\n",
      "echo: 3479  loss: 0.33008\n",
      "echo: 3480  loss: 0.33008\n",
      "echo: 3481  loss: 0.33008\n",
      "echo: 3482  loss: 0.33008\n",
      "echo: 3483  loss: 0.33008\n",
      "echo: 3484  loss: 0.33008\n",
      "echo: 3485  loss: 0.33008\n",
      "echo: 3486  loss: 0.33008\n",
      "echo: 3487  loss: 0.33007\n",
      "echo: 3488  loss: 0.33007\n",
      "echo: 3489  loss: 0.33007\n",
      "echo: 3490  loss: 0.33007\n",
      "echo: 3491  loss: 0.33007\n",
      "echo: 3492  loss: 0.33007\n",
      "echo: 3493  loss: 0.33007\n",
      "echo: 3494  loss: 0.33007\n",
      "echo: 3495  loss: 0.33007\n",
      "echo: 3496  loss: 0.33006\n",
      "echo: 3497  loss: 0.33006\n",
      "echo: 3498  loss: 0.33006\n",
      "echo: 3499  loss: 0.33006\n",
      "echo: 3500  loss: 0.33006\n",
      "echo: 3501  loss: 0.33006\n",
      "echo: 3502  loss: 0.33006\n",
      "echo: 3503  loss: 0.33006\n",
      "echo: 3504  loss: 0.33006\n",
      "echo: 3505  loss: 0.33005\n",
      "echo: 3506  loss: 0.33005\n",
      "echo: 3507  loss: 0.33005\n",
      "echo: 3508  loss: 0.33005\n",
      "echo: 3509  loss: 0.33005\n",
      "echo: 3510  loss: 0.33005\n",
      "echo: 3511  loss: 0.33005\n",
      "echo: 3512  loss: 0.33005\n",
      "echo: 3513  loss: 0.33005\n",
      "echo: 3514  loss: 0.33004\n",
      "echo: 3515  loss: 0.33004\n",
      "echo: 3516  loss: 0.33004\n",
      "echo: 3517  loss: 0.33004\n",
      "echo: 3518  loss: 0.33004\n",
      "echo: 3519  loss: 0.33004\n",
      "echo: 3520  loss: 0.33004\n",
      "echo: 3521  loss: 0.33004\n",
      "echo: 3522  loss: 0.33004\n",
      "echo: 3523  loss: 0.33003\n",
      "echo: 3524  loss: 0.33003\n",
      "echo: 3525  loss: 0.33003\n",
      "echo: 3526  loss: 0.33003\n",
      "echo: 3527  loss: 0.33003\n",
      "echo: 3528  loss: 0.33003\n",
      "echo: 3529  loss: 0.33003\n",
      "echo: 3530  loss: 0.33003\n",
      "echo: 3531  loss: 0.33003\n",
      "echo: 3532  loss: 0.33002\n",
      "echo: 3533  loss: 0.33002\n",
      "echo: 3534  loss: 0.33002\n",
      "echo: 3535  loss: 0.33002\n",
      "echo: 3536  loss: 0.33002\n",
      "echo: 3537  loss: 0.33002\n",
      "echo: 3538  loss: 0.33002\n",
      "echo: 3539  loss: 0.33002\n",
      "echo: 3540  loss: 0.33002\n",
      "echo: 3541  loss: 0.33002\n",
      "echo: 3542  loss: 0.33001\n",
      "echo: 3543  loss: 0.33001\n",
      "echo: 3544  loss: 0.33001\n",
      "echo: 3545  loss: 0.33001\n",
      "echo: 3546  loss: 0.33001\n",
      "echo: 3547  loss: 0.33001\n",
      "echo: 3548  loss: 0.33001\n",
      "echo: 3549  loss: 0.33001\n",
      "echo: 3550  loss: 0.33001\n",
      "echo: 3551  loss: 0.33\n",
      "echo: 3552  loss: 0.33\n",
      "echo: 3553  loss: 0.33\n",
      "echo: 3554  loss: 0.33\n",
      "echo: 3555  loss: 0.33\n",
      "echo: 3556  loss: 0.33\n",
      "echo: 3557  loss: 0.33\n",
      "echo: 3558  loss: 0.33\n",
      "echo: 3559  loss: 0.33\n",
      "echo: 3560  loss: 0.32999\n",
      "echo: 3561  loss: 0.32999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 3562  loss: 0.32999\n",
      "echo: 3563  loss: 0.32999\n",
      "echo: 3564  loss: 0.32999\n",
      "echo: 3565  loss: 0.32999\n",
      "echo: 3566  loss: 0.32999\n",
      "echo: 3567  loss: 0.32999\n",
      "echo: 3568  loss: 0.32999\n",
      "echo: 3569  loss: 0.32999\n",
      "echo: 3570  loss: 0.32998\n",
      "echo: 3571  loss: 0.32998\n",
      "echo: 3572  loss: 0.32998\n",
      "echo: 3573  loss: 0.32998\n",
      "echo: 3574  loss: 0.32998\n",
      "echo: 3575  loss: 0.32998\n",
      "echo: 3576  loss: 0.32998\n",
      "echo: 3577  loss: 0.32998\n",
      "echo: 3578  loss: 0.32998\n",
      "echo: 3579  loss: 0.32997\n",
      "echo: 3580  loss: 0.32997\n",
      "echo: 3581  loss: 0.32997\n",
      "echo: 3582  loss: 0.32997\n",
      "echo: 3583  loss: 0.32997\n",
      "echo: 3584  loss: 0.32997\n",
      "echo: 3585  loss: 0.32997\n",
      "echo: 3586  loss: 0.32997\n",
      "echo: 3587  loss: 0.32997\n",
      "echo: 3588  loss: 0.32997\n",
      "echo: 3589  loss: 0.32996\n",
      "echo: 3590  loss: 0.32996\n",
      "echo: 3591  loss: 0.32996\n",
      "echo: 3592  loss: 0.32996\n",
      "echo: 3593  loss: 0.32996\n",
      "echo: 3594  loss: 0.32996\n",
      "echo: 3595  loss: 0.32996\n",
      "echo: 3596  loss: 0.32996\n",
      "echo: 3597  loss: 0.32996\n",
      "echo: 3598  loss: 0.32995\n",
      "echo: 3599  loss: 0.32995\n",
      "echo: 3600  loss: 0.32995\n",
      "echo: 3601  loss: 0.32995\n",
      "echo: 3602  loss: 0.32995\n",
      "echo: 3603  loss: 0.32995\n",
      "echo: 3604  loss: 0.32995\n",
      "echo: 3605  loss: 0.32995\n",
      "echo: 3606  loss: 0.32995\n",
      "echo: 3607  loss: 0.32995\n",
      "echo: 3608  loss: 0.32994\n",
      "echo: 3609  loss: 0.32994\n",
      "echo: 3610  loss: 0.32994\n",
      "echo: 3611  loss: 0.32994\n",
      "echo: 3612  loss: 0.32994\n",
      "echo: 3613  loss: 0.32994\n",
      "echo: 3614  loss: 0.32994\n",
      "echo: 3615  loss: 0.32994\n",
      "echo: 3616  loss: 0.32994\n",
      "echo: 3617  loss: 0.32993\n",
      "echo: 3618  loss: 0.32993\n",
      "echo: 3619  loss: 0.32993\n",
      "echo: 3620  loss: 0.32993\n",
      "echo: 3621  loss: 0.32993\n",
      "echo: 3622  loss: 0.32993\n",
      "echo: 3623  loss: 0.32993\n",
      "echo: 3624  loss: 0.32993\n",
      "echo: 3625  loss: 0.32993\n",
      "echo: 3626  loss: 0.32993\n",
      "echo: 3627  loss: 0.32992\n",
      "echo: 3628  loss: 0.32992\n",
      "echo: 3629  loss: 0.32992\n",
      "echo: 3630  loss: 0.32992\n",
      "echo: 3631  loss: 0.32992\n",
      "echo: 3632  loss: 0.32992\n",
      "echo: 3633  loss: 0.32992\n",
      "echo: 3634  loss: 0.32992\n",
      "echo: 3635  loss: 0.32992\n",
      "echo: 3636  loss: 0.32992\n",
      "echo: 3637  loss: 0.32991\n",
      "echo: 3638  loss: 0.32991\n",
      "echo: 3639  loss: 0.32991\n",
      "echo: 3640  loss: 0.32991\n",
      "echo: 3641  loss: 0.32991\n",
      "echo: 3642  loss: 0.32991\n",
      "echo: 3643  loss: 0.32991\n",
      "echo: 3644  loss: 0.32991\n",
      "echo: 3645  loss: 0.32991\n",
      "echo: 3646  loss: 0.32991\n",
      "echo: 3647  loss: 0.3299\n",
      "echo: 3648  loss: 0.3299\n",
      "echo: 3649  loss: 0.3299\n",
      "echo: 3650  loss: 0.3299\n",
      "echo: 3651  loss: 0.3299\n",
      "echo: 3652  loss: 0.3299\n",
      "echo: 3653  loss: 0.3299\n",
      "echo: 3654  loss: 0.3299\n",
      "echo: 3655  loss: 0.3299\n",
      "echo: 3656  loss: 0.3299\n",
      "echo: 3657  loss: 0.32989\n",
      "echo: 3658  loss: 0.32989\n",
      "echo: 3659  loss: 0.32989\n",
      "echo: 3660  loss: 0.32989\n",
      "echo: 3661  loss: 0.32989\n",
      "echo: 3662  loss: 0.32989\n",
      "echo: 3663  loss: 0.32989\n",
      "echo: 3664  loss: 0.32989\n",
      "echo: 3665  loss: 0.32989\n",
      "echo: 3666  loss: 0.32989\n",
      "echo: 3667  loss: 0.32988\n",
      "echo: 3668  loss: 0.32988\n",
      "echo: 3669  loss: 0.32988\n",
      "echo: 3670  loss: 0.32988\n",
      "echo: 3671  loss: 0.32988\n",
      "echo: 3672  loss: 0.32988\n",
      "echo: 3673  loss: 0.32988\n",
      "echo: 3674  loss: 0.32988\n",
      "echo: 3675  loss: 0.32988\n",
      "echo: 3676  loss: 0.32988\n",
      "echo: 3677  loss: 0.32987\n",
      "echo: 3678  loss: 0.32987\n",
      "echo: 3679  loss: 0.32987\n",
      "echo: 3680  loss: 0.32987\n",
      "echo: 3681  loss: 0.32987\n",
      "echo: 3682  loss: 0.32987\n",
      "echo: 3683  loss: 0.32987\n",
      "echo: 3684  loss: 0.32987\n",
      "echo: 3685  loss: 0.32987\n",
      "echo: 3686  loss: 0.32987\n",
      "echo: 3687  loss: 0.32986\n",
      "echo: 3688  loss: 0.32986\n",
      "echo: 3689  loss: 0.32986\n",
      "echo: 3690  loss: 0.32986\n",
      "echo: 3691  loss: 0.32986\n",
      "echo: 3692  loss: 0.32986\n",
      "echo: 3693  loss: 0.32986\n",
      "echo: 3694  loss: 0.32986\n",
      "echo: 3695  loss: 0.32986\n",
      "echo: 3696  loss: 0.32986\n",
      "echo: 3697  loss: 0.32985\n",
      "echo: 3698  loss: 0.32985\n",
      "echo: 3699  loss: 0.32985\n",
      "echo: 3700  loss: 0.32985\n",
      "echo: 3701  loss: 0.32985\n",
      "echo: 3702  loss: 0.32985\n",
      "echo: 3703  loss: 0.32985\n",
      "echo: 3704  loss: 0.32985\n",
      "echo: 3705  loss: 0.32985\n",
      "echo: 3706  loss: 0.32985\n",
      "echo: 3707  loss: 0.32984\n",
      "echo: 3708  loss: 0.32984\n",
      "echo: 3709  loss: 0.32984\n",
      "echo: 3710  loss: 0.32984\n",
      "echo: 3711  loss: 0.32984\n",
      "echo: 3712  loss: 0.32984\n",
      "echo: 3713  loss: 0.32984\n",
      "echo: 3714  loss: 0.32984\n",
      "echo: 3715  loss: 0.32984\n",
      "echo: 3716  loss: 0.32984\n",
      "echo: 3717  loss: 0.32984\n",
      "echo: 3718  loss: 0.32983\n",
      "echo: 3719  loss: 0.32983\n",
      "echo: 3720  loss: 0.32983\n",
      "echo: 3721  loss: 0.32983\n",
      "echo: 3722  loss: 0.32983\n",
      "echo: 3723  loss: 0.32983\n",
      "echo: 3724  loss: 0.32983\n",
      "echo: 3725  loss: 0.32983\n",
      "echo: 3726  loss: 0.32983\n",
      "echo: 3727  loss: 0.32983\n",
      "echo: 3728  loss: 0.32982\n",
      "echo: 3729  loss: 0.32982\n",
      "echo: 3730  loss: 0.32982\n",
      "echo: 3731  loss: 0.32982\n",
      "echo: 3732  loss: 0.32982\n",
      "echo: 3733  loss: 0.32982\n",
      "echo: 3734  loss: 0.32982\n",
      "echo: 3735  loss: 0.32982\n",
      "echo: 3736  loss: 0.32982\n",
      "echo: 3737  loss: 0.32982\n",
      "echo: 3738  loss: 0.32982\n",
      "echo: 3739  loss: 0.32981\n",
      "echo: 3740  loss: 0.32981\n",
      "echo: 3741  loss: 0.32981\n",
      "echo: 3742  loss: 0.32981\n",
      "echo: 3743  loss: 0.32981\n",
      "echo: 3744  loss: 0.32981\n",
      "echo: 3745  loss: 0.32981\n",
      "echo: 3746  loss: 0.32981\n",
      "echo: 3747  loss: 0.32981\n",
      "echo: 3748  loss: 0.32981\n",
      "echo: 3749  loss: 0.3298\n",
      "echo: 3750  loss: 0.3298\n",
      "echo: 3751  loss: 0.3298\n",
      "echo: 3752  loss: 0.3298\n",
      "echo: 3753  loss: 0.3298\n",
      "echo: 3754  loss: 0.3298\n",
      "echo: 3755  loss: 0.3298\n",
      "echo: 3756  loss: 0.3298\n",
      "echo: 3757  loss: 0.3298\n",
      "echo: 3758  loss: 0.3298\n",
      "echo: 3759  loss: 0.3298\n",
      "echo: 3760  loss: 0.32979\n",
      "echo: 3761  loss: 0.32979\n",
      "echo: 3762  loss: 0.32979\n",
      "echo: 3763  loss: 0.32979\n",
      "echo: 3764  loss: 0.32979\n",
      "echo: 3765  loss: 0.32979\n",
      "echo: 3766  loss: 0.32979\n",
      "echo: 3767  loss: 0.32979\n",
      "echo: 3768  loss: 0.32979\n",
      "echo: 3769  loss: 0.32979\n",
      "echo: 3770  loss: 0.32978\n",
      "echo: 3771  loss: 0.32978\n",
      "echo: 3772  loss: 0.32978\n",
      "echo: 3773  loss: 0.32978\n",
      "echo: 3774  loss: 0.32978\n",
      "echo: 3775  loss: 0.32978\n",
      "echo: 3776  loss: 0.32978\n",
      "echo: 3777  loss: 0.32978\n",
      "echo: 3778  loss: 0.32978\n",
      "echo: 3779  loss: 0.32978\n",
      "echo: 3780  loss: 0.32978\n",
      "echo: 3781  loss: 0.32977\n",
      "echo: 3782  loss: 0.32977\n",
      "echo: 3783  loss: 0.32977\n",
      "echo: 3784  loss: 0.32977\n",
      "echo: 3785  loss: 0.32977\n",
      "echo: 3786  loss: 0.32977\n",
      "echo: 3787  loss: 0.32977\n",
      "echo: 3788  loss: 0.32977\n",
      "echo: 3789  loss: 0.32977\n",
      "echo: 3790  loss: 0.32977\n",
      "echo: 3791  loss: 0.32977\n",
      "echo: 3792  loss: 0.32976\n",
      "echo: 3793  loss: 0.32976\n",
      "echo: 3794  loss: 0.32976\n",
      "echo: 3795  loss: 0.32976\n",
      "echo: 3796  loss: 0.32976\n",
      "echo: 3797  loss: 0.32976\n",
      "echo: 3798  loss: 0.32976\n",
      "echo: 3799  loss: 0.32976\n",
      "echo: 3800  loss: 0.32976\n",
      "echo: 3801  loss: 0.32976\n",
      "echo: 3802  loss: 0.32976\n",
      "echo: 3803  loss: 0.32975\n",
      "echo: 3804  loss: 0.32975\n",
      "echo: 3805  loss: 0.32975\n",
      "echo: 3806  loss: 0.32975\n",
      "echo: 3807  loss: 0.32975\n",
      "echo: 3808  loss: 0.32975\n",
      "echo: 3809  loss: 0.32975\n",
      "echo: 3810  loss: 0.32975\n",
      "echo: 3811  loss: 0.32975\n",
      "echo: 3812  loss: 0.32975\n",
      "echo: 3813  loss: 0.32975\n",
      "echo: 3814  loss: 0.32974\n",
      "echo: 3815  loss: 0.32974\n",
      "echo: 3816  loss: 0.32974\n",
      "echo: 3817  loss: 0.32974\n",
      "echo: 3818  loss: 0.32974\n",
      "echo: 3819  loss: 0.32974\n",
      "echo: 3820  loss: 0.32974\n",
      "echo: 3821  loss: 0.32974\n",
      "echo: 3822  loss: 0.32974\n",
      "echo: 3823  loss: 0.32974\n",
      "echo: 3824  loss: 0.32974\n",
      "echo: 3825  loss: 0.32973\n",
      "echo: 3826  loss: 0.32973\n",
      "echo: 3827  loss: 0.32973\n",
      "echo: 3828  loss: 0.32973\n",
      "echo: 3829  loss: 0.32973\n",
      "echo: 3830  loss: 0.32973\n",
      "echo: 3831  loss: 0.32973\n",
      "echo: 3832  loss: 0.32973\n",
      "echo: 3833  loss: 0.32973\n",
      "echo: 3834  loss: 0.32973\n",
      "echo: 3835  loss: 0.32973\n",
      "echo: 3836  loss: 0.32972\n",
      "echo: 3837  loss: 0.32972\n",
      "echo: 3838  loss: 0.32972\n",
      "echo: 3839  loss: 0.32972\n",
      "echo: 3840  loss: 0.32972\n",
      "echo: 3841  loss: 0.32972\n",
      "echo: 3842  loss: 0.32972\n",
      "echo: 3843  loss: 0.32972\n",
      "echo: 3844  loss: 0.32972\n",
      "echo: 3845  loss: 0.32972\n",
      "echo: 3846  loss: 0.32972\n",
      "echo: 3847  loss: 0.32971\n",
      "echo: 3848  loss: 0.32971\n",
      "echo: 3849  loss: 0.32971\n",
      "echo: 3850  loss: 0.32971\n",
      "echo: 3851  loss: 0.32971\n",
      "echo: 3852  loss: 0.32971\n",
      "echo: 3853  loss: 0.32971\n",
      "echo: 3854  loss: 0.32971\n",
      "echo: 3855  loss: 0.32971\n",
      "echo: 3856  loss: 0.32971\n",
      "echo: 3857  loss: 0.32971\n",
      "echo: 3858  loss: 0.32971\n",
      "echo: 3859  loss: 0.3297\n",
      "echo: 3860  loss: 0.3297\n",
      "echo: 3861  loss: 0.3297\n",
      "echo: 3862  loss: 0.3297\n",
      "echo: 3863  loss: 0.3297\n",
      "echo: 3864  loss: 0.3297\n",
      "echo: 3865  loss: 0.3297\n",
      "echo: 3866  loss: 0.3297\n",
      "echo: 3867  loss: 0.3297\n",
      "echo: 3868  loss: 0.3297\n",
      "echo: 3869  loss: 0.3297\n",
      "echo: 3870  loss: 0.32969\n",
      "echo: 3871  loss: 0.32969\n",
      "echo: 3872  loss: 0.32969\n",
      "echo: 3873  loss: 0.32969\n",
      "echo: 3874  loss: 0.32969\n",
      "echo: 3875  loss: 0.32969\n",
      "echo: 3876  loss: 0.32969\n",
      "echo: 3877  loss: 0.32969\n",
      "echo: 3878  loss: 0.32969\n",
      "echo: 3879  loss: 0.32969\n",
      "echo: 3880  loss: 0.32969\n",
      "echo: 3881  loss: 0.32968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 3882  loss: 0.32968\n",
      "echo: 3883  loss: 0.32968\n",
      "echo: 3884  loss: 0.32968\n",
      "echo: 3885  loss: 0.32968\n",
      "echo: 3886  loss: 0.32968\n",
      "echo: 3887  loss: 0.32968\n",
      "echo: 3888  loss: 0.32968\n",
      "echo: 3889  loss: 0.32968\n",
      "echo: 3890  loss: 0.32968\n",
      "echo: 3891  loss: 0.32968\n",
      "echo: 3892  loss: 0.32968\n",
      "echo: 3893  loss: 0.32967\n",
      "echo: 3894  loss: 0.32967\n",
      "echo: 3895  loss: 0.32967\n",
      "echo: 3896  loss: 0.32967\n",
      "echo: 3897  loss: 0.32967\n",
      "echo: 3898  loss: 0.32967\n",
      "echo: 3899  loss: 0.32967\n",
      "echo: 3900  loss: 0.32967\n",
      "echo: 3901  loss: 0.32967\n",
      "echo: 3902  loss: 0.32967\n",
      "echo: 3903  loss: 0.32967\n",
      "echo: 3904  loss: 0.32967\n",
      "echo: 3905  loss: 0.32966\n",
      "echo: 3906  loss: 0.32966\n",
      "echo: 3907  loss: 0.32966\n",
      "echo: 3908  loss: 0.32966\n",
      "echo: 3909  loss: 0.32966\n",
      "echo: 3910  loss: 0.32966\n",
      "echo: 3911  loss: 0.32966\n",
      "echo: 3912  loss: 0.32966\n",
      "echo: 3913  loss: 0.32966\n",
      "echo: 3914  loss: 0.32966\n",
      "echo: 3915  loss: 0.32966\n",
      "echo: 3916  loss: 0.32965\n",
      "echo: 3917  loss: 0.32965\n",
      "echo: 3918  loss: 0.32965\n",
      "echo: 3919  loss: 0.32965\n",
      "echo: 3920  loss: 0.32965\n",
      "echo: 3921  loss: 0.32965\n",
      "echo: 3922  loss: 0.32965\n",
      "echo: 3923  loss: 0.32965\n",
      "echo: 3924  loss: 0.32965\n",
      "echo: 3925  loss: 0.32965\n",
      "echo: 3926  loss: 0.32965\n",
      "echo: 3927  loss: 0.32965\n",
      "echo: 3928  loss: 0.32964\n",
      "echo: 3929  loss: 0.32964\n",
      "echo: 3930  loss: 0.32964\n",
      "echo: 3931  loss: 0.32964\n",
      "echo: 3932  loss: 0.32964\n",
      "echo: 3933  loss: 0.32964\n",
      "echo: 3934  loss: 0.32964\n",
      "echo: 3935  loss: 0.32964\n",
      "echo: 3936  loss: 0.32964\n",
      "echo: 3937  loss: 0.32964\n",
      "echo: 3938  loss: 0.32964\n",
      "echo: 3939  loss: 0.32964\n",
      "echo: 3940  loss: 0.32963\n",
      "echo: 3941  loss: 0.32963\n",
      "echo: 3942  loss: 0.32963\n",
      "echo: 3943  loss: 0.32963\n",
      "echo: 3944  loss: 0.32963\n",
      "echo: 3945  loss: 0.32963\n",
      "echo: 3946  loss: 0.32963\n",
      "echo: 3947  loss: 0.32963\n",
      "echo: 3948  loss: 0.32963\n",
      "echo: 3949  loss: 0.32963\n",
      "echo: 3950  loss: 0.32963\n",
      "echo: 3951  loss: 0.32963\n",
      "echo: 3952  loss: 0.32962\n",
      "echo: 3953  loss: 0.32962\n",
      "echo: 3954  loss: 0.32962\n",
      "echo: 3955  loss: 0.32962\n",
      "echo: 3956  loss: 0.32962\n",
      "echo: 3957  loss: 0.32962\n",
      "echo: 3958  loss: 0.32962\n",
      "echo: 3959  loss: 0.32962\n",
      "echo: 3960  loss: 0.32962\n",
      "echo: 3961  loss: 0.32962\n",
      "echo: 3962  loss: 0.32962\n",
      "echo: 3963  loss: 0.32962\n",
      "echo: 3964  loss: 0.32961\n",
      "echo: 3965  loss: 0.32961\n",
      "echo: 3966  loss: 0.32961\n",
      "echo: 3967  loss: 0.32961\n",
      "echo: 3968  loss: 0.32961\n",
      "echo: 3969  loss: 0.32961\n",
      "echo: 3970  loss: 0.32961\n",
      "echo: 3971  loss: 0.32961\n",
      "echo: 3972  loss: 0.32961\n",
      "echo: 3973  loss: 0.32961\n",
      "echo: 3974  loss: 0.32961\n",
      "echo: 3975  loss: 0.32961\n",
      "echo: 3976  loss: 0.3296\n",
      "echo: 3977  loss: 0.3296\n",
      "echo: 3978  loss: 0.3296\n",
      "echo: 3979  loss: 0.3296\n",
      "echo: 3980  loss: 0.3296\n",
      "echo: 3981  loss: 0.3296\n",
      "echo: 3982  loss: 0.3296\n",
      "echo: 3983  loss: 0.3296\n",
      "echo: 3984  loss: 0.3296\n",
      "echo: 3985  loss: 0.3296\n",
      "echo: 3986  loss: 0.3296\n",
      "echo: 3987  loss: 0.3296\n",
      "echo: 3988  loss: 0.3296\n",
      "echo: 3989  loss: 0.32959\n",
      "echo: 3990  loss: 0.32959\n",
      "echo: 3991  loss: 0.32959\n",
      "echo: 3992  loss: 0.32959\n",
      "echo: 3993  loss: 0.32959\n",
      "echo: 3994  loss: 0.32959\n",
      "echo: 3995  loss: 0.32959\n",
      "echo: 3996  loss: 0.32959\n",
      "echo: 3997  loss: 0.32959\n",
      "echo: 3998  loss: 0.32959\n",
      "echo: 3999  loss: 0.32959\n",
      "echo: 4000  loss: 0.32959\n",
      "echo: 4001  loss: 0.32958\n",
      "echo: 4002  loss: 0.32958\n",
      "echo: 4003  loss: 0.32958\n",
      "echo: 4004  loss: 0.32958\n",
      "echo: 4005  loss: 0.32958\n",
      "echo: 4006  loss: 0.32958\n",
      "echo: 4007  loss: 0.32958\n",
      "echo: 4008  loss: 0.32958\n",
      "echo: 4009  loss: 0.32958\n",
      "echo: 4010  loss: 0.32958\n",
      "echo: 4011  loss: 0.32958\n",
      "echo: 4012  loss: 0.32958\n",
      "echo: 4013  loss: 0.32957\n",
      "echo: 4014  loss: 0.32957\n",
      "echo: 4015  loss: 0.32957\n",
      "echo: 4016  loss: 0.32957\n",
      "echo: 4017  loss: 0.32957\n",
      "echo: 4018  loss: 0.32957\n",
      "echo: 4019  loss: 0.32957\n",
      "echo: 4020  loss: 0.32957\n",
      "echo: 4021  loss: 0.32957\n",
      "echo: 4022  loss: 0.32957\n",
      "echo: 4023  loss: 0.32957\n",
      "echo: 4024  loss: 0.32957\n",
      "echo: 4025  loss: 0.32957\n",
      "echo: 4026  loss: 0.32956\n",
      "echo: 4027  loss: 0.32956\n",
      "echo: 4028  loss: 0.32956\n",
      "echo: 4029  loss: 0.32956\n",
      "echo: 4030  loss: 0.32956\n",
      "echo: 4031  loss: 0.32956\n",
      "echo: 4032  loss: 0.32956\n",
      "echo: 4033  loss: 0.32956\n",
      "echo: 4034  loss: 0.32956\n",
      "echo: 4035  loss: 0.32956\n",
      "echo: 4036  loss: 0.32956\n",
      "echo: 4037  loss: 0.32956\n",
      "echo: 4038  loss: 0.32956\n",
      "echo: 4039  loss: 0.32955\n",
      "echo: 4040  loss: 0.32955\n",
      "echo: 4041  loss: 0.32955\n",
      "echo: 4042  loss: 0.32955\n",
      "echo: 4043  loss: 0.32955\n",
      "echo: 4044  loss: 0.32955\n",
      "echo: 4045  loss: 0.32955\n",
      "echo: 4046  loss: 0.32955\n",
      "echo: 4047  loss: 0.32955\n",
      "echo: 4048  loss: 0.32955\n",
      "echo: 4049  loss: 0.32955\n",
      "echo: 4050  loss: 0.32955\n",
      "echo: 4051  loss: 0.32954\n",
      "echo: 4052  loss: 0.32954\n",
      "echo: 4053  loss: 0.32954\n",
      "echo: 4054  loss: 0.32954\n",
      "echo: 4055  loss: 0.32954\n",
      "echo: 4056  loss: 0.32954\n",
      "echo: 4057  loss: 0.32954\n",
      "echo: 4058  loss: 0.32954\n",
      "echo: 4059  loss: 0.32954\n",
      "echo: 4060  loss: 0.32954\n",
      "echo: 4061  loss: 0.32954\n",
      "echo: 4062  loss: 0.32954\n",
      "echo: 4063  loss: 0.32954\n",
      "echo: 4064  loss: 0.32953\n",
      "echo: 4065  loss: 0.32953\n",
      "echo: 4066  loss: 0.32953\n",
      "echo: 4067  loss: 0.32953\n",
      "echo: 4068  loss: 0.32953\n",
      "echo: 4069  loss: 0.32953\n",
      "echo: 4070  loss: 0.32953\n",
      "echo: 4071  loss: 0.32953\n",
      "echo: 4072  loss: 0.32953\n",
      "echo: 4073  loss: 0.32953\n",
      "echo: 4074  loss: 0.32953\n",
      "echo: 4075  loss: 0.32953\n",
      "echo: 4076  loss: 0.32953\n",
      "echo: 4077  loss: 0.32952\n",
      "echo: 4078  loss: 0.32952\n",
      "echo: 4079  loss: 0.32952\n",
      "echo: 4080  loss: 0.32952\n",
      "echo: 4081  loss: 0.32952\n",
      "echo: 4082  loss: 0.32952\n",
      "echo: 4083  loss: 0.32952\n",
      "echo: 4084  loss: 0.32952\n",
      "echo: 4085  loss: 0.32952\n",
      "echo: 4086  loss: 0.32952\n",
      "echo: 4087  loss: 0.32952\n",
      "echo: 4088  loss: 0.32952\n",
      "echo: 4089  loss: 0.32952\n",
      "echo: 4090  loss: 0.32952\n",
      "echo: 4091  loss: 0.32951\n",
      "echo: 4092  loss: 0.32951\n",
      "echo: 4093  loss: 0.32951\n",
      "echo: 4094  loss: 0.32951\n",
      "echo: 4095  loss: 0.32951\n",
      "echo: 4096  loss: 0.32951\n",
      "echo: 4097  loss: 0.32951\n",
      "echo: 4098  loss: 0.32951\n",
      "echo: 4099  loss: 0.32951\n",
      "echo: 4100  loss: 0.32951\n",
      "echo: 4101  loss: 0.32951\n",
      "echo: 4102  loss: 0.32951\n",
      "echo: 4103  loss: 0.32951\n",
      "echo: 4104  loss: 0.3295\n",
      "echo: 4105  loss: 0.3295\n",
      "echo: 4106  loss: 0.3295\n",
      "echo: 4107  loss: 0.3295\n",
      "echo: 4108  loss: 0.3295\n",
      "echo: 4109  loss: 0.3295\n",
      "echo: 4110  loss: 0.3295\n",
      "echo: 4111  loss: 0.3295\n",
      "echo: 4112  loss: 0.3295\n",
      "echo: 4113  loss: 0.3295\n",
      "echo: 4114  loss: 0.3295\n",
      "echo: 4115  loss: 0.3295\n",
      "echo: 4116  loss: 0.3295\n",
      "echo: 4117  loss: 0.32949\n",
      "echo: 4118  loss: 0.32949\n",
      "echo: 4119  loss: 0.32949\n",
      "echo: 4120  loss: 0.32949\n",
      "echo: 4121  loss: 0.32949\n",
      "echo: 4122  loss: 0.32949\n",
      "echo: 4123  loss: 0.32949\n",
      "echo: 4124  loss: 0.32949\n",
      "echo: 4125  loss: 0.32949\n",
      "echo: 4126  loss: 0.32949\n",
      "echo: 4127  loss: 0.32949\n",
      "echo: 4128  loss: 0.32949\n",
      "echo: 4129  loss: 0.32949\n",
      "echo: 4130  loss: 0.32948\n",
      "echo: 4131  loss: 0.32948\n",
      "echo: 4132  loss: 0.32948\n",
      "echo: 4133  loss: 0.32948\n",
      "echo: 4134  loss: 0.32948\n",
      "echo: 4135  loss: 0.32948\n",
      "echo: 4136  loss: 0.32948\n",
      "echo: 4137  loss: 0.32948\n",
      "echo: 4138  loss: 0.32948\n",
      "echo: 4139  loss: 0.32948\n",
      "echo: 4140  loss: 0.32948\n",
      "echo: 4141  loss: 0.32948\n",
      "echo: 4142  loss: 0.32948\n",
      "echo: 4143  loss: 0.32948\n",
      "echo: 4144  loss: 0.32947\n",
      "echo: 4145  loss: 0.32947\n",
      "echo: 4146  loss: 0.32947\n",
      "echo: 4147  loss: 0.32947\n",
      "echo: 4148  loss: 0.32947\n",
      "echo: 4149  loss: 0.32947\n",
      "echo: 4150  loss: 0.32947\n",
      "echo: 4151  loss: 0.32947\n",
      "echo: 4152  loss: 0.32947\n",
      "echo: 4153  loss: 0.32947\n",
      "echo: 4154  loss: 0.32947\n",
      "echo: 4155  loss: 0.32947\n",
      "echo: 4156  loss: 0.32947\n",
      "echo: 4157  loss: 0.32947\n",
      "echo: 4158  loss: 0.32946\n",
      "echo: 4159  loss: 0.32946\n",
      "echo: 4160  loss: 0.32946\n",
      "echo: 4161  loss: 0.32946\n",
      "echo: 4162  loss: 0.32946\n",
      "echo: 4163  loss: 0.32946\n",
      "echo: 4164  loss: 0.32946\n",
      "echo: 4165  loss: 0.32946\n",
      "echo: 4166  loss: 0.32946\n",
      "echo: 4167  loss: 0.32946\n",
      "echo: 4168  loss: 0.32946\n",
      "echo: 4169  loss: 0.32946\n",
      "echo: 4170  loss: 0.32946\n",
      "echo: 4171  loss: 0.32945\n",
      "echo: 4172  loss: 0.32945\n",
      "echo: 4173  loss: 0.32945\n",
      "echo: 4174  loss: 0.32945\n",
      "echo: 4175  loss: 0.32945\n",
      "echo: 4176  loss: 0.32945\n",
      "echo: 4177  loss: 0.32945\n",
      "echo: 4178  loss: 0.32945\n",
      "echo: 4179  loss: 0.32945\n",
      "echo: 4180  loss: 0.32945\n",
      "echo: 4181  loss: 0.32945\n",
      "echo: 4182  loss: 0.32945\n",
      "echo: 4183  loss: 0.32945\n",
      "echo: 4184  loss: 0.32945\n",
      "echo: 4185  loss: 0.32944\n",
      "echo: 4186  loss: 0.32944\n",
      "echo: 4187  loss: 0.32944\n",
      "echo: 4188  loss: 0.32944\n",
      "echo: 4189  loss: 0.32944\n",
      "echo: 4190  loss: 0.32944\n",
      "echo: 4191  loss: 0.32944\n",
      "echo: 4192  loss: 0.32944\n",
      "echo: 4193  loss: 0.32944\n",
      "echo: 4194  loss: 0.32944\n",
      "echo: 4195  loss: 0.32944\n",
      "echo: 4196  loss: 0.32944\n",
      "echo: 4197  loss: 0.32944\n",
      "echo: 4198  loss: 0.32944\n",
      "echo: 4199  loss: 0.32943\n",
      "echo: 4200  loss: 0.32943\n",
      "echo: 4201  loss: 0.32943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 4202  loss: 0.32943\n",
      "echo: 4203  loss: 0.32943\n",
      "echo: 4204  loss: 0.32943\n",
      "echo: 4205  loss: 0.32943\n",
      "echo: 4206  loss: 0.32943\n",
      "echo: 4207  loss: 0.32943\n",
      "echo: 4208  loss: 0.32943\n",
      "echo: 4209  loss: 0.32943\n",
      "echo: 4210  loss: 0.32943\n",
      "echo: 4211  loss: 0.32943\n",
      "echo: 4212  loss: 0.32943\n",
      "echo: 4213  loss: 0.32943\n",
      "echo: 4214  loss: 0.32942\n",
      "echo: 4215  loss: 0.32942\n",
      "echo: 4216  loss: 0.32942\n",
      "echo: 4217  loss: 0.32942\n",
      "echo: 4218  loss: 0.32942\n",
      "echo: 4219  loss: 0.32942\n",
      "echo: 4220  loss: 0.32942\n",
      "echo: 4221  loss: 0.32942\n",
      "echo: 4222  loss: 0.32942\n",
      "echo: 4223  loss: 0.32942\n",
      "echo: 4224  loss: 0.32942\n",
      "echo: 4225  loss: 0.32942\n",
      "echo: 4226  loss: 0.32942\n",
      "echo: 4227  loss: 0.32942\n",
      "echo: 4228  loss: 0.32941\n",
      "echo: 4229  loss: 0.32941\n",
      "echo: 4230  loss: 0.32941\n",
      "echo: 4231  loss: 0.32941\n",
      "echo: 4232  loss: 0.32941\n",
      "echo: 4233  loss: 0.32941\n",
      "echo: 4234  loss: 0.32941\n",
      "echo: 4235  loss: 0.32941\n",
      "echo: 4236  loss: 0.32941\n",
      "echo: 4237  loss: 0.32941\n",
      "echo: 4238  loss: 0.32941\n",
      "echo: 4239  loss: 0.32941\n",
      "echo: 4240  loss: 0.32941\n",
      "echo: 4241  loss: 0.32941\n",
      "echo: 4242  loss: 0.3294\n",
      "echo: 4243  loss: 0.3294\n",
      "echo: 4244  loss: 0.3294\n",
      "echo: 4245  loss: 0.3294\n",
      "echo: 4246  loss: 0.3294\n",
      "echo: 4247  loss: 0.3294\n",
      "echo: 4248  loss: 0.3294\n",
      "echo: 4249  loss: 0.3294\n",
      "echo: 4250  loss: 0.3294\n",
      "echo: 4251  loss: 0.3294\n",
      "echo: 4252  loss: 0.3294\n",
      "echo: 4253  loss: 0.3294\n",
      "echo: 4254  loss: 0.3294\n",
      "echo: 4255  loss: 0.3294\n",
      "echo: 4256  loss: 0.3294\n",
      "echo: 4257  loss: 0.32939\n",
      "echo: 4258  loss: 0.32939\n",
      "echo: 4259  loss: 0.32939\n",
      "echo: 4260  loss: 0.32939\n",
      "echo: 4261  loss: 0.32939\n",
      "echo: 4262  loss: 0.32939\n",
      "echo: 4263  loss: 0.32939\n",
      "echo: 4264  loss: 0.32939\n",
      "echo: 4265  loss: 0.32939\n",
      "echo: 4266  loss: 0.32939\n",
      "echo: 4267  loss: 0.32939\n",
      "echo: 4268  loss: 0.32939\n",
      "echo: 4269  loss: 0.32939\n",
      "echo: 4270  loss: 0.32939\n",
      "echo: 4271  loss: 0.32939\n",
      "echo: 4272  loss: 0.32938\n",
      "echo: 4273  loss: 0.32938\n",
      "echo: 4274  loss: 0.32938\n",
      "echo: 4275  loss: 0.32938\n",
      "echo: 4276  loss: 0.32938\n",
      "echo: 4277  loss: 0.32938\n",
      "echo: 4278  loss: 0.32938\n",
      "echo: 4279  loss: 0.32938\n",
      "echo: 4280  loss: 0.32938\n",
      "echo: 4281  loss: 0.32938\n",
      "echo: 4282  loss: 0.32938\n",
      "echo: 4283  loss: 0.32938\n",
      "echo: 4284  loss: 0.32938\n",
      "echo: 4285  loss: 0.32938\n",
      "echo: 4286  loss: 0.32938\n",
      "echo: 4287  loss: 0.32937\n",
      "echo: 4288  loss: 0.32937\n",
      "echo: 4289  loss: 0.32937\n",
      "echo: 4290  loss: 0.32937\n",
      "echo: 4291  loss: 0.32937\n",
      "echo: 4292  loss: 0.32937\n",
      "echo: 4293  loss: 0.32937\n",
      "echo: 4294  loss: 0.32937\n",
      "echo: 4295  loss: 0.32937\n",
      "echo: 4296  loss: 0.32937\n",
      "echo: 4297  loss: 0.32937\n",
      "echo: 4298  loss: 0.32937\n",
      "echo: 4299  loss: 0.32937\n",
      "echo: 4300  loss: 0.32937\n",
      "echo: 4301  loss: 0.32937\n",
      "echo: 4302  loss: 0.32936\n",
      "echo: 4303  loss: 0.32936\n",
      "echo: 4304  loss: 0.32936\n",
      "echo: 4305  loss: 0.32936\n",
      "echo: 4306  loss: 0.32936\n",
      "echo: 4307  loss: 0.32936\n",
      "echo: 4308  loss: 0.32936\n",
      "echo: 4309  loss: 0.32936\n",
      "echo: 4310  loss: 0.32936\n",
      "echo: 4311  loss: 0.32936\n",
      "echo: 4312  loss: 0.32936\n",
      "echo: 4313  loss: 0.32936\n",
      "echo: 4314  loss: 0.32936\n",
      "echo: 4315  loss: 0.32936\n",
      "echo: 4316  loss: 0.32936\n",
      "echo: 4317  loss: 0.32935\n",
      "echo: 4318  loss: 0.32935\n",
      "echo: 4319  loss: 0.32935\n",
      "echo: 4320  loss: 0.32935\n",
      "echo: 4321  loss: 0.32935\n",
      "echo: 4322  loss: 0.32935\n",
      "echo: 4323  loss: 0.32935\n",
      "echo: 4324  loss: 0.32935\n",
      "echo: 4325  loss: 0.32935\n",
      "echo: 4326  loss: 0.32935\n",
      "echo: 4327  loss: 0.32935\n",
      "echo: 4328  loss: 0.32935\n",
      "echo: 4329  loss: 0.32935\n",
      "echo: 4330  loss: 0.32935\n",
      "echo: 4331  loss: 0.32935\n",
      "echo: 4332  loss: 0.32934\n",
      "echo: 4333  loss: 0.32934\n",
      "echo: 4334  loss: 0.32934\n",
      "echo: 4335  loss: 0.32934\n",
      "echo: 4336  loss: 0.32934\n",
      "echo: 4337  loss: 0.32934\n",
      "echo: 4338  loss: 0.32934\n",
      "echo: 4339  loss: 0.32934\n",
      "echo: 4340  loss: 0.32934\n",
      "echo: 4341  loss: 0.32934\n",
      "echo: 4342  loss: 0.32934\n",
      "echo: 4343  loss: 0.32934\n",
      "echo: 4344  loss: 0.32934\n",
      "echo: 4345  loss: 0.32934\n",
      "echo: 4346  loss: 0.32934\n",
      "echo: 4347  loss: 0.32933\n",
      "echo: 4348  loss: 0.32933\n",
      "echo: 4349  loss: 0.32933\n",
      "echo: 4350  loss: 0.32933\n",
      "echo: 4351  loss: 0.32933\n",
      "echo: 4352  loss: 0.32933\n",
      "echo: 4353  loss: 0.32933\n",
      "echo: 4354  loss: 0.32933\n",
      "echo: 4355  loss: 0.32933\n",
      "echo: 4356  loss: 0.32933\n",
      "echo: 4357  loss: 0.32933\n",
      "echo: 4358  loss: 0.32933\n",
      "echo: 4359  loss: 0.32933\n",
      "echo: 4360  loss: 0.32933\n",
      "echo: 4361  loss: 0.32933\n",
      "echo: 4362  loss: 0.32933\n",
      "echo: 4363  loss: 0.32932\n",
      "echo: 4364  loss: 0.32932\n",
      "echo: 4365  loss: 0.32932\n",
      "echo: 4366  loss: 0.32932\n",
      "echo: 4367  loss: 0.32932\n",
      "echo: 4368  loss: 0.32932\n",
      "echo: 4369  loss: 0.32932\n",
      "echo: 4370  loss: 0.32932\n",
      "echo: 4371  loss: 0.32932\n",
      "echo: 4372  loss: 0.32932\n",
      "echo: 4373  loss: 0.32932\n",
      "echo: 4374  loss: 0.32932\n",
      "echo: 4375  loss: 0.32932\n",
      "echo: 4376  loss: 0.32932\n",
      "echo: 4377  loss: 0.32932\n",
      "echo: 4378  loss: 0.32932\n",
      "echo: 4379  loss: 0.32931\n",
      "echo: 4380  loss: 0.32931\n",
      "echo: 4381  loss: 0.32931\n",
      "echo: 4382  loss: 0.32931\n",
      "echo: 4383  loss: 0.32931\n",
      "echo: 4384  loss: 0.32931\n",
      "echo: 4385  loss: 0.32931\n",
      "echo: 4386  loss: 0.32931\n",
      "echo: 4387  loss: 0.32931\n",
      "echo: 4388  loss: 0.32931\n",
      "echo: 4389  loss: 0.32931\n",
      "echo: 4390  loss: 0.32931\n",
      "echo: 4391  loss: 0.32931\n",
      "echo: 4392  loss: 0.32931\n",
      "echo: 4393  loss: 0.32931\n",
      "echo: 4394  loss: 0.32931\n",
      "echo: 4395  loss: 0.3293\n",
      "echo: 4396  loss: 0.3293\n",
      "echo: 4397  loss: 0.3293\n",
      "echo: 4398  loss: 0.3293\n",
      "echo: 4399  loss: 0.3293\n",
      "echo: 4400  loss: 0.3293\n",
      "echo: 4401  loss: 0.3293\n",
      "echo: 4402  loss: 0.3293\n",
      "echo: 4403  loss: 0.3293\n",
      "echo: 4404  loss: 0.3293\n",
      "echo: 4405  loss: 0.3293\n",
      "echo: 4406  loss: 0.3293\n",
      "echo: 4407  loss: 0.3293\n",
      "echo: 4408  loss: 0.3293\n",
      "echo: 4409  loss: 0.3293\n",
      "echo: 4410  loss: 0.3293\n",
      "echo: 4411  loss: 0.32929\n",
      "echo: 4412  loss: 0.32929\n",
      "echo: 4413  loss: 0.32929\n",
      "echo: 4414  loss: 0.32929\n",
      "echo: 4415  loss: 0.32929\n",
      "echo: 4416  loss: 0.32929\n",
      "echo: 4417  loss: 0.32929\n",
      "echo: 4418  loss: 0.32929\n",
      "echo: 4419  loss: 0.32929\n",
      "echo: 4420  loss: 0.32929\n",
      "echo: 4421  loss: 0.32929\n",
      "echo: 4422  loss: 0.32929\n",
      "echo: 4423  loss: 0.32929\n",
      "echo: 4424  loss: 0.32929\n",
      "echo: 4425  loss: 0.32929\n",
      "echo: 4426  loss: 0.32929\n",
      "echo: 4427  loss: 0.32928\n",
      "echo: 4428  loss: 0.32928\n",
      "echo: 4429  loss: 0.32928\n",
      "echo: 4430  loss: 0.32928\n",
      "echo: 4431  loss: 0.32928\n",
      "echo: 4432  loss: 0.32928\n",
      "echo: 4433  loss: 0.32928\n",
      "echo: 4434  loss: 0.32928\n",
      "echo: 4435  loss: 0.32928\n",
      "echo: 4436  loss: 0.32928\n",
      "echo: 4437  loss: 0.32928\n",
      "echo: 4438  loss: 0.32928\n",
      "echo: 4439  loss: 0.32928\n",
      "echo: 4440  loss: 0.32928\n",
      "echo: 4441  loss: 0.32928\n",
      "echo: 4442  loss: 0.32928\n",
      "echo: 4443  loss: 0.32927\n",
      "echo: 4444  loss: 0.32927\n",
      "echo: 4445  loss: 0.32927\n",
      "echo: 4446  loss: 0.32927\n",
      "echo: 4447  loss: 0.32927\n",
      "echo: 4448  loss: 0.32927\n",
      "echo: 4449  loss: 0.32927\n",
      "echo: 4450  loss: 0.32927\n",
      "echo: 4451  loss: 0.32927\n",
      "echo: 4452  loss: 0.32927\n",
      "echo: 4453  loss: 0.32927\n",
      "echo: 4454  loss: 0.32927\n",
      "echo: 4455  loss: 0.32927\n",
      "echo: 4456  loss: 0.32927\n",
      "echo: 4457  loss: 0.32927\n",
      "echo: 4458  loss: 0.32927\n",
      "echo: 4459  loss: 0.32927\n",
      "echo: 4460  loss: 0.32926\n",
      "echo: 4461  loss: 0.32926\n",
      "echo: 4462  loss: 0.32926\n",
      "echo: 4463  loss: 0.32926\n",
      "echo: 4464  loss: 0.32926\n",
      "echo: 4465  loss: 0.32926\n",
      "echo: 4466  loss: 0.32926\n",
      "echo: 4467  loss: 0.32926\n",
      "echo: 4468  loss: 0.32926\n",
      "echo: 4469  loss: 0.32926\n",
      "echo: 4470  loss: 0.32926\n",
      "echo: 4471  loss: 0.32926\n",
      "echo: 4472  loss: 0.32926\n",
      "echo: 4473  loss: 0.32926\n",
      "echo: 4474  loss: 0.32926\n",
      "echo: 4475  loss: 0.32926\n",
      "echo: 4476  loss: 0.32926\n",
      "echo: 4477  loss: 0.32925\n",
      "echo: 4478  loss: 0.32925\n",
      "echo: 4479  loss: 0.32925\n",
      "echo: 4480  loss: 0.32925\n",
      "echo: 4481  loss: 0.32925\n",
      "echo: 4482  loss: 0.32925\n",
      "echo: 4483  loss: 0.32925\n",
      "echo: 4484  loss: 0.32925\n",
      "echo: 4485  loss: 0.32925\n",
      "echo: 4486  loss: 0.32925\n",
      "echo: 4487  loss: 0.32925\n",
      "echo: 4488  loss: 0.32925\n",
      "echo: 4489  loss: 0.32925\n",
      "echo: 4490  loss: 0.32925\n",
      "echo: 4491  loss: 0.32925\n",
      "echo: 4492  loss: 0.32925\n",
      "echo: 4493  loss: 0.32925\n",
      "echo: 4494  loss: 0.32924\n",
      "echo: 4495  loss: 0.32924\n",
      "echo: 4496  loss: 0.32924\n",
      "echo: 4497  loss: 0.32924\n",
      "echo: 4498  loss: 0.32924\n",
      "echo: 4499  loss: 0.32924\n",
      "echo: 4500  loss: 0.32924\n",
      "echo: 4501  loss: 0.32924\n",
      "echo: 4502  loss: 0.32924\n",
      "echo: 4503  loss: 0.32924\n",
      "echo: 4504  loss: 0.32924\n",
      "echo: 4505  loss: 0.32924\n",
      "echo: 4506  loss: 0.32924\n",
      "echo: 4507  loss: 0.32924\n",
      "echo: 4508  loss: 0.32924\n",
      "echo: 4509  loss: 0.32924\n",
      "echo: 4510  loss: 0.32924\n",
      "echo: 4511  loss: 0.32923\n",
      "echo: 4512  loss: 0.32923\n",
      "echo: 4513  loss: 0.32923\n",
      "echo: 4514  loss: 0.32923\n",
      "echo: 4515  loss: 0.32923\n",
      "echo: 4516  loss: 0.32923\n",
      "echo: 4517  loss: 0.32923\n",
      "echo: 4518  loss: 0.32923\n",
      "echo: 4519  loss: 0.32923\n",
      "echo: 4520  loss: 0.32923\n",
      "echo: 4521  loss: 0.32923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 4522  loss: 0.32923\n",
      "echo: 4523  loss: 0.32923\n",
      "echo: 4524  loss: 0.32923\n",
      "echo: 4525  loss: 0.32923\n",
      "echo: 4526  loss: 0.32923\n",
      "echo: 4527  loss: 0.32923\n",
      "echo: 4528  loss: 0.32922\n",
      "echo: 4529  loss: 0.32922\n",
      "echo: 4530  loss: 0.32922\n",
      "echo: 4531  loss: 0.32922\n",
      "echo: 4532  loss: 0.32922\n",
      "echo: 4533  loss: 0.32922\n",
      "echo: 4534  loss: 0.32922\n",
      "echo: 4535  loss: 0.32922\n",
      "echo: 4536  loss: 0.32922\n",
      "echo: 4537  loss: 0.32922\n",
      "echo: 4538  loss: 0.32922\n",
      "echo: 4539  loss: 0.32922\n",
      "echo: 4540  loss: 0.32922\n",
      "echo: 4541  loss: 0.32922\n",
      "echo: 4542  loss: 0.32922\n",
      "echo: 4543  loss: 0.32922\n",
      "echo: 4544  loss: 0.32922\n",
      "echo: 4545  loss: 0.32922\n",
      "echo: 4546  loss: 0.32921\n",
      "echo: 4547  loss: 0.32921\n",
      "echo: 4548  loss: 0.32921\n",
      "echo: 4549  loss: 0.32921\n",
      "echo: 4550  loss: 0.32921\n",
      "echo: 4551  loss: 0.32921\n",
      "echo: 4552  loss: 0.32921\n",
      "echo: 4553  loss: 0.32921\n",
      "echo: 4554  loss: 0.32921\n",
      "echo: 4555  loss: 0.32921\n",
      "echo: 4556  loss: 0.32921\n",
      "echo: 4557  loss: 0.32921\n",
      "echo: 4558  loss: 0.32921\n",
      "echo: 4559  loss: 0.32921\n",
      "echo: 4560  loss: 0.32921\n",
      "echo: 4561  loss: 0.32921\n",
      "echo: 4562  loss: 0.32921\n",
      "echo: 4563  loss: 0.3292\n",
      "echo: 4564  loss: 0.3292\n",
      "echo: 4565  loss: 0.3292\n",
      "echo: 4566  loss: 0.3292\n",
      "echo: 4567  loss: 0.3292\n",
      "echo: 4568  loss: 0.3292\n",
      "echo: 4569  loss: 0.3292\n",
      "echo: 4570  loss: 0.3292\n",
      "echo: 4571  loss: 0.3292\n",
      "echo: 4572  loss: 0.3292\n",
      "echo: 4573  loss: 0.3292\n",
      "echo: 4574  loss: 0.3292\n",
      "echo: 4575  loss: 0.3292\n",
      "echo: 4576  loss: 0.3292\n",
      "echo: 4577  loss: 0.3292\n",
      "echo: 4578  loss: 0.3292\n",
      "echo: 4579  loss: 0.3292\n",
      "echo: 4580  loss: 0.3292\n",
      "echo: 4581  loss: 0.32919\n",
      "echo: 4582  loss: 0.32919\n",
      "echo: 4583  loss: 0.32919\n",
      "echo: 4584  loss: 0.32919\n",
      "echo: 4585  loss: 0.32919\n",
      "echo: 4586  loss: 0.32919\n",
      "echo: 4587  loss: 0.32919\n",
      "echo: 4588  loss: 0.32919\n",
      "echo: 4589  loss: 0.32919\n",
      "echo: 4590  loss: 0.32919\n",
      "echo: 4591  loss: 0.32919\n",
      "echo: 4592  loss: 0.32919\n",
      "echo: 4593  loss: 0.32919\n",
      "echo: 4594  loss: 0.32919\n",
      "echo: 4595  loss: 0.32919\n",
      "echo: 4596  loss: 0.32919\n",
      "echo: 4597  loss: 0.32919\n",
      "echo: 4598  loss: 0.32919\n",
      "echo: 4599  loss: 0.32919\n",
      "echo: 4600  loss: 0.32918\n",
      "echo: 4601  loss: 0.32918\n",
      "echo: 4602  loss: 0.32918\n",
      "echo: 4603  loss: 0.32918\n",
      "echo: 4604  loss: 0.32918\n",
      "echo: 4605  loss: 0.32918\n",
      "echo: 4606  loss: 0.32918\n",
      "echo: 4607  loss: 0.32918\n",
      "echo: 4608  loss: 0.32918\n",
      "echo: 4609  loss: 0.32918\n",
      "echo: 4610  loss: 0.32918\n",
      "echo: 4611  loss: 0.32918\n",
      "echo: 4612  loss: 0.32918\n",
      "echo: 4613  loss: 0.32918\n",
      "echo: 4614  loss: 0.32918\n",
      "echo: 4615  loss: 0.32918\n",
      "echo: 4616  loss: 0.32918\n",
      "echo: 4617  loss: 0.32918\n",
      "echo: 4618  loss: 0.32917\n",
      "echo: 4619  loss: 0.32917\n",
      "echo: 4620  loss: 0.32917\n",
      "echo: 4621  loss: 0.32917\n",
      "echo: 4622  loss: 0.32917\n",
      "echo: 4623  loss: 0.32917\n",
      "echo: 4624  loss: 0.32917\n",
      "echo: 4625  loss: 0.32917\n",
      "echo: 4626  loss: 0.32917\n",
      "echo: 4627  loss: 0.32917\n",
      "echo: 4628  loss: 0.32917\n",
      "echo: 4629  loss: 0.32917\n",
      "echo: 4630  loss: 0.32917\n",
      "echo: 4631  loss: 0.32917\n",
      "echo: 4632  loss: 0.32917\n",
      "echo: 4633  loss: 0.32917\n",
      "echo: 4634  loss: 0.32917\n",
      "echo: 4635  loss: 0.32917\n",
      "echo: 4636  loss: 0.32916\n",
      "echo: 4637  loss: 0.32916\n",
      "echo: 4638  loss: 0.32916\n",
      "echo: 4639  loss: 0.32916\n",
      "echo: 4640  loss: 0.32916\n",
      "echo: 4641  loss: 0.32916\n",
      "echo: 4642  loss: 0.32916\n",
      "echo: 4643  loss: 0.32916\n",
      "echo: 4644  loss: 0.32916\n",
      "echo: 4645  loss: 0.32916\n",
      "echo: 4646  loss: 0.32916\n",
      "echo: 4647  loss: 0.32916\n",
      "echo: 4648  loss: 0.32916\n",
      "echo: 4649  loss: 0.32916\n",
      "echo: 4650  loss: 0.32916\n",
      "echo: 4651  loss: 0.32916\n",
      "echo: 4652  loss: 0.32916\n",
      "echo: 4653  loss: 0.32916\n",
      "echo: 4654  loss: 0.32916\n",
      "echo: 4655  loss: 0.32915\n",
      "echo: 4656  loss: 0.32915\n",
      "echo: 4657  loss: 0.32915\n",
      "echo: 4658  loss: 0.32915\n",
      "echo: 4659  loss: 0.32915\n",
      "echo: 4660  loss: 0.32915\n",
      "echo: 4661  loss: 0.32915\n",
      "echo: 4662  loss: 0.32915\n",
      "echo: 4663  loss: 0.32915\n",
      "echo: 4664  loss: 0.32915\n",
      "echo: 4665  loss: 0.32915\n",
      "echo: 4666  loss: 0.32915\n",
      "echo: 4667  loss: 0.32915\n",
      "echo: 4668  loss: 0.32915\n",
      "echo: 4669  loss: 0.32915\n",
      "echo: 4670  loss: 0.32915\n",
      "echo: 4671  loss: 0.32915\n",
      "echo: 4672  loss: 0.32915\n",
      "echo: 4673  loss: 0.32915\n",
      "echo: 4674  loss: 0.32914\n",
      "echo: 4675  loss: 0.32914\n",
      "echo: 4676  loss: 0.32914\n",
      "echo: 4677  loss: 0.32914\n",
      "echo: 4678  loss: 0.32914\n",
      "echo: 4679  loss: 0.32914\n",
      "echo: 4680  loss: 0.32914\n",
      "echo: 4681  loss: 0.32914\n",
      "echo: 4682  loss: 0.32914\n",
      "echo: 4683  loss: 0.32914\n",
      "echo: 4684  loss: 0.32914\n",
      "echo: 4685  loss: 0.32914\n",
      "echo: 4686  loss: 0.32914\n",
      "echo: 4687  loss: 0.32914\n",
      "echo: 4688  loss: 0.32914\n",
      "echo: 4689  loss: 0.32914\n",
      "echo: 4690  loss: 0.32914\n",
      "echo: 4691  loss: 0.32914\n",
      "echo: 4692  loss: 0.32914\n",
      "echo: 4693  loss: 0.32913\n",
      "echo: 4694  loss: 0.32913\n",
      "echo: 4695  loss: 0.32913\n",
      "echo: 4696  loss: 0.32913\n",
      "echo: 4697  loss: 0.32913\n",
      "echo: 4698  loss: 0.32913\n",
      "echo: 4699  loss: 0.32913\n",
      "echo: 4700  loss: 0.32913\n",
      "echo: 4701  loss: 0.32913\n",
      "echo: 4702  loss: 0.32913\n",
      "echo: 4703  loss: 0.32913\n",
      "echo: 4704  loss: 0.32913\n",
      "echo: 4705  loss: 0.32913\n",
      "echo: 4706  loss: 0.32913\n",
      "echo: 4707  loss: 0.32913\n",
      "echo: 4708  loss: 0.32913\n",
      "echo: 4709  loss: 0.32913\n",
      "echo: 4710  loss: 0.32913\n",
      "echo: 4711  loss: 0.32913\n",
      "echo: 4712  loss: 0.32913\n",
      "echo: 4713  loss: 0.32912\n",
      "echo: 4714  loss: 0.32912\n",
      "echo: 4715  loss: 0.32912\n",
      "echo: 4716  loss: 0.32912\n",
      "echo: 4717  loss: 0.32912\n",
      "echo: 4718  loss: 0.32912\n",
      "echo: 4719  loss: 0.32912\n",
      "echo: 4720  loss: 0.32912\n",
      "echo: 4721  loss: 0.32912\n",
      "echo: 4722  loss: 0.32912\n",
      "echo: 4723  loss: 0.32912\n",
      "echo: 4724  loss: 0.32912\n",
      "echo: 4725  loss: 0.32912\n",
      "echo: 4726  loss: 0.32912\n",
      "echo: 4727  loss: 0.32912\n",
      "echo: 4728  loss: 0.32912\n",
      "echo: 4729  loss: 0.32912\n",
      "echo: 4730  loss: 0.32912\n",
      "echo: 4731  loss: 0.32912\n",
      "echo: 4732  loss: 0.32912\n",
      "echo: 4733  loss: 0.32911\n",
      "echo: 4734  loss: 0.32911\n",
      "echo: 4735  loss: 0.32911\n",
      "echo: 4736  loss: 0.32911\n",
      "echo: 4737  loss: 0.32911\n",
      "echo: 4738  loss: 0.32911\n",
      "echo: 4739  loss: 0.32911\n",
      "echo: 4740  loss: 0.32911\n",
      "echo: 4741  loss: 0.32911\n",
      "echo: 4742  loss: 0.32911\n",
      "echo: 4743  loss: 0.32911\n",
      "echo: 4744  loss: 0.32911\n",
      "echo: 4745  loss: 0.32911\n",
      "echo: 4746  loss: 0.32911\n",
      "echo: 4747  loss: 0.32911\n",
      "echo: 4748  loss: 0.32911\n",
      "echo: 4749  loss: 0.32911\n",
      "echo: 4750  loss: 0.32911\n",
      "echo: 4751  loss: 0.32911\n",
      "echo: 4752  loss: 0.32911\n",
      "echo: 4753  loss: 0.3291\n",
      "echo: 4754  loss: 0.3291\n",
      "echo: 4755  loss: 0.3291\n",
      "echo: 4756  loss: 0.3291\n",
      "echo: 4757  loss: 0.3291\n",
      "echo: 4758  loss: 0.3291\n",
      "echo: 4759  loss: 0.3291\n",
      "echo: 4760  loss: 0.3291\n",
      "echo: 4761  loss: 0.3291\n",
      "echo: 4762  loss: 0.3291\n",
      "echo: 4763  loss: 0.3291\n",
      "echo: 4764  loss: 0.3291\n",
      "echo: 4765  loss: 0.3291\n",
      "echo: 4766  loss: 0.3291\n",
      "echo: 4767  loss: 0.3291\n",
      "echo: 4768  loss: 0.3291\n",
      "echo: 4769  loss: 0.3291\n",
      "echo: 4770  loss: 0.3291\n",
      "echo: 4771  loss: 0.3291\n",
      "echo: 4772  loss: 0.3291\n",
      "echo: 4773  loss: 0.32909\n",
      "echo: 4774  loss: 0.32909\n",
      "echo: 4775  loss: 0.32909\n",
      "echo: 4776  loss: 0.32909\n",
      "echo: 4777  loss: 0.32909\n",
      "echo: 4778  loss: 0.32909\n",
      "echo: 4779  loss: 0.32909\n",
      "echo: 4780  loss: 0.32909\n",
      "echo: 4781  loss: 0.32909\n",
      "echo: 4782  loss: 0.32909\n",
      "echo: 4783  loss: 0.32909\n",
      "echo: 4784  loss: 0.32909\n",
      "echo: 4785  loss: 0.32909\n",
      "echo: 4786  loss: 0.32909\n",
      "echo: 4787  loss: 0.32909\n",
      "echo: 4788  loss: 0.32909\n",
      "echo: 4789  loss: 0.32909\n",
      "echo: 4790  loss: 0.32909\n",
      "echo: 4791  loss: 0.32909\n",
      "echo: 4792  loss: 0.32909\n",
      "echo: 4793  loss: 0.32908\n",
      "echo: 4794  loss: 0.32908\n",
      "echo: 4795  loss: 0.32908\n",
      "echo: 4796  loss: 0.32908\n",
      "echo: 4797  loss: 0.32908\n",
      "echo: 4798  loss: 0.32908\n",
      "echo: 4799  loss: 0.32908\n",
      "echo: 4800  loss: 0.32908\n",
      "echo: 4801  loss: 0.32908\n",
      "echo: 4802  loss: 0.32908\n",
      "echo: 4803  loss: 0.32908\n",
      "echo: 4804  loss: 0.32908\n",
      "echo: 4805  loss: 0.32908\n",
      "echo: 4806  loss: 0.32908\n",
      "echo: 4807  loss: 0.32908\n",
      "echo: 4808  loss: 0.32908\n",
      "echo: 4809  loss: 0.32908\n",
      "echo: 4810  loss: 0.32908\n",
      "echo: 4811  loss: 0.32908\n",
      "echo: 4812  loss: 0.32908\n",
      "echo: 4813  loss: 0.32908\n",
      "echo: 4814  loss: 0.32907\n",
      "echo: 4815  loss: 0.32907\n",
      "echo: 4816  loss: 0.32907\n",
      "echo: 4817  loss: 0.32907\n",
      "echo: 4818  loss: 0.32907\n",
      "echo: 4819  loss: 0.32907\n",
      "echo: 4820  loss: 0.32907\n",
      "echo: 4821  loss: 0.32907\n",
      "echo: 4822  loss: 0.32907\n",
      "echo: 4823  loss: 0.32907\n",
      "echo: 4824  loss: 0.32907\n",
      "echo: 4825  loss: 0.32907\n",
      "echo: 4826  loss: 0.32907\n",
      "echo: 4827  loss: 0.32907\n",
      "echo: 4828  loss: 0.32907\n",
      "echo: 4829  loss: 0.32907\n",
      "echo: 4830  loss: 0.32907\n",
      "echo: 4831  loss: 0.32907\n",
      "echo: 4832  loss: 0.32907\n",
      "echo: 4833  loss: 0.32907\n",
      "echo: 4834  loss: 0.32907\n",
      "echo: 4835  loss: 0.32906\n",
      "echo: 4836  loss: 0.32906\n",
      "echo: 4837  loss: 0.32906\n",
      "echo: 4838  loss: 0.32906\n",
      "echo: 4839  loss: 0.32906\n",
      "echo: 4840  loss: 0.32906\n",
      "echo: 4841  loss: 0.32906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 4842  loss: 0.32906\n",
      "echo: 4843  loss: 0.32906\n",
      "echo: 4844  loss: 0.32906\n",
      "echo: 4845  loss: 0.32906\n",
      "echo: 4846  loss: 0.32906\n",
      "echo: 4847  loss: 0.32906\n",
      "echo: 4848  loss: 0.32906\n",
      "echo: 4849  loss: 0.32906\n",
      "echo: 4850  loss: 0.32906\n",
      "echo: 4851  loss: 0.32906\n",
      "echo: 4852  loss: 0.32906\n",
      "echo: 4853  loss: 0.32906\n",
      "echo: 4854  loss: 0.32906\n",
      "echo: 4855  loss: 0.32906\n",
      "echo: 4856  loss: 0.32906\n",
      "echo: 4857  loss: 0.32905\n",
      "echo: 4858  loss: 0.32905\n",
      "echo: 4859  loss: 0.32905\n",
      "echo: 4860  loss: 0.32905\n",
      "echo: 4861  loss: 0.32905\n",
      "echo: 4862  loss: 0.32905\n",
      "echo: 4863  loss: 0.32905\n",
      "echo: 4864  loss: 0.32905\n",
      "echo: 4865  loss: 0.32905\n",
      "echo: 4866  loss: 0.32905\n",
      "echo: 4867  loss: 0.32905\n",
      "echo: 4868  loss: 0.32905\n",
      "echo: 4869  loss: 0.32905\n",
      "echo: 4870  loss: 0.32905\n",
      "echo: 4871  loss: 0.32905\n",
      "echo: 4872  loss: 0.32905\n",
      "echo: 4873  loss: 0.32905\n",
      "echo: 4874  loss: 0.32905\n",
      "echo: 4875  loss: 0.32905\n",
      "echo: 4876  loss: 0.32905\n",
      "echo: 4877  loss: 0.32905\n",
      "echo: 4878  loss: 0.32904\n",
      "echo: 4879  loss: 0.32904\n",
      "echo: 4880  loss: 0.32904\n",
      "echo: 4881  loss: 0.32904\n",
      "echo: 4882  loss: 0.32904\n",
      "echo: 4883  loss: 0.32904\n",
      "echo: 4884  loss: 0.32904\n",
      "echo: 4885  loss: 0.32904\n",
      "echo: 4886  loss: 0.32904\n",
      "echo: 4887  loss: 0.32904\n",
      "echo: 4888  loss: 0.32904\n",
      "echo: 4889  loss: 0.32904\n",
      "echo: 4890  loss: 0.32904\n",
      "echo: 4891  loss: 0.32904\n",
      "echo: 4892  loss: 0.32904\n",
      "echo: 4893  loss: 0.32904\n",
      "echo: 4894  loss: 0.32904\n",
      "echo: 4895  loss: 0.32904\n",
      "echo: 4896  loss: 0.32904\n",
      "echo: 4897  loss: 0.32904\n",
      "echo: 4898  loss: 0.32904\n",
      "echo: 4899  loss: 0.32904\n",
      "echo: 4900  loss: 0.32903\n",
      "echo: 4901  loss: 0.32903\n",
      "echo: 4902  loss: 0.32903\n",
      "echo: 4903  loss: 0.32903\n",
      "echo: 4904  loss: 0.32903\n",
      "echo: 4905  loss: 0.32903\n",
      "echo: 4906  loss: 0.32903\n",
      "echo: 4907  loss: 0.32903\n",
      "echo: 4908  loss: 0.32903\n",
      "echo: 4909  loss: 0.32903\n",
      "echo: 4910  loss: 0.32903\n",
      "echo: 4911  loss: 0.32903\n",
      "echo: 4912  loss: 0.32903\n",
      "echo: 4913  loss: 0.32903\n",
      "echo: 4914  loss: 0.32903\n",
      "echo: 4915  loss: 0.32903\n",
      "echo: 4916  loss: 0.32903\n",
      "echo: 4917  loss: 0.32903\n",
      "echo: 4918  loss: 0.32903\n",
      "echo: 4919  loss: 0.32903\n",
      "echo: 4920  loss: 0.32903\n",
      "echo: 4921  loss: 0.32903\n",
      "echo: 4922  loss: 0.32903\n",
      "echo: 4923  loss: 0.32902\n",
      "echo: 4924  loss: 0.32902\n",
      "echo: 4925  loss: 0.32902\n",
      "echo: 4926  loss: 0.32902\n",
      "echo: 4927  loss: 0.32902\n",
      "echo: 4928  loss: 0.32902\n",
      "echo: 4929  loss: 0.32902\n",
      "echo: 4930  loss: 0.32902\n",
      "echo: 4931  loss: 0.32902\n",
      "echo: 4932  loss: 0.32902\n",
      "echo: 4933  loss: 0.32902\n",
      "echo: 4934  loss: 0.32902\n",
      "echo: 4935  loss: 0.32902\n",
      "echo: 4936  loss: 0.32902\n",
      "echo: 4937  loss: 0.32902\n",
      "echo: 4938  loss: 0.32902\n",
      "echo: 4939  loss: 0.32902\n",
      "echo: 4940  loss: 0.32902\n",
      "echo: 4941  loss: 0.32902\n",
      "echo: 4942  loss: 0.32902\n",
      "echo: 4943  loss: 0.32902\n",
      "echo: 4944  loss: 0.32902\n",
      "echo: 4945  loss: 0.32901\n",
      "echo: 4946  loss: 0.32901\n",
      "echo: 4947  loss: 0.32901\n",
      "echo: 4948  loss: 0.32901\n",
      "echo: 4949  loss: 0.32901\n",
      "echo: 4950  loss: 0.32901\n",
      "echo: 4951  loss: 0.32901\n",
      "echo: 4952  loss: 0.32901\n",
      "echo: 4953  loss: 0.32901\n",
      "echo: 4954  loss: 0.32901\n",
      "echo: 4955  loss: 0.32901\n",
      "echo: 4956  loss: 0.32901\n",
      "echo: 4957  loss: 0.32901\n",
      "echo: 4958  loss: 0.32901\n",
      "echo: 4959  loss: 0.32901\n",
      "echo: 4960  loss: 0.32901\n",
      "echo: 4961  loss: 0.32901\n",
      "echo: 4962  loss: 0.32901\n",
      "echo: 4963  loss: 0.32901\n",
      "echo: 4964  loss: 0.32901\n",
      "echo: 4965  loss: 0.32901\n",
      "echo: 4966  loss: 0.32901\n",
      "echo: 4967  loss: 0.32901\n",
      "echo: 4968  loss: 0.329\n",
      "echo: 4969  loss: 0.329\n",
      "echo: 4970  loss: 0.329\n",
      "echo: 4971  loss: 0.329\n",
      "echo: 4972  loss: 0.329\n",
      "echo: 4973  loss: 0.329\n",
      "echo: 4974  loss: 0.329\n",
      "echo: 4975  loss: 0.329\n",
      "echo: 4976  loss: 0.329\n",
      "echo: 4977  loss: 0.329\n",
      "echo: 4978  loss: 0.329\n",
      "echo: 4979  loss: 0.329\n",
      "echo: 4980  loss: 0.329\n",
      "echo: 4981  loss: 0.329\n",
      "echo: 4982  loss: 0.329\n",
      "echo: 4983  loss: 0.329\n",
      "echo: 4984  loss: 0.329\n",
      "echo: 4985  loss: 0.329\n",
      "echo: 4986  loss: 0.329\n",
      "echo: 4987  loss: 0.329\n",
      "echo: 4988  loss: 0.329\n",
      "echo: 4989  loss: 0.329\n",
      "echo: 4990  loss: 0.329\n",
      "echo: 4991  loss: 0.32899\n",
      "echo: 4992  loss: 0.32899\n",
      "echo: 4993  loss: 0.32899\n",
      "echo: 4994  loss: 0.32899\n",
      "echo: 4995  loss: 0.32899\n",
      "echo: 4996  loss: 0.32899\n",
      "echo: 4997  loss: 0.32899\n",
      "echo: 4998  loss: 0.32899\n",
      "echo: 4999  loss: 0.32899\n",
      "echo: 5000  loss: 0.32899\n",
      "echo: 5001  loss: 0.32899\n",
      "echo: 5002  loss: 0.32899\n",
      "echo: 5003  loss: 0.32899\n",
      "echo: 5004  loss: 0.32899\n",
      "echo: 5005  loss: 0.32899\n",
      "echo: 5006  loss: 0.32899\n",
      "echo: 5007  loss: 0.32899\n",
      "echo: 5008  loss: 0.32899\n",
      "echo: 5009  loss: 0.32899\n",
      "echo: 5010  loss: 0.32899\n",
      "echo: 5011  loss: 0.32899\n",
      "echo: 5012  loss: 0.32899\n",
      "echo: 5013  loss: 0.32899\n",
      "echo: 5014  loss: 0.32899\n",
      "echo: 5015  loss: 0.32898\n",
      "echo: 5016  loss: 0.32898\n",
      "echo: 5017  loss: 0.32898\n",
      "echo: 5018  loss: 0.32898\n",
      "echo: 5019  loss: 0.32898\n",
      "echo: 5020  loss: 0.32898\n",
      "echo: 5021  loss: 0.32898\n",
      "echo: 5022  loss: 0.32898\n",
      "echo: 5023  loss: 0.32898\n",
      "echo: 5024  loss: 0.32898\n",
      "echo: 5025  loss: 0.32898\n",
      "echo: 5026  loss: 0.32898\n",
      "echo: 5027  loss: 0.32898\n",
      "echo: 5028  loss: 0.32898\n",
      "echo: 5029  loss: 0.32898\n",
      "echo: 5030  loss: 0.32898\n",
      "echo: 5031  loss: 0.32898\n",
      "echo: 5032  loss: 0.32898\n",
      "echo: 5033  loss: 0.32898\n",
      "echo: 5034  loss: 0.32898\n",
      "echo: 5035  loss: 0.32898\n",
      "echo: 5036  loss: 0.32898\n",
      "echo: 5037  loss: 0.32898\n",
      "echo: 5038  loss: 0.32898\n",
      "echo: 5039  loss: 0.32897\n",
      "echo: 5040  loss: 0.32897\n",
      "echo: 5041  loss: 0.32897\n",
      "echo: 5042  loss: 0.32897\n",
      "echo: 5043  loss: 0.32897\n",
      "echo: 5044  loss: 0.32897\n",
      "echo: 5045  loss: 0.32897\n",
      "echo: 5046  loss: 0.32897\n",
      "echo: 5047  loss: 0.32897\n",
      "echo: 5048  loss: 0.32897\n",
      "echo: 5049  loss: 0.32897\n",
      "echo: 5050  loss: 0.32897\n",
      "echo: 5051  loss: 0.32897\n",
      "echo: 5052  loss: 0.32897\n",
      "echo: 5053  loss: 0.32897\n",
      "echo: 5054  loss: 0.32897\n",
      "echo: 5055  loss: 0.32897\n",
      "echo: 5056  loss: 0.32897\n",
      "echo: 5057  loss: 0.32897\n",
      "echo: 5058  loss: 0.32897\n",
      "echo: 5059  loss: 0.32897\n",
      "echo: 5060  loss: 0.32897\n",
      "echo: 5061  loss: 0.32897\n",
      "echo: 5062  loss: 0.32897\n",
      "echo: 5063  loss: 0.32896\n",
      "echo: 5064  loss: 0.32896\n",
      "echo: 5065  loss: 0.32896\n",
      "echo: 5066  loss: 0.32896\n",
      "echo: 5067  loss: 0.32896\n",
      "echo: 5068  loss: 0.32896\n",
      "echo: 5069  loss: 0.32896\n",
      "echo: 5070  loss: 0.32896\n",
      "echo: 5071  loss: 0.32896\n",
      "echo: 5072  loss: 0.32896\n",
      "echo: 5073  loss: 0.32896\n",
      "echo: 5074  loss: 0.32896\n",
      "echo: 5075  loss: 0.32896\n",
      "echo: 5076  loss: 0.32896\n",
      "echo: 5077  loss: 0.32896\n",
      "echo: 5078  loss: 0.32896\n",
      "echo: 5079  loss: 0.32896\n",
      "echo: 5080  loss: 0.32896\n",
      "echo: 5081  loss: 0.32896\n",
      "echo: 5082  loss: 0.32896\n",
      "echo: 5083  loss: 0.32896\n",
      "echo: 5084  loss: 0.32896\n",
      "echo: 5085  loss: 0.32896\n",
      "echo: 5086  loss: 0.32896\n",
      "echo: 5087  loss: 0.32895\n",
      "echo: 5088  loss: 0.32895\n",
      "echo: 5089  loss: 0.32895\n",
      "echo: 5090  loss: 0.32895\n",
      "echo: 5091  loss: 0.32895\n",
      "echo: 5092  loss: 0.32895\n",
      "echo: 5093  loss: 0.32895\n",
      "echo: 5094  loss: 0.32895\n",
      "echo: 5095  loss: 0.32895\n",
      "echo: 5096  loss: 0.32895\n",
      "echo: 5097  loss: 0.32895\n",
      "echo: 5098  loss: 0.32895\n",
      "echo: 5099  loss: 0.32895\n",
      "echo: 5100  loss: 0.32895\n",
      "echo: 5101  loss: 0.32895\n",
      "echo: 5102  loss: 0.32895\n",
      "echo: 5103  loss: 0.32895\n",
      "echo: 5104  loss: 0.32895\n",
      "echo: 5105  loss: 0.32895\n",
      "echo: 5106  loss: 0.32895\n",
      "echo: 5107  loss: 0.32895\n",
      "echo: 5108  loss: 0.32895\n",
      "echo: 5109  loss: 0.32895\n",
      "echo: 5110  loss: 0.32895\n",
      "echo: 5111  loss: 0.32895\n",
      "echo: 5112  loss: 0.32895\n",
      "echo: 5113  loss: 0.32894\n",
      "echo: 5114  loss: 0.32894\n",
      "echo: 5115  loss: 0.32894\n",
      "echo: 5116  loss: 0.32894\n",
      "echo: 5117  loss: 0.32894\n",
      "echo: 5118  loss: 0.32894\n",
      "echo: 5119  loss: 0.32894\n",
      "echo: 5120  loss: 0.32894\n",
      "echo: 5121  loss: 0.32894\n",
      "echo: 5122  loss: 0.32894\n",
      "echo: 5123  loss: 0.32894\n",
      "echo: 5124  loss: 0.32894\n",
      "echo: 5125  loss: 0.32894\n",
      "echo: 5126  loss: 0.32894\n",
      "echo: 5127  loss: 0.32894\n",
      "echo: 5128  loss: 0.32894\n",
      "echo: 5129  loss: 0.32894\n",
      "echo: 5130  loss: 0.32894\n",
      "echo: 5131  loss: 0.32894\n",
      "echo: 5132  loss: 0.32894\n",
      "echo: 5133  loss: 0.32894\n",
      "echo: 5134  loss: 0.32894\n",
      "echo: 5135  loss: 0.32894\n",
      "echo: 5136  loss: 0.32894\n",
      "echo: 5137  loss: 0.32894\n",
      "echo: 5138  loss: 0.32893\n",
      "echo: 5139  loss: 0.32893\n",
      "echo: 5140  loss: 0.32893\n",
      "echo: 5141  loss: 0.32893\n",
      "echo: 5142  loss: 0.32893\n",
      "echo: 5143  loss: 0.32893\n",
      "echo: 5144  loss: 0.32893\n",
      "echo: 5145  loss: 0.32893\n",
      "echo: 5146  loss: 0.32893\n",
      "echo: 5147  loss: 0.32893\n",
      "echo: 5148  loss: 0.32893\n",
      "echo: 5149  loss: 0.32893\n",
      "echo: 5150  loss: 0.32893\n",
      "echo: 5151  loss: 0.32893\n",
      "echo: 5152  loss: 0.32893\n",
      "echo: 5153  loss: 0.32893\n",
      "echo: 5154  loss: 0.32893\n",
      "echo: 5155  loss: 0.32893\n",
      "echo: 5156  loss: 0.32893\n",
      "echo: 5157  loss: 0.32893\n",
      "echo: 5158  loss: 0.32893\n",
      "echo: 5159  loss: 0.32893\n",
      "echo: 5160  loss: 0.32893\n",
      "echo: 5161  loss: 0.32893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 5162  loss: 0.32893\n",
      "echo: 5163  loss: 0.32893\n",
      "echo: 5164  loss: 0.32892\n",
      "echo: 5165  loss: 0.32892\n",
      "echo: 5166  loss: 0.32892\n",
      "echo: 5167  loss: 0.32892\n",
      "echo: 5168  loss: 0.32892\n",
      "echo: 5169  loss: 0.32892\n",
      "echo: 5170  loss: 0.32892\n",
      "echo: 5171  loss: 0.32892\n",
      "echo: 5172  loss: 0.32892\n",
      "echo: 5173  loss: 0.32892\n",
      "echo: 5174  loss: 0.32892\n",
      "echo: 5175  loss: 0.32892\n",
      "echo: 5176  loss: 0.32892\n",
      "echo: 5177  loss: 0.32892\n",
      "echo: 5178  loss: 0.32892\n",
      "echo: 5179  loss: 0.32892\n",
      "echo: 5180  loss: 0.32892\n",
      "echo: 5181  loss: 0.32892\n",
      "echo: 5182  loss: 0.32892\n",
      "echo: 5183  loss: 0.32892\n",
      "echo: 5184  loss: 0.32892\n",
      "echo: 5185  loss: 0.32892\n",
      "echo: 5186  loss: 0.32892\n",
      "echo: 5187  loss: 0.32892\n",
      "echo: 5188  loss: 0.32892\n",
      "echo: 5189  loss: 0.32892\n",
      "echo: 5190  loss: 0.32891\n",
      "echo: 5191  loss: 0.32891\n",
      "echo: 5192  loss: 0.32891\n",
      "echo: 5193  loss: 0.32891\n",
      "echo: 5194  loss: 0.32891\n",
      "echo: 5195  loss: 0.32891\n",
      "echo: 5196  loss: 0.32891\n",
      "echo: 5197  loss: 0.32891\n",
      "echo: 5198  loss: 0.32891\n",
      "echo: 5199  loss: 0.32891\n",
      "echo: 5200  loss: 0.32891\n",
      "echo: 5201  loss: 0.32891\n",
      "echo: 5202  loss: 0.32891\n",
      "echo: 5203  loss: 0.32891\n",
      "echo: 5204  loss: 0.32891\n",
      "echo: 5205  loss: 0.32891\n",
      "echo: 5206  loss: 0.32891\n",
      "echo: 5207  loss: 0.32891\n",
      "echo: 5208  loss: 0.32891\n",
      "echo: 5209  loss: 0.32891\n",
      "echo: 5210  loss: 0.32891\n",
      "echo: 5211  loss: 0.32891\n",
      "echo: 5212  loss: 0.32891\n",
      "echo: 5213  loss: 0.32891\n",
      "echo: 5214  loss: 0.32891\n",
      "echo: 5215  loss: 0.32891\n",
      "echo: 5216  loss: 0.32891\n",
      "echo: 5217  loss: 0.3289\n",
      "echo: 5218  loss: 0.3289\n",
      "echo: 5219  loss: 0.3289\n",
      "echo: 5220  loss: 0.3289\n",
      "echo: 5221  loss: 0.3289\n",
      "echo: 5222  loss: 0.3289\n",
      "echo: 5223  loss: 0.3289\n",
      "echo: 5224  loss: 0.3289\n",
      "echo: 5225  loss: 0.3289\n",
      "echo: 5226  loss: 0.3289\n",
      "echo: 5227  loss: 0.3289\n",
      "echo: 5228  loss: 0.3289\n",
      "echo: 5229  loss: 0.3289\n",
      "echo: 5230  loss: 0.3289\n",
      "echo: 5231  loss: 0.3289\n",
      "echo: 5232  loss: 0.3289\n",
      "echo: 5233  loss: 0.3289\n",
      "echo: 5234  loss: 0.3289\n",
      "echo: 5235  loss: 0.3289\n",
      "echo: 5236  loss: 0.3289\n",
      "echo: 5237  loss: 0.3289\n",
      "echo: 5238  loss: 0.3289\n",
      "echo: 5239  loss: 0.3289\n",
      "echo: 5240  loss: 0.3289\n",
      "echo: 5241  loss: 0.3289\n",
      "echo: 5242  loss: 0.3289\n",
      "echo: 5243  loss: 0.3289\n",
      "echo: 5244  loss: 0.32889\n",
      "echo: 5245  loss: 0.32889\n",
      "echo: 5246  loss: 0.32889\n",
      "echo: 5247  loss: 0.32889\n",
      "echo: 5248  loss: 0.32889\n",
      "echo: 5249  loss: 0.32889\n",
      "echo: 5250  loss: 0.32889\n",
      "echo: 5251  loss: 0.32889\n",
      "echo: 5252  loss: 0.32889\n",
      "echo: 5253  loss: 0.32889\n",
      "echo: 5254  loss: 0.32889\n",
      "echo: 5255  loss: 0.32889\n",
      "echo: 5256  loss: 0.32889\n",
      "echo: 5257  loss: 0.32889\n",
      "echo: 5258  loss: 0.32889\n",
      "echo: 5259  loss: 0.32889\n",
      "echo: 5260  loss: 0.32889\n",
      "echo: 5261  loss: 0.32889\n",
      "echo: 5262  loss: 0.32889\n",
      "echo: 5263  loss: 0.32889\n",
      "echo: 5264  loss: 0.32889\n",
      "echo: 5265  loss: 0.32889\n",
      "echo: 5266  loss: 0.32889\n",
      "echo: 5267  loss: 0.32889\n",
      "echo: 5268  loss: 0.32889\n",
      "echo: 5269  loss: 0.32889\n",
      "echo: 5270  loss: 0.32889\n",
      "echo: 5271  loss: 0.32889\n",
      "echo: 5272  loss: 0.32888\n",
      "echo: 5273  loss: 0.32888\n",
      "echo: 5274  loss: 0.32888\n",
      "echo: 5275  loss: 0.32888\n",
      "echo: 5276  loss: 0.32888\n",
      "echo: 5277  loss: 0.32888\n",
      "echo: 5278  loss: 0.32888\n",
      "echo: 5279  loss: 0.32888\n",
      "echo: 5280  loss: 0.32888\n",
      "echo: 5281  loss: 0.32888\n",
      "echo: 5282  loss: 0.32888\n",
      "echo: 5283  loss: 0.32888\n",
      "echo: 5284  loss: 0.32888\n",
      "echo: 5285  loss: 0.32888\n",
      "echo: 5286  loss: 0.32888\n",
      "echo: 5287  loss: 0.32888\n",
      "echo: 5288  loss: 0.32888\n",
      "echo: 5289  loss: 0.32888\n",
      "echo: 5290  loss: 0.32888\n",
      "echo: 5291  loss: 0.32888\n",
      "echo: 5292  loss: 0.32888\n",
      "echo: 5293  loss: 0.32888\n",
      "echo: 5294  loss: 0.32888\n",
      "echo: 5295  loss: 0.32888\n",
      "echo: 5296  loss: 0.32888\n",
      "echo: 5297  loss: 0.32888\n",
      "echo: 5298  loss: 0.32888\n",
      "echo: 5299  loss: 0.32888\n",
      "echo: 5300  loss: 0.32887\n",
      "echo: 5301  loss: 0.32887\n",
      "echo: 5302  loss: 0.32887\n",
      "echo: 5303  loss: 0.32887\n",
      "echo: 5304  loss: 0.32887\n",
      "echo: 5305  loss: 0.32887\n",
      "echo: 5306  loss: 0.32887\n",
      "echo: 5307  loss: 0.32887\n",
      "echo: 5308  loss: 0.32887\n",
      "echo: 5309  loss: 0.32887\n",
      "echo: 5310  loss: 0.32887\n",
      "echo: 5311  loss: 0.32887\n",
      "echo: 5312  loss: 0.32887\n",
      "echo: 5313  loss: 0.32887\n",
      "echo: 5314  loss: 0.32887\n",
      "echo: 5315  loss: 0.32887\n",
      "echo: 5316  loss: 0.32887\n",
      "echo: 5317  loss: 0.32887\n",
      "echo: 5318  loss: 0.32887\n",
      "echo: 5319  loss: 0.32887\n",
      "echo: 5320  loss: 0.32887\n",
      "echo: 5321  loss: 0.32887\n",
      "echo: 5322  loss: 0.32887\n",
      "echo: 5323  loss: 0.32887\n",
      "echo: 5324  loss: 0.32887\n",
      "echo: 5325  loss: 0.32887\n",
      "echo: 5326  loss: 0.32887\n",
      "echo: 5327  loss: 0.32887\n",
      "echo: 5328  loss: 0.32886\n",
      "echo: 5329  loss: 0.32886\n",
      "echo: 5330  loss: 0.32886\n",
      "echo: 5331  loss: 0.32886\n",
      "echo: 5332  loss: 0.32886\n",
      "echo: 5333  loss: 0.32886\n",
      "echo: 5334  loss: 0.32886\n",
      "echo: 5335  loss: 0.32886\n",
      "echo: 5336  loss: 0.32886\n",
      "echo: 5337  loss: 0.32886\n",
      "echo: 5338  loss: 0.32886\n",
      "echo: 5339  loss: 0.32886\n",
      "echo: 5340  loss: 0.32886\n",
      "echo: 5341  loss: 0.32886\n",
      "echo: 5342  loss: 0.32886\n",
      "echo: 5343  loss: 0.32886\n",
      "echo: 5344  loss: 0.32886\n",
      "echo: 5345  loss: 0.32886\n",
      "echo: 5346  loss: 0.32886\n",
      "echo: 5347  loss: 0.32886\n",
      "echo: 5348  loss: 0.32886\n",
      "echo: 5349  loss: 0.32886\n",
      "echo: 5350  loss: 0.32886\n",
      "echo: 5351  loss: 0.32886\n",
      "echo: 5352  loss: 0.32886\n",
      "echo: 5353  loss: 0.32886\n",
      "echo: 5354  loss: 0.32886\n",
      "echo: 5355  loss: 0.32886\n",
      "echo: 5356  loss: 0.32886\n",
      "echo: 5357  loss: 0.32886\n",
      "echo: 5358  loss: 0.32885\n",
      "echo: 5359  loss: 0.32885\n",
      "echo: 5360  loss: 0.32885\n",
      "echo: 5361  loss: 0.32885\n",
      "echo: 5362  loss: 0.32885\n",
      "echo: 5363  loss: 0.32885\n",
      "echo: 5364  loss: 0.32885\n",
      "echo: 5365  loss: 0.32885\n",
      "echo: 5366  loss: 0.32885\n",
      "echo: 5367  loss: 0.32885\n",
      "echo: 5368  loss: 0.32885\n",
      "echo: 5369  loss: 0.32885\n",
      "echo: 5370  loss: 0.32885\n",
      "echo: 5371  loss: 0.32885\n",
      "echo: 5372  loss: 0.32885\n",
      "echo: 5373  loss: 0.32885\n",
      "echo: 5374  loss: 0.32885\n",
      "echo: 5375  loss: 0.32885\n",
      "echo: 5376  loss: 0.32885\n",
      "echo: 5377  loss: 0.32885\n",
      "echo: 5378  loss: 0.32885\n",
      "echo: 5379  loss: 0.32885\n",
      "echo: 5380  loss: 0.32885\n",
      "echo: 5381  loss: 0.32885\n",
      "echo: 5382  loss: 0.32885\n",
      "echo: 5383  loss: 0.32885\n",
      "echo: 5384  loss: 0.32885\n",
      "echo: 5385  loss: 0.32885\n",
      "echo: 5386  loss: 0.32885\n",
      "echo: 5387  loss: 0.32884\n",
      "echo: 5388  loss: 0.32884\n",
      "echo: 5389  loss: 0.32884\n",
      "echo: 5390  loss: 0.32884\n",
      "echo: 5391  loss: 0.32884\n",
      "echo: 5392  loss: 0.32884\n",
      "echo: 5393  loss: 0.32884\n",
      "echo: 5394  loss: 0.32884\n",
      "echo: 5395  loss: 0.32884\n",
      "echo: 5396  loss: 0.32884\n",
      "echo: 5397  loss: 0.32884\n",
      "echo: 5398  loss: 0.32884\n",
      "echo: 5399  loss: 0.32884\n",
      "echo: 5400  loss: 0.32884\n",
      "echo: 5401  loss: 0.32884\n",
      "echo: 5402  loss: 0.32884\n",
      "echo: 5403  loss: 0.32884\n",
      "echo: 5404  loss: 0.32884\n",
      "echo: 5405  loss: 0.32884\n",
      "echo: 5406  loss: 0.32884\n",
      "echo: 5407  loss: 0.32884\n",
      "echo: 5408  loss: 0.32884\n",
      "echo: 5409  loss: 0.32884\n",
      "echo: 5410  loss: 0.32884\n",
      "echo: 5411  loss: 0.32884\n",
      "echo: 5412  loss: 0.32884\n",
      "echo: 5413  loss: 0.32884\n",
      "echo: 5414  loss: 0.32884\n",
      "echo: 5415  loss: 0.32884\n",
      "echo: 5416  loss: 0.32884\n",
      "echo: 5417  loss: 0.32884\n",
      "echo: 5418  loss: 0.32883\n",
      "echo: 5419  loss: 0.32883\n",
      "echo: 5420  loss: 0.32883\n",
      "echo: 5421  loss: 0.32883\n",
      "echo: 5422  loss: 0.32883\n",
      "echo: 5423  loss: 0.32883\n",
      "echo: 5424  loss: 0.32883\n",
      "echo: 5425  loss: 0.32883\n",
      "echo: 5426  loss: 0.32883\n",
      "echo: 5427  loss: 0.32883\n",
      "echo: 5428  loss: 0.32883\n",
      "echo: 5429  loss: 0.32883\n",
      "echo: 5430  loss: 0.32883\n",
      "echo: 5431  loss: 0.32883\n",
      "echo: 5432  loss: 0.32883\n",
      "echo: 5433  loss: 0.32883\n",
      "echo: 5434  loss: 0.32883\n",
      "echo: 5435  loss: 0.32883\n",
      "echo: 5436  loss: 0.32883\n",
      "echo: 5437  loss: 0.32883\n",
      "echo: 5438  loss: 0.32883\n",
      "echo: 5439  loss: 0.32883\n",
      "echo: 5440  loss: 0.32883\n",
      "echo: 5441  loss: 0.32883\n",
      "echo: 5442  loss: 0.32883\n",
      "echo: 5443  loss: 0.32883\n",
      "echo: 5444  loss: 0.32883\n",
      "echo: 5445  loss: 0.32883\n",
      "echo: 5446  loss: 0.32883\n",
      "echo: 5447  loss: 0.32883\n",
      "echo: 5448  loss: 0.32882\n",
      "echo: 5449  loss: 0.32882\n",
      "echo: 5450  loss: 0.32882\n",
      "echo: 5451  loss: 0.32882\n",
      "echo: 5452  loss: 0.32882\n",
      "echo: 5453  loss: 0.32882\n",
      "echo: 5454  loss: 0.32882\n",
      "echo: 5455  loss: 0.32882\n",
      "echo: 5456  loss: 0.32882\n",
      "echo: 5457  loss: 0.32882\n",
      "echo: 5458  loss: 0.32882\n",
      "echo: 5459  loss: 0.32882\n",
      "echo: 5460  loss: 0.32882\n",
      "echo: 5461  loss: 0.32882\n",
      "echo: 5462  loss: 0.32882\n",
      "echo: 5463  loss: 0.32882\n",
      "echo: 5464  loss: 0.32882\n",
      "echo: 5465  loss: 0.32882\n",
      "echo: 5466  loss: 0.32882\n",
      "echo: 5467  loss: 0.32882\n",
      "echo: 5468  loss: 0.32882\n",
      "echo: 5469  loss: 0.32882\n",
      "echo: 5470  loss: 0.32882\n",
      "echo: 5471  loss: 0.32882\n",
      "echo: 5472  loss: 0.32882\n",
      "echo: 5473  loss: 0.32882\n",
      "echo: 5474  loss: 0.32882\n",
      "echo: 5475  loss: 0.32882\n",
      "echo: 5476  loss: 0.32882\n",
      "echo: 5477  loss: 0.32882\n",
      "echo: 5478  loss: 0.32882\n",
      "echo: 5479  loss: 0.32882\n",
      "echo: 5480  loss: 0.32881\n",
      "echo: 5481  loss: 0.32881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 5482  loss: 0.32881\n",
      "echo: 5483  loss: 0.32881\n",
      "echo: 5484  loss: 0.32881\n",
      "echo: 5485  loss: 0.32881\n",
      "echo: 5486  loss: 0.32881\n",
      "echo: 5487  loss: 0.32881\n",
      "echo: 5488  loss: 0.32881\n",
      "echo: 5489  loss: 0.32881\n",
      "echo: 5490  loss: 0.32881\n",
      "echo: 5491  loss: 0.32881\n",
      "echo: 5492  loss: 0.32881\n",
      "echo: 5493  loss: 0.32881\n",
      "echo: 5494  loss: 0.32881\n",
      "echo: 5495  loss: 0.32881\n",
      "echo: 5496  loss: 0.32881\n",
      "echo: 5497  loss: 0.32881\n",
      "echo: 5498  loss: 0.32881\n",
      "echo: 5499  loss: 0.32881\n",
      "echo: 5500  loss: 0.32881\n",
      "echo: 5501  loss: 0.32881\n",
      "echo: 5502  loss: 0.32881\n",
      "echo: 5503  loss: 0.32881\n",
      "echo: 5504  loss: 0.32881\n",
      "echo: 5505  loss: 0.32881\n",
      "echo: 5506  loss: 0.32881\n",
      "echo: 5507  loss: 0.32881\n",
      "echo: 5508  loss: 0.32881\n",
      "echo: 5509  loss: 0.32881\n",
      "echo: 5510  loss: 0.32881\n",
      "echo: 5511  loss: 0.32881\n",
      "echo: 5512  loss: 0.3288\n",
      "echo: 5513  loss: 0.3288\n",
      "echo: 5514  loss: 0.3288\n",
      "echo: 5515  loss: 0.3288\n",
      "echo: 5516  loss: 0.3288\n",
      "echo: 5517  loss: 0.3288\n",
      "echo: 5518  loss: 0.3288\n",
      "echo: 5519  loss: 0.3288\n",
      "echo: 5520  loss: 0.3288\n",
      "echo: 5521  loss: 0.3288\n",
      "echo: 5522  loss: 0.3288\n",
      "echo: 5523  loss: 0.3288\n",
      "echo: 5524  loss: 0.3288\n",
      "echo: 5525  loss: 0.3288\n",
      "echo: 5526  loss: 0.3288\n",
      "echo: 5527  loss: 0.3288\n",
      "echo: 5528  loss: 0.3288\n",
      "echo: 5529  loss: 0.3288\n",
      "echo: 5530  loss: 0.3288\n",
      "echo: 5531  loss: 0.3288\n",
      "echo: 5532  loss: 0.3288\n",
      "echo: 5533  loss: 0.3288\n",
      "echo: 5534  loss: 0.3288\n",
      "echo: 5535  loss: 0.3288\n",
      "echo: 5536  loss: 0.3288\n",
      "echo: 5537  loss: 0.3288\n",
      "echo: 5538  loss: 0.3288\n",
      "echo: 5539  loss: 0.3288\n",
      "echo: 5540  loss: 0.3288\n",
      "echo: 5541  loss: 0.3288\n",
      "echo: 5542  loss: 0.3288\n",
      "echo: 5543  loss: 0.3288\n",
      "echo: 5544  loss: 0.3288\n",
      "echo: 5545  loss: 0.32879\n",
      "echo: 5546  loss: 0.32879\n",
      "echo: 5547  loss: 0.32879\n",
      "echo: 5548  loss: 0.32879\n",
      "echo: 5549  loss: 0.32879\n",
      "echo: 5550  loss: 0.32879\n",
      "echo: 5551  loss: 0.32879\n",
      "echo: 5552  loss: 0.32879\n",
      "echo: 5553  loss: 0.32879\n",
      "echo: 5554  loss: 0.32879\n",
      "echo: 5555  loss: 0.32879\n",
      "echo: 5556  loss: 0.32879\n",
      "echo: 5557  loss: 0.32879\n",
      "echo: 5558  loss: 0.32879\n",
      "echo: 5559  loss: 0.32879\n",
      "echo: 5560  loss: 0.32879\n",
      "echo: 5561  loss: 0.32879\n",
      "echo: 5562  loss: 0.32879\n",
      "echo: 5563  loss: 0.32879\n",
      "echo: 5564  loss: 0.32879\n",
      "echo: 5565  loss: 0.32879\n",
      "echo: 5566  loss: 0.32879\n",
      "echo: 5567  loss: 0.32879\n",
      "echo: 5568  loss: 0.32879\n",
      "echo: 5569  loss: 0.32879\n",
      "echo: 5570  loss: 0.32879\n",
      "echo: 5571  loss: 0.32879\n",
      "echo: 5572  loss: 0.32879\n",
      "echo: 5573  loss: 0.32879\n",
      "echo: 5574  loss: 0.32879\n",
      "echo: 5575  loss: 0.32879\n",
      "echo: 5576  loss: 0.32879\n",
      "echo: 5577  loss: 0.32879\n",
      "echo: 5578  loss: 0.32878\n",
      "echo: 5579  loss: 0.32878\n",
      "echo: 5580  loss: 0.32878\n",
      "echo: 5581  loss: 0.32878\n",
      "echo: 5582  loss: 0.32878\n",
      "echo: 5583  loss: 0.32878\n",
      "echo: 5584  loss: 0.32878\n",
      "echo: 5585  loss: 0.32878\n",
      "echo: 5586  loss: 0.32878\n",
      "echo: 5587  loss: 0.32878\n",
      "echo: 5588  loss: 0.32878\n",
      "echo: 5589  loss: 0.32878\n",
      "echo: 5590  loss: 0.32878\n",
      "echo: 5591  loss: 0.32878\n",
      "echo: 5592  loss: 0.32878\n",
      "echo: 5593  loss: 0.32878\n",
      "echo: 5594  loss: 0.32878\n",
      "echo: 5595  loss: 0.32878\n",
      "echo: 5596  loss: 0.32878\n",
      "echo: 5597  loss: 0.32878\n",
      "echo: 5598  loss: 0.32878\n",
      "echo: 5599  loss: 0.32878\n",
      "echo: 5600  loss: 0.32878\n",
      "echo: 5601  loss: 0.32878\n",
      "echo: 5602  loss: 0.32878\n",
      "echo: 5603  loss: 0.32878\n",
      "echo: 5604  loss: 0.32878\n",
      "echo: 5605  loss: 0.32878\n",
      "echo: 5606  loss: 0.32878\n",
      "echo: 5607  loss: 0.32878\n",
      "echo: 5608  loss: 0.32878\n",
      "echo: 5609  loss: 0.32878\n",
      "echo: 5610  loss: 0.32878\n",
      "echo: 5611  loss: 0.32878\n",
      "echo: 5612  loss: 0.32877\n",
      "echo: 5613  loss: 0.32877\n",
      "echo: 5614  loss: 0.32877\n",
      "echo: 5615  loss: 0.32877\n",
      "echo: 5616  loss: 0.32877\n",
      "echo: 5617  loss: 0.32877\n",
      "echo: 5618  loss: 0.32877\n",
      "echo: 5619  loss: 0.32877\n",
      "echo: 5620  loss: 0.32877\n",
      "echo: 5621  loss: 0.32877\n",
      "echo: 5622  loss: 0.32877\n",
      "echo: 5623  loss: 0.32877\n",
      "echo: 5624  loss: 0.32877\n",
      "echo: 5625  loss: 0.32877\n",
      "echo: 5626  loss: 0.32877\n",
      "echo: 5627  loss: 0.32877\n",
      "echo: 5628  loss: 0.32877\n",
      "echo: 5629  loss: 0.32877\n",
      "echo: 5630  loss: 0.32877\n",
      "echo: 5631  loss: 0.32877\n",
      "echo: 5632  loss: 0.32877\n",
      "echo: 5633  loss: 0.32877\n",
      "echo: 5634  loss: 0.32877\n",
      "echo: 5635  loss: 0.32877\n",
      "echo: 5636  loss: 0.32877\n",
      "echo: 5637  loss: 0.32877\n",
      "echo: 5638  loss: 0.32877\n",
      "echo: 5639  loss: 0.32877\n",
      "echo: 5640  loss: 0.32877\n",
      "echo: 5641  loss: 0.32877\n",
      "echo: 5642  loss: 0.32877\n",
      "echo: 5643  loss: 0.32877\n",
      "echo: 5644  loss: 0.32877\n",
      "echo: 5645  loss: 0.32877\n",
      "echo: 5646  loss: 0.32877\n",
      "echo: 5647  loss: 0.32876\n",
      "echo: 5648  loss: 0.32876\n",
      "echo: 5649  loss: 0.32876\n",
      "echo: 5650  loss: 0.32876\n",
      "echo: 5651  loss: 0.32876\n",
      "echo: 5652  loss: 0.32876\n",
      "echo: 5653  loss: 0.32876\n",
      "echo: 5654  loss: 0.32876\n",
      "echo: 5655  loss: 0.32876\n",
      "echo: 5656  loss: 0.32876\n",
      "echo: 5657  loss: 0.32876\n",
      "echo: 5658  loss: 0.32876\n",
      "echo: 5659  loss: 0.32876\n",
      "echo: 5660  loss: 0.32876\n",
      "echo: 5661  loss: 0.32876\n",
      "echo: 5662  loss: 0.32876\n",
      "echo: 5663  loss: 0.32876\n",
      "echo: 5664  loss: 0.32876\n",
      "echo: 5665  loss: 0.32876\n",
      "echo: 5666  loss: 0.32876\n",
      "echo: 5667  loss: 0.32876\n",
      "echo: 5668  loss: 0.32876\n",
      "echo: 5669  loss: 0.32876\n",
      "echo: 5670  loss: 0.32876\n",
      "echo: 5671  loss: 0.32876\n",
      "echo: 5672  loss: 0.32876\n",
      "echo: 5673  loss: 0.32876\n",
      "echo: 5674  loss: 0.32876\n",
      "echo: 5675  loss: 0.32876\n",
      "echo: 5676  loss: 0.32876\n",
      "echo: 5677  loss: 0.32876\n",
      "echo: 5678  loss: 0.32876\n",
      "echo: 5679  loss: 0.32876\n",
      "echo: 5680  loss: 0.32876\n",
      "echo: 5681  loss: 0.32876\n",
      "echo: 5682  loss: 0.32875\n",
      "echo: 5683  loss: 0.32875\n",
      "echo: 5684  loss: 0.32875\n",
      "echo: 5685  loss: 0.32875\n",
      "echo: 5686  loss: 0.32875\n",
      "echo: 5687  loss: 0.32875\n",
      "echo: 5688  loss: 0.32875\n",
      "echo: 5689  loss: 0.32875\n",
      "echo: 5690  loss: 0.32875\n",
      "echo: 5691  loss: 0.32875\n",
      "echo: 5692  loss: 0.32875\n",
      "echo: 5693  loss: 0.32875\n",
      "echo: 5694  loss: 0.32875\n",
      "echo: 5695  loss: 0.32875\n",
      "echo: 5696  loss: 0.32875\n",
      "echo: 5697  loss: 0.32875\n",
      "echo: 5698  loss: 0.32875\n",
      "echo: 5699  loss: 0.32875\n",
      "echo: 5700  loss: 0.32875\n",
      "echo: 5701  loss: 0.32875\n",
      "echo: 5702  loss: 0.32875\n",
      "echo: 5703  loss: 0.32875\n",
      "echo: 5704  loss: 0.32875\n",
      "echo: 5705  loss: 0.32875\n",
      "echo: 5706  loss: 0.32875\n",
      "echo: 5707  loss: 0.32875\n",
      "echo: 5708  loss: 0.32875\n",
      "echo: 5709  loss: 0.32875\n",
      "echo: 5710  loss: 0.32875\n",
      "echo: 5711  loss: 0.32875\n",
      "echo: 5712  loss: 0.32875\n",
      "echo: 5713  loss: 0.32875\n",
      "echo: 5714  loss: 0.32875\n",
      "echo: 5715  loss: 0.32875\n",
      "echo: 5716  loss: 0.32875\n",
      "echo: 5717  loss: 0.32875\n",
      "echo: 5718  loss: 0.32875\n",
      "echo: 5719  loss: 0.32874\n",
      "echo: 5720  loss: 0.32874\n",
      "echo: 5721  loss: 0.32874\n",
      "echo: 5722  loss: 0.32874\n",
      "echo: 5723  loss: 0.32874\n",
      "echo: 5724  loss: 0.32874\n",
      "echo: 5725  loss: 0.32874\n",
      "echo: 5726  loss: 0.32874\n",
      "echo: 5727  loss: 0.32874\n",
      "echo: 5728  loss: 0.32874\n",
      "echo: 5729  loss: 0.32874\n",
      "echo: 5730  loss: 0.32874\n",
      "echo: 5731  loss: 0.32874\n",
      "echo: 5732  loss: 0.32874\n",
      "echo: 5733  loss: 0.32874\n",
      "echo: 5734  loss: 0.32874\n",
      "echo: 5735  loss: 0.32874\n",
      "echo: 5736  loss: 0.32874\n",
      "echo: 5737  loss: 0.32874\n",
      "echo: 5738  loss: 0.32874\n",
      "echo: 5739  loss: 0.32874\n",
      "echo: 5740  loss: 0.32874\n",
      "echo: 5741  loss: 0.32874\n",
      "echo: 5742  loss: 0.32874\n",
      "echo: 5743  loss: 0.32874\n",
      "echo: 5744  loss: 0.32874\n",
      "echo: 5745  loss: 0.32874\n",
      "echo: 5746  loss: 0.32874\n",
      "echo: 5747  loss: 0.32874\n",
      "echo: 5748  loss: 0.32874\n",
      "echo: 5749  loss: 0.32874\n",
      "echo: 5750  loss: 0.32874\n",
      "echo: 5751  loss: 0.32874\n",
      "echo: 5752  loss: 0.32874\n",
      "echo: 5753  loss: 0.32874\n",
      "echo: 5754  loss: 0.32874\n",
      "echo: 5755  loss: 0.32874\n",
      "echo: 5756  loss: 0.32873\n",
      "echo: 5757  loss: 0.32873\n",
      "echo: 5758  loss: 0.32873\n",
      "echo: 5759  loss: 0.32873\n",
      "echo: 5760  loss: 0.32873\n",
      "echo: 5761  loss: 0.32873\n",
      "echo: 5762  loss: 0.32873\n",
      "echo: 5763  loss: 0.32873\n",
      "echo: 5764  loss: 0.32873\n",
      "echo: 5765  loss: 0.32873\n",
      "echo: 5766  loss: 0.32873\n",
      "echo: 5767  loss: 0.32873\n",
      "echo: 5768  loss: 0.32873\n",
      "echo: 5769  loss: 0.32873\n",
      "echo: 5770  loss: 0.32873\n",
      "echo: 5771  loss: 0.32873\n",
      "echo: 5772  loss: 0.32873\n",
      "echo: 5773  loss: 0.32873\n",
      "echo: 5774  loss: 0.32873\n",
      "echo: 5775  loss: 0.32873\n",
      "echo: 5776  loss: 0.32873\n",
      "echo: 5777  loss: 0.32873\n",
      "echo: 5778  loss: 0.32873\n",
      "echo: 5779  loss: 0.32873\n",
      "echo: 5780  loss: 0.32873\n",
      "echo: 5781  loss: 0.32873\n",
      "echo: 5782  loss: 0.32873\n",
      "echo: 5783  loss: 0.32873\n",
      "echo: 5784  loss: 0.32873\n",
      "echo: 5785  loss: 0.32873\n",
      "echo: 5786  loss: 0.32873\n",
      "echo: 5787  loss: 0.32873\n",
      "echo: 5788  loss: 0.32873\n",
      "echo: 5789  loss: 0.32873\n",
      "echo: 5790  loss: 0.32873\n",
      "echo: 5791  loss: 0.32873\n",
      "echo: 5792  loss: 0.32873\n",
      "echo: 5793  loss: 0.32873\n",
      "echo: 5794  loss: 0.32872\n",
      "echo: 5795  loss: 0.32872\n",
      "echo: 5796  loss: 0.32872\n",
      "echo: 5797  loss: 0.32872\n",
      "echo: 5798  loss: 0.32872\n",
      "echo: 5799  loss: 0.32872\n",
      "echo: 5800  loss: 0.32872\n",
      "echo: 5801  loss: 0.32872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 5802  loss: 0.32872\n",
      "echo: 5803  loss: 0.32872\n",
      "echo: 5804  loss: 0.32872\n",
      "echo: 5805  loss: 0.32872\n",
      "echo: 5806  loss: 0.32872\n",
      "echo: 5807  loss: 0.32872\n",
      "echo: 5808  loss: 0.32872\n",
      "echo: 5809  loss: 0.32872\n",
      "echo: 5810  loss: 0.32872\n",
      "echo: 5811  loss: 0.32872\n",
      "echo: 5812  loss: 0.32872\n",
      "echo: 5813  loss: 0.32872\n",
      "echo: 5814  loss: 0.32872\n",
      "echo: 5815  loss: 0.32872\n",
      "echo: 5816  loss: 0.32872\n",
      "echo: 5817  loss: 0.32872\n",
      "echo: 5818  loss: 0.32872\n",
      "echo: 5819  loss: 0.32872\n",
      "echo: 5820  loss: 0.32872\n",
      "echo: 5821  loss: 0.32872\n",
      "echo: 5822  loss: 0.32872\n",
      "echo: 5823  loss: 0.32872\n",
      "echo: 5824  loss: 0.32872\n",
      "echo: 5825  loss: 0.32872\n",
      "echo: 5826  loss: 0.32872\n",
      "echo: 5827  loss: 0.32872\n",
      "echo: 5828  loss: 0.32872\n",
      "echo: 5829  loss: 0.32872\n",
      "echo: 5830  loss: 0.32872\n",
      "echo: 5831  loss: 0.32872\n",
      "echo: 5832  loss: 0.32872\n",
      "echo: 5833  loss: 0.32871\n",
      "echo: 5834  loss: 0.32871\n",
      "echo: 5835  loss: 0.32871\n",
      "echo: 5836  loss: 0.32871\n",
      "echo: 5837  loss: 0.32871\n",
      "echo: 5838  loss: 0.32871\n",
      "echo: 5839  loss: 0.32871\n",
      "echo: 5840  loss: 0.32871\n",
      "echo: 5841  loss: 0.32871\n",
      "echo: 5842  loss: 0.32871\n",
      "echo: 5843  loss: 0.32871\n",
      "echo: 5844  loss: 0.32871\n",
      "echo: 5845  loss: 0.32871\n",
      "echo: 5846  loss: 0.32871\n",
      "echo: 5847  loss: 0.32871\n",
      "echo: 5848  loss: 0.32871\n",
      "echo: 5849  loss: 0.32871\n",
      "echo: 5850  loss: 0.32871\n",
      "echo: 5851  loss: 0.32871\n",
      "echo: 5852  loss: 0.32871\n",
      "echo: 5853  loss: 0.32871\n",
      "echo: 5854  loss: 0.32871\n",
      "echo: 5855  loss: 0.32871\n",
      "echo: 5856  loss: 0.32871\n",
      "echo: 5857  loss: 0.32871\n",
      "echo: 5858  loss: 0.32871\n",
      "echo: 5859  loss: 0.32871\n",
      "echo: 5860  loss: 0.32871\n",
      "echo: 5861  loss: 0.32871\n",
      "echo: 5862  loss: 0.32871\n",
      "echo: 5863  loss: 0.32871\n",
      "echo: 5864  loss: 0.32871\n",
      "echo: 5865  loss: 0.32871\n",
      "echo: 5866  loss: 0.32871\n",
      "echo: 5867  loss: 0.32871\n",
      "echo: 5868  loss: 0.32871\n",
      "echo: 5869  loss: 0.32871\n",
      "echo: 5870  loss: 0.32871\n",
      "echo: 5871  loss: 0.32871\n",
      "echo: 5872  loss: 0.32871\n",
      "echo: 5873  loss: 0.3287\n",
      "echo: 5874  loss: 0.3287\n",
      "echo: 5875  loss: 0.3287\n",
      "echo: 5876  loss: 0.3287\n",
      "echo: 5877  loss: 0.3287\n",
      "echo: 5878  loss: 0.3287\n",
      "echo: 5879  loss: 0.3287\n",
      "echo: 5880  loss: 0.3287\n",
      "echo: 5881  loss: 0.3287\n",
      "echo: 5882  loss: 0.3287\n",
      "echo: 5883  loss: 0.3287\n",
      "echo: 5884  loss: 0.3287\n",
      "echo: 5885  loss: 0.3287\n",
      "echo: 5886  loss: 0.3287\n",
      "echo: 5887  loss: 0.3287\n",
      "echo: 5888  loss: 0.3287\n",
      "echo: 5889  loss: 0.3287\n",
      "echo: 5890  loss: 0.3287\n",
      "echo: 5891  loss: 0.3287\n",
      "echo: 5892  loss: 0.3287\n",
      "echo: 5893  loss: 0.3287\n",
      "echo: 5894  loss: 0.3287\n",
      "echo: 5895  loss: 0.3287\n",
      "echo: 5896  loss: 0.3287\n",
      "echo: 5897  loss: 0.3287\n",
      "echo: 5898  loss: 0.3287\n",
      "echo: 5899  loss: 0.3287\n",
      "echo: 5900  loss: 0.3287\n",
      "echo: 5901  loss: 0.3287\n",
      "echo: 5902  loss: 0.3287\n",
      "echo: 5903  loss: 0.3287\n",
      "echo: 5904  loss: 0.3287\n",
      "echo: 5905  loss: 0.3287\n",
      "echo: 5906  loss: 0.3287\n",
      "echo: 5907  loss: 0.3287\n",
      "echo: 5908  loss: 0.3287\n",
      "echo: 5909  loss: 0.3287\n",
      "echo: 5910  loss: 0.3287\n",
      "echo: 5911  loss: 0.3287\n",
      "echo: 5912  loss: 0.3287\n",
      "echo: 5913  loss: 0.3287\n",
      "echo: 5914  loss: 0.32869\n",
      "echo: 5915  loss: 0.32869\n",
      "echo: 5916  loss: 0.32869\n",
      "echo: 5917  loss: 0.32869\n",
      "echo: 5918  loss: 0.32869\n",
      "echo: 5919  loss: 0.32869\n",
      "echo: 5920  loss: 0.32869\n",
      "echo: 5921  loss: 0.32869\n",
      "echo: 5922  loss: 0.32869\n",
      "echo: 5923  loss: 0.32869\n",
      "echo: 5924  loss: 0.32869\n",
      "echo: 5925  loss: 0.32869\n",
      "echo: 5926  loss: 0.32869\n",
      "echo: 5927  loss: 0.32869\n",
      "echo: 5928  loss: 0.32869\n",
      "echo: 5929  loss: 0.32869\n",
      "echo: 5930  loss: 0.32869\n",
      "echo: 5931  loss: 0.32869\n",
      "echo: 5932  loss: 0.32869\n",
      "echo: 5933  loss: 0.32869\n",
      "echo: 5934  loss: 0.32869\n",
      "echo: 5935  loss: 0.32869\n",
      "echo: 5936  loss: 0.32869\n",
      "echo: 5937  loss: 0.32869\n",
      "echo: 5938  loss: 0.32869\n",
      "echo: 5939  loss: 0.32869\n",
      "echo: 5940  loss: 0.32869\n",
      "echo: 5941  loss: 0.32869\n",
      "echo: 5942  loss: 0.32869\n",
      "echo: 5943  loss: 0.32869\n",
      "echo: 5944  loss: 0.32869\n",
      "echo: 5945  loss: 0.32869\n",
      "echo: 5946  loss: 0.32869\n",
      "echo: 5947  loss: 0.32869\n",
      "echo: 5948  loss: 0.32869\n",
      "echo: 5949  loss: 0.32869\n",
      "echo: 5950  loss: 0.32869\n",
      "echo: 5951  loss: 0.32869\n",
      "echo: 5952  loss: 0.32869\n",
      "echo: 5953  loss: 0.32869\n",
      "echo: 5954  loss: 0.32869\n",
      "echo: 5955  loss: 0.32869\n",
      "echo: 5956  loss: 0.32868\n",
      "echo: 5957  loss: 0.32868\n",
      "echo: 5958  loss: 0.32868\n",
      "echo: 5959  loss: 0.32868\n",
      "echo: 5960  loss: 0.32868\n",
      "echo: 5961  loss: 0.32868\n",
      "echo: 5962  loss: 0.32868\n",
      "echo: 5963  loss: 0.32868\n",
      "echo: 5964  loss: 0.32868\n",
      "echo: 5965  loss: 0.32868\n",
      "echo: 5966  loss: 0.32868\n",
      "echo: 5967  loss: 0.32868\n",
      "echo: 5968  loss: 0.32868\n",
      "echo: 5969  loss: 0.32868\n",
      "echo: 5970  loss: 0.32868\n",
      "echo: 5971  loss: 0.32868\n",
      "echo: 5972  loss: 0.32868\n",
      "echo: 5973  loss: 0.32868\n",
      "echo: 5974  loss: 0.32868\n",
      "echo: 5975  loss: 0.32868\n",
      "echo: 5976  loss: 0.32868\n",
      "echo: 5977  loss: 0.32868\n",
      "echo: 5978  loss: 0.32868\n",
      "echo: 5979  loss: 0.32868\n",
      "echo: 5980  loss: 0.32868\n",
      "echo: 5981  loss: 0.32868\n",
      "echo: 5982  loss: 0.32868\n",
      "echo: 5983  loss: 0.32868\n",
      "echo: 5984  loss: 0.32868\n",
      "echo: 5985  loss: 0.32868\n",
      "echo: 5986  loss: 0.32868\n",
      "echo: 5987  loss: 0.32868\n",
      "echo: 5988  loss: 0.32868\n",
      "echo: 5989  loss: 0.32868\n",
      "echo: 5990  loss: 0.32868\n",
      "echo: 5991  loss: 0.32868\n",
      "echo: 5992  loss: 0.32868\n",
      "echo: 5993  loss: 0.32868\n",
      "echo: 5994  loss: 0.32868\n",
      "echo: 5995  loss: 0.32868\n",
      "echo: 5996  loss: 0.32868\n",
      "echo: 5997  loss: 0.32868\n",
      "echo: 5998  loss: 0.32868\n",
      "echo: 5999  loss: 0.32867\n",
      "echo: 6000  loss: 0.32867\n",
      "echo: 6001  loss: 0.32867\n",
      "echo: 6002  loss: 0.32867\n",
      "echo: 6003  loss: 0.32867\n",
      "echo: 6004  loss: 0.32867\n",
      "echo: 6005  loss: 0.32867\n",
      "echo: 6006  loss: 0.32867\n",
      "echo: 6007  loss: 0.32867\n",
      "echo: 6008  loss: 0.32867\n",
      "echo: 6009  loss: 0.32867\n",
      "echo: 6010  loss: 0.32867\n",
      "echo: 6011  loss: 0.32867\n",
      "echo: 6012  loss: 0.32867\n",
      "echo: 6013  loss: 0.32867\n",
      "echo: 6014  loss: 0.32867\n",
      "echo: 6015  loss: 0.32867\n",
      "echo: 6016  loss: 0.32867\n",
      "echo: 6017  loss: 0.32867\n",
      "echo: 6018  loss: 0.32867\n",
      "echo: 6019  loss: 0.32867\n",
      "echo: 6020  loss: 0.32867\n",
      "echo: 6021  loss: 0.32867\n",
      "echo: 6022  loss: 0.32867\n",
      "echo: 6023  loss: 0.32867\n",
      "echo: 6024  loss: 0.32867\n",
      "echo: 6025  loss: 0.32867\n",
      "echo: 6026  loss: 0.32867\n",
      "echo: 6027  loss: 0.32867\n",
      "echo: 6028  loss: 0.32867\n",
      "echo: 6029  loss: 0.32867\n",
      "echo: 6030  loss: 0.32867\n",
      "echo: 6031  loss: 0.32867\n",
      "echo: 6032  loss: 0.32867\n",
      "echo: 6033  loss: 0.32867\n",
      "echo: 6034  loss: 0.32867\n",
      "echo: 6035  loss: 0.32867\n",
      "echo: 6036  loss: 0.32867\n",
      "echo: 6037  loss: 0.32867\n",
      "echo: 6038  loss: 0.32867\n",
      "echo: 6039  loss: 0.32867\n",
      "echo: 6040  loss: 0.32867\n",
      "echo: 6041  loss: 0.32867\n",
      "echo: 6042  loss: 0.32867\n",
      "echo: 6043  loss: 0.32866\n",
      "echo: 6044  loss: 0.32866\n",
      "echo: 6045  loss: 0.32866\n",
      "echo: 6046  loss: 0.32866\n",
      "echo: 6047  loss: 0.32866\n",
      "echo: 6048  loss: 0.32866\n",
      "echo: 6049  loss: 0.32866\n",
      "echo: 6050  loss: 0.32866\n",
      "echo: 6051  loss: 0.32866\n",
      "echo: 6052  loss: 0.32866\n",
      "echo: 6053  loss: 0.32866\n",
      "echo: 6054  loss: 0.32866\n",
      "echo: 6055  loss: 0.32866\n",
      "echo: 6056  loss: 0.32866\n",
      "echo: 6057  loss: 0.32866\n",
      "echo: 6058  loss: 0.32866\n",
      "echo: 6059  loss: 0.32866\n",
      "echo: 6060  loss: 0.32866\n",
      "echo: 6061  loss: 0.32866\n",
      "echo: 6062  loss: 0.32866\n",
      "echo: 6063  loss: 0.32866\n",
      "echo: 6064  loss: 0.32866\n",
      "echo: 6065  loss: 0.32866\n",
      "echo: 6066  loss: 0.32866\n",
      "echo: 6067  loss: 0.32866\n",
      "echo: 6068  loss: 0.32866\n",
      "echo: 6069  loss: 0.32866\n",
      "echo: 6070  loss: 0.32866\n",
      "echo: 6071  loss: 0.32866\n",
      "echo: 6072  loss: 0.32866\n",
      "echo: 6073  loss: 0.32866\n",
      "echo: 6074  loss: 0.32866\n",
      "echo: 6075  loss: 0.32866\n",
      "echo: 6076  loss: 0.32866\n",
      "echo: 6077  loss: 0.32866\n",
      "echo: 6078  loss: 0.32866\n",
      "echo: 6079  loss: 0.32866\n",
      "echo: 6080  loss: 0.32866\n",
      "echo: 6081  loss: 0.32866\n",
      "echo: 6082  loss: 0.32866\n",
      "echo: 6083  loss: 0.32866\n",
      "echo: 6084  loss: 0.32866\n",
      "echo: 6085  loss: 0.32866\n",
      "echo: 6086  loss: 0.32866\n",
      "echo: 6087  loss: 0.32866\n",
      "echo: 6088  loss: 0.32865\n",
      "echo: 6089  loss: 0.32865\n",
      "echo: 6090  loss: 0.32865\n",
      "echo: 6091  loss: 0.32865\n",
      "echo: 6092  loss: 0.32865\n",
      "echo: 6093  loss: 0.32865\n",
      "echo: 6094  loss: 0.32865\n",
      "echo: 6095  loss: 0.32865\n",
      "echo: 6096  loss: 0.32865\n",
      "echo: 6097  loss: 0.32865\n",
      "echo: 6098  loss: 0.32865\n",
      "echo: 6099  loss: 0.32865\n",
      "echo: 6100  loss: 0.32865\n",
      "echo: 6101  loss: 0.32865\n",
      "echo: 6102  loss: 0.32865\n",
      "echo: 6103  loss: 0.32865\n",
      "echo: 6104  loss: 0.32865\n",
      "echo: 6105  loss: 0.32865\n",
      "echo: 6106  loss: 0.32865\n",
      "echo: 6107  loss: 0.32865\n",
      "echo: 6108  loss: 0.32865\n",
      "echo: 6109  loss: 0.32865\n",
      "echo: 6110  loss: 0.32865\n",
      "echo: 6111  loss: 0.32865\n",
      "echo: 6112  loss: 0.32865\n",
      "echo: 6113  loss: 0.32865\n",
      "echo: 6114  loss: 0.32865\n",
      "echo: 6115  loss: 0.32865\n",
      "echo: 6116  loss: 0.32865\n",
      "echo: 6117  loss: 0.32865\n",
      "echo: 6118  loss: 0.32865\n",
      "echo: 6119  loss: 0.32865\n",
      "echo: 6120  loss: 0.32865\n",
      "echo: 6121  loss: 0.32865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 6122  loss: 0.32865\n",
      "echo: 6123  loss: 0.32865\n",
      "echo: 6124  loss: 0.32865\n",
      "echo: 6125  loss: 0.32865\n",
      "echo: 6126  loss: 0.32865\n",
      "echo: 6127  loss: 0.32865\n",
      "echo: 6128  loss: 0.32865\n",
      "echo: 6129  loss: 0.32865\n",
      "echo: 6130  loss: 0.32865\n",
      "echo: 6131  loss: 0.32865\n",
      "echo: 6132  loss: 0.32865\n",
      "echo: 6133  loss: 0.32865\n",
      "echo: 6134  loss: 0.32865\n",
      "echo: 6135  loss: 0.32864\n",
      "echo: 6136  loss: 0.32864\n",
      "echo: 6137  loss: 0.32864\n",
      "echo: 6138  loss: 0.32864\n",
      "echo: 6139  loss: 0.32864\n",
      "echo: 6140  loss: 0.32864\n",
      "echo: 6141  loss: 0.32864\n",
      "echo: 6142  loss: 0.32864\n",
      "echo: 6143  loss: 0.32864\n",
      "echo: 6144  loss: 0.32864\n",
      "echo: 6145  loss: 0.32864\n",
      "echo: 6146  loss: 0.32864\n",
      "echo: 6147  loss: 0.32864\n",
      "echo: 6148  loss: 0.32864\n",
      "echo: 6149  loss: 0.32864\n",
      "echo: 6150  loss: 0.32864\n",
      "echo: 6151  loss: 0.32864\n",
      "echo: 6152  loss: 0.32864\n",
      "echo: 6153  loss: 0.32864\n",
      "echo: 6154  loss: 0.32864\n",
      "echo: 6155  loss: 0.32864\n",
      "echo: 6156  loss: 0.32864\n",
      "echo: 6157  loss: 0.32864\n",
      "echo: 6158  loss: 0.32864\n",
      "echo: 6159  loss: 0.32864\n",
      "echo: 6160  loss: 0.32864\n",
      "echo: 6161  loss: 0.32864\n",
      "echo: 6162  loss: 0.32864\n",
      "echo: 6163  loss: 0.32864\n",
      "echo: 6164  loss: 0.32864\n",
      "echo: 6165  loss: 0.32864\n",
      "echo: 6166  loss: 0.32864\n",
      "echo: 6167  loss: 0.32864\n",
      "echo: 6168  loss: 0.32864\n",
      "echo: 6169  loss: 0.32864\n",
      "echo: 6170  loss: 0.32864\n",
      "echo: 6171  loss: 0.32864\n",
      "echo: 6172  loss: 0.32864\n",
      "echo: 6173  loss: 0.32864\n",
      "echo: 6174  loss: 0.32864\n",
      "echo: 6175  loss: 0.32864\n",
      "echo: 6176  loss: 0.32864\n",
      "echo: 6177  loss: 0.32864\n",
      "echo: 6178  loss: 0.32864\n",
      "echo: 6179  loss: 0.32864\n",
      "echo: 6180  loss: 0.32864\n",
      "echo: 6181  loss: 0.32864\n",
      "echo: 6182  loss: 0.32864\n",
      "echo: 6183  loss: 0.32863\n",
      "echo: 6184  loss: 0.32863\n",
      "echo: 6185  loss: 0.32863\n",
      "echo: 6186  loss: 0.32863\n",
      "echo: 6187  loss: 0.32863\n",
      "echo: 6188  loss: 0.32863\n",
      "echo: 6189  loss: 0.32863\n",
      "echo: 6190  loss: 0.32863\n",
      "echo: 6191  loss: 0.32863\n",
      "echo: 6192  loss: 0.32863\n",
      "echo: 6193  loss: 0.32863\n",
      "echo: 6194  loss: 0.32863\n",
      "echo: 6195  loss: 0.32863\n",
      "echo: 6196  loss: 0.32863\n",
      "echo: 6197  loss: 0.32863\n",
      "echo: 6198  loss: 0.32863\n",
      "echo: 6199  loss: 0.32863\n",
      "echo: 6200  loss: 0.32863\n",
      "echo: 6201  loss: 0.32863\n",
      "echo: 6202  loss: 0.32863\n",
      "echo: 6203  loss: 0.32863\n",
      "echo: 6204  loss: 0.32863\n",
      "echo: 6205  loss: 0.32863\n",
      "echo: 6206  loss: 0.32863\n",
      "echo: 6207  loss: 0.32863\n",
      "echo: 6208  loss: 0.32863\n",
      "echo: 6209  loss: 0.32863\n",
      "echo: 6210  loss: 0.32863\n",
      "echo: 6211  loss: 0.32863\n",
      "echo: 6212  loss: 0.32863\n",
      "echo: 6213  loss: 0.32863\n",
      "echo: 6214  loss: 0.32863\n",
      "echo: 6215  loss: 0.32863\n",
      "echo: 6216  loss: 0.32863\n",
      "echo: 6217  loss: 0.32863\n",
      "echo: 6218  loss: 0.32863\n",
      "echo: 6219  loss: 0.32863\n",
      "echo: 6220  loss: 0.32863\n",
      "echo: 6221  loss: 0.32863\n",
      "echo: 6222  loss: 0.32863\n",
      "echo: 6223  loss: 0.32863\n",
      "echo: 6224  loss: 0.32863\n",
      "echo: 6225  loss: 0.32863\n",
      "echo: 6226  loss: 0.32863\n",
      "echo: 6227  loss: 0.32863\n",
      "echo: 6228  loss: 0.32863\n",
      "echo: 6229  loss: 0.32863\n",
      "echo: 6230  loss: 0.32863\n",
      "echo: 6231  loss: 0.32863\n",
      "echo: 6232  loss: 0.32862\n",
      "echo: 6233  loss: 0.32862\n",
      "echo: 6234  loss: 0.32862\n",
      "echo: 6235  loss: 0.32862\n",
      "echo: 6236  loss: 0.32862\n",
      "echo: 6237  loss: 0.32862\n",
      "echo: 6238  loss: 0.32862\n",
      "echo: 6239  loss: 0.32862\n",
      "echo: 6240  loss: 0.32862\n",
      "echo: 6241  loss: 0.32862\n",
      "echo: 6242  loss: 0.32862\n",
      "echo: 6243  loss: 0.32862\n",
      "echo: 6244  loss: 0.32862\n",
      "echo: 6245  loss: 0.32862\n",
      "echo: 6246  loss: 0.32862\n",
      "echo: 6247  loss: 0.32862\n",
      "echo: 6248  loss: 0.32862\n",
      "echo: 6249  loss: 0.32862\n",
      "echo: 6250  loss: 0.32862\n",
      "echo: 6251  loss: 0.32862\n",
      "echo: 6252  loss: 0.32862\n",
      "echo: 6253  loss: 0.32862\n",
      "echo: 6254  loss: 0.32862\n",
      "echo: 6255  loss: 0.32862\n",
      "echo: 6256  loss: 0.32862\n",
      "echo: 6257  loss: 0.32862\n",
      "echo: 6258  loss: 0.32862\n",
      "echo: 6259  loss: 0.32862\n",
      "echo: 6260  loss: 0.32862\n",
      "echo: 6261  loss: 0.32862\n",
      "echo: 6262  loss: 0.32862\n",
      "echo: 6263  loss: 0.32862\n",
      "echo: 6264  loss: 0.32862\n",
      "echo: 6265  loss: 0.32862\n",
      "echo: 6266  loss: 0.32862\n",
      "echo: 6267  loss: 0.32862\n",
      "echo: 6268  loss: 0.32862\n",
      "echo: 6269  loss: 0.32862\n",
      "echo: 6270  loss: 0.32862\n",
      "echo: 6271  loss: 0.32862\n",
      "echo: 6272  loss: 0.32862\n",
      "echo: 6273  loss: 0.32862\n",
      "echo: 6274  loss: 0.32862\n",
      "echo: 6275  loss: 0.32862\n",
      "echo: 6276  loss: 0.32862\n",
      "echo: 6277  loss: 0.32862\n",
      "echo: 6278  loss: 0.32862\n",
      "echo: 6279  loss: 0.32862\n",
      "echo: 6280  loss: 0.32862\n",
      "echo: 6281  loss: 0.32862\n",
      "echo: 6282  loss: 0.32862\n",
      "echo: 6283  loss: 0.32861\n",
      "echo: 6284  loss: 0.32861\n",
      "echo: 6285  loss: 0.32861\n",
      "echo: 6286  loss: 0.32861\n",
      "echo: 6287  loss: 0.32861\n",
      "echo: 6288  loss: 0.32861\n",
      "echo: 6289  loss: 0.32861\n",
      "echo: 6290  loss: 0.32861\n",
      "echo: 6291  loss: 0.32861\n",
      "echo: 6292  loss: 0.32861\n",
      "echo: 6293  loss: 0.32861\n",
      "echo: 6294  loss: 0.32861\n",
      "echo: 6295  loss: 0.32861\n",
      "echo: 6296  loss: 0.32861\n",
      "echo: 6297  loss: 0.32861\n",
      "echo: 6298  loss: 0.32861\n",
      "echo: 6299  loss: 0.32861\n",
      "echo: 6300  loss: 0.32861\n",
      "echo: 6301  loss: 0.32861\n",
      "echo: 6302  loss: 0.32861\n",
      "echo: 6303  loss: 0.32861\n",
      "echo: 6304  loss: 0.32861\n",
      "echo: 6305  loss: 0.32861\n",
      "echo: 6306  loss: 0.32861\n",
      "echo: 6307  loss: 0.32861\n",
      "echo: 6308  loss: 0.32861\n",
      "echo: 6309  loss: 0.32861\n",
      "echo: 6310  loss: 0.32861\n",
      "echo: 6311  loss: 0.32861\n",
      "echo: 6312  loss: 0.32861\n",
      "echo: 6313  loss: 0.32861\n",
      "echo: 6314  loss: 0.32861\n",
      "echo: 6315  loss: 0.32861\n",
      "echo: 6316  loss: 0.32861\n",
      "echo: 6317  loss: 0.32861\n",
      "echo: 6318  loss: 0.32861\n",
      "echo: 6319  loss: 0.32861\n",
      "echo: 6320  loss: 0.32861\n",
      "echo: 6321  loss: 0.32861\n",
      "echo: 6322  loss: 0.32861\n",
      "echo: 6323  loss: 0.32861\n",
      "echo: 6324  loss: 0.32861\n",
      "echo: 6325  loss: 0.32861\n",
      "echo: 6326  loss: 0.32861\n",
      "echo: 6327  loss: 0.32861\n",
      "echo: 6328  loss: 0.32861\n",
      "echo: 6329  loss: 0.32861\n",
      "echo: 6330  loss: 0.32861\n",
      "echo: 6331  loss: 0.32861\n",
      "echo: 6332  loss: 0.32861\n",
      "echo: 6333  loss: 0.32861\n",
      "echo: 6334  loss: 0.32861\n",
      "echo: 6335  loss: 0.3286\n",
      "echo: 6336  loss: 0.3286\n",
      "echo: 6337  loss: 0.3286\n",
      "echo: 6338  loss: 0.3286\n",
      "echo: 6339  loss: 0.3286\n",
      "echo: 6340  loss: 0.3286\n",
      "echo: 6341  loss: 0.3286\n",
      "echo: 6342  loss: 0.3286\n",
      "echo: 6343  loss: 0.3286\n",
      "echo: 6344  loss: 0.3286\n",
      "echo: 6345  loss: 0.3286\n",
      "echo: 6346  loss: 0.3286\n",
      "echo: 6347  loss: 0.3286\n",
      "echo: 6348  loss: 0.3286\n",
      "echo: 6349  loss: 0.3286\n",
      "echo: 6350  loss: 0.3286\n",
      "echo: 6351  loss: 0.3286\n",
      "echo: 6352  loss: 0.3286\n",
      "echo: 6353  loss: 0.3286\n",
      "echo: 6354  loss: 0.3286\n",
      "echo: 6355  loss: 0.3286\n",
      "echo: 6356  loss: 0.3286\n",
      "echo: 6357  loss: 0.3286\n",
      "echo: 6358  loss: 0.3286\n",
      "echo: 6359  loss: 0.3286\n",
      "echo: 6360  loss: 0.3286\n",
      "echo: 6361  loss: 0.3286\n",
      "echo: 6362  loss: 0.3286\n",
      "echo: 6363  loss: 0.3286\n",
      "echo: 6364  loss: 0.3286\n",
      "echo: 6365  loss: 0.3286\n",
      "echo: 6366  loss: 0.3286\n",
      "echo: 6367  loss: 0.3286\n",
      "echo: 6368  loss: 0.3286\n",
      "echo: 6369  loss: 0.3286\n",
      "echo: 6370  loss: 0.3286\n",
      "echo: 6371  loss: 0.3286\n",
      "echo: 6372  loss: 0.3286\n",
      "echo: 6373  loss: 0.3286\n",
      "echo: 6374  loss: 0.3286\n",
      "echo: 6375  loss: 0.3286\n",
      "echo: 6376  loss: 0.3286\n",
      "echo: 6377  loss: 0.3286\n",
      "echo: 6378  loss: 0.3286\n",
      "echo: 6379  loss: 0.3286\n",
      "echo: 6380  loss: 0.3286\n",
      "echo: 6381  loss: 0.3286\n",
      "echo: 6382  loss: 0.3286\n",
      "echo: 6383  loss: 0.3286\n",
      "echo: 6384  loss: 0.3286\n",
      "echo: 6385  loss: 0.3286\n",
      "echo: 6386  loss: 0.3286\n",
      "echo: 6387  loss: 0.3286\n",
      "echo: 6388  loss: 0.3286\n",
      "echo: 6389  loss: 0.32859\n",
      "echo: 6390  loss: 0.32859\n",
      "echo: 6391  loss: 0.32859\n",
      "echo: 6392  loss: 0.32859\n",
      "echo: 6393  loss: 0.32859\n",
      "echo: 6394  loss: 0.32859\n",
      "echo: 6395  loss: 0.32859\n",
      "echo: 6396  loss: 0.32859\n",
      "echo: 6397  loss: 0.32859\n",
      "echo: 6398  loss: 0.32859\n",
      "echo: 6399  loss: 0.32859\n",
      "echo: 6400  loss: 0.32859\n",
      "echo: 6401  loss: 0.32859\n",
      "echo: 6402  loss: 0.32859\n",
      "echo: 6403  loss: 0.32859\n",
      "echo: 6404  loss: 0.32859\n",
      "echo: 6405  loss: 0.32859\n",
      "echo: 6406  loss: 0.32859\n",
      "echo: 6407  loss: 0.32859\n",
      "echo: 6408  loss: 0.32859\n",
      "echo: 6409  loss: 0.32859\n",
      "echo: 6410  loss: 0.32859\n",
      "echo: 6411  loss: 0.32859\n",
      "echo: 6412  loss: 0.32859\n",
      "echo: 6413  loss: 0.32859\n",
      "echo: 6414  loss: 0.32859\n",
      "echo: 6415  loss: 0.32859\n",
      "echo: 6416  loss: 0.32859\n",
      "echo: 6417  loss: 0.32859\n",
      "echo: 6418  loss: 0.32859\n",
      "echo: 6419  loss: 0.32859\n",
      "echo: 6420  loss: 0.32859\n",
      "echo: 6421  loss: 0.32859\n",
      "echo: 6422  loss: 0.32859\n",
      "echo: 6423  loss: 0.32859\n",
      "echo: 6424  loss: 0.32859\n",
      "echo: 6425  loss: 0.32859\n",
      "echo: 6426  loss: 0.32859\n",
      "echo: 6427  loss: 0.32859\n",
      "echo: 6428  loss: 0.32859\n",
      "echo: 6429  loss: 0.32859\n",
      "echo: 6430  loss: 0.32859\n",
      "echo: 6431  loss: 0.32859\n",
      "echo: 6432  loss: 0.32859\n",
      "echo: 6433  loss: 0.32859\n",
      "echo: 6434  loss: 0.32859\n",
      "echo: 6435  loss: 0.32859\n",
      "echo: 6436  loss: 0.32859\n",
      "echo: 6437  loss: 0.32859\n",
      "echo: 6438  loss: 0.32859\n",
      "echo: 6439  loss: 0.32859\n",
      "echo: 6440  loss: 0.32859\n",
      "echo: 6441  loss: 0.32859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 6442  loss: 0.32859\n",
      "echo: 6443  loss: 0.32859\n",
      "echo: 6444  loss: 0.32859\n",
      "echo: 6445  loss: 0.32858\n",
      "echo: 6446  loss: 0.32858\n",
      "echo: 6447  loss: 0.32858\n",
      "echo: 6448  loss: 0.32858\n",
      "echo: 6449  loss: 0.32858\n",
      "echo: 6450  loss: 0.32858\n",
      "echo: 6451  loss: 0.32858\n",
      "echo: 6452  loss: 0.32858\n",
      "echo: 6453  loss: 0.32858\n",
      "echo: 6454  loss: 0.32858\n",
      "echo: 6455  loss: 0.32858\n",
      "echo: 6456  loss: 0.32858\n",
      "echo: 6457  loss: 0.32858\n",
      "echo: 6458  loss: 0.32858\n",
      "echo: 6459  loss: 0.32858\n",
      "echo: 6460  loss: 0.32858\n",
      "echo: 6461  loss: 0.32858\n",
      "echo: 6462  loss: 0.32858\n",
      "echo: 6463  loss: 0.32858\n",
      "echo: 6464  loss: 0.32858\n",
      "echo: 6465  loss: 0.32858\n",
      "echo: 6466  loss: 0.32858\n",
      "echo: 6467  loss: 0.32858\n",
      "echo: 6468  loss: 0.32858\n",
      "echo: 6469  loss: 0.32858\n",
      "echo: 6470  loss: 0.32858\n",
      "echo: 6471  loss: 0.32858\n",
      "echo: 6472  loss: 0.32858\n",
      "echo: 6473  loss: 0.32858\n",
      "echo: 6474  loss: 0.32858\n",
      "echo: 6475  loss: 0.32858\n",
      "echo: 6476  loss: 0.32858\n",
      "echo: 6477  loss: 0.32858\n",
      "echo: 6478  loss: 0.32858\n",
      "echo: 6479  loss: 0.32858\n",
      "echo: 6480  loss: 0.32858\n",
      "echo: 6481  loss: 0.32858\n",
      "echo: 6482  loss: 0.32858\n",
      "echo: 6483  loss: 0.32858\n",
      "echo: 6484  loss: 0.32858\n",
      "echo: 6485  loss: 0.32858\n",
      "echo: 6486  loss: 0.32858\n",
      "echo: 6487  loss: 0.32858\n",
      "echo: 6488  loss: 0.32858\n",
      "echo: 6489  loss: 0.32858\n",
      "echo: 6490  loss: 0.32858\n",
      "echo: 6491  loss: 0.32858\n",
      "echo: 6492  loss: 0.32858\n",
      "echo: 6493  loss: 0.32858\n",
      "echo: 6494  loss: 0.32858\n",
      "echo: 6495  loss: 0.32858\n",
      "echo: 6496  loss: 0.32858\n",
      "echo: 6497  loss: 0.32858\n",
      "echo: 6498  loss: 0.32858\n",
      "echo: 6499  loss: 0.32858\n",
      "echo: 6500  loss: 0.32858\n",
      "echo: 6501  loss: 0.32858\n",
      "echo: 6502  loss: 0.32858\n",
      "echo: 6503  loss: 0.32857\n",
      "echo: 6504  loss: 0.32857\n",
      "echo: 6505  loss: 0.32857\n",
      "echo: 6506  loss: 0.32857\n",
      "echo: 6507  loss: 0.32857\n",
      "echo: 6508  loss: 0.32857\n",
      "echo: 6509  loss: 0.32857\n",
      "echo: 6510  loss: 0.32857\n",
      "echo: 6511  loss: 0.32857\n",
      "echo: 6512  loss: 0.32857\n",
      "echo: 6513  loss: 0.32857\n",
      "echo: 6514  loss: 0.32857\n",
      "echo: 6515  loss: 0.32857\n",
      "echo: 6516  loss: 0.32857\n",
      "echo: 6517  loss: 0.32857\n",
      "echo: 6518  loss: 0.32857\n",
      "echo: 6519  loss: 0.32857\n",
      "echo: 6520  loss: 0.32857\n",
      "echo: 6521  loss: 0.32857\n",
      "echo: 6522  loss: 0.32857\n",
      "echo: 6523  loss: 0.32857\n",
      "echo: 6524  loss: 0.32857\n",
      "echo: 6525  loss: 0.32857\n",
      "echo: 6526  loss: 0.32857\n",
      "echo: 6527  loss: 0.32857\n",
      "echo: 6528  loss: 0.32857\n",
      "echo: 6529  loss: 0.32857\n",
      "echo: 6530  loss: 0.32857\n",
      "echo: 6531  loss: 0.32857\n",
      "echo: 6532  loss: 0.32857\n",
      "echo: 6533  loss: 0.32857\n",
      "echo: 6534  loss: 0.32857\n",
      "echo: 6535  loss: 0.32857\n",
      "echo: 6536  loss: 0.32857\n",
      "echo: 6537  loss: 0.32857\n",
      "echo: 6538  loss: 0.32857\n",
      "echo: 6539  loss: 0.32857\n",
      "echo: 6540  loss: 0.32857\n",
      "echo: 6541  loss: 0.32857\n",
      "echo: 6542  loss: 0.32857\n",
      "echo: 6543  loss: 0.32857\n",
      "echo: 6544  loss: 0.32857\n",
      "echo: 6545  loss: 0.32857\n",
      "echo: 6546  loss: 0.32857\n",
      "echo: 6547  loss: 0.32857\n",
      "echo: 6548  loss: 0.32857\n",
      "echo: 6549  loss: 0.32857\n",
      "echo: 6550  loss: 0.32857\n",
      "echo: 6551  loss: 0.32857\n",
      "echo: 6552  loss: 0.32857\n",
      "echo: 6553  loss: 0.32857\n",
      "echo: 6554  loss: 0.32857\n",
      "echo: 6555  loss: 0.32857\n",
      "echo: 6556  loss: 0.32857\n",
      "echo: 6557  loss: 0.32857\n",
      "echo: 6558  loss: 0.32857\n",
      "echo: 6559  loss: 0.32857\n",
      "echo: 6560  loss: 0.32857\n",
      "echo: 6561  loss: 0.32857\n",
      "echo: 6562  loss: 0.32857\n",
      "echo: 6563  loss: 0.32856\n",
      "echo: 6564  loss: 0.32856\n",
      "echo: 6565  loss: 0.32856\n",
      "echo: 6566  loss: 0.32856\n",
      "echo: 6567  loss: 0.32856\n",
      "echo: 6568  loss: 0.32856\n",
      "echo: 6569  loss: 0.32856\n",
      "echo: 6570  loss: 0.32856\n",
      "echo: 6571  loss: 0.32856\n",
      "echo: 6572  loss: 0.32856\n",
      "echo: 6573  loss: 0.32856\n",
      "echo: 6574  loss: 0.32856\n",
      "echo: 6575  loss: 0.32856\n",
      "echo: 6576  loss: 0.32856\n",
      "echo: 6577  loss: 0.32856\n",
      "echo: 6578  loss: 0.32856\n",
      "echo: 6579  loss: 0.32856\n",
      "echo: 6580  loss: 0.32856\n",
      "echo: 6581  loss: 0.32856\n",
      "echo: 6582  loss: 0.32856\n",
      "echo: 6583  loss: 0.32856\n",
      "echo: 6584  loss: 0.32856\n",
      "echo: 6585  loss: 0.32856\n",
      "echo: 6586  loss: 0.32856\n",
      "echo: 6587  loss: 0.32856\n",
      "echo: 6588  loss: 0.32856\n",
      "echo: 6589  loss: 0.32856\n",
      "echo: 6590  loss: 0.32856\n",
      "echo: 6591  loss: 0.32856\n",
      "echo: 6592  loss: 0.32856\n",
      "echo: 6593  loss: 0.32856\n",
      "echo: 6594  loss: 0.32856\n",
      "echo: 6595  loss: 0.32856\n",
      "echo: 6596  loss: 0.32856\n",
      "echo: 6597  loss: 0.32856\n",
      "echo: 6598  loss: 0.32856\n",
      "echo: 6599  loss: 0.32856\n",
      "echo: 6600  loss: 0.32856\n",
      "echo: 6601  loss: 0.32856\n",
      "echo: 6602  loss: 0.32856\n",
      "echo: 6603  loss: 0.32856\n",
      "echo: 6604  loss: 0.32856\n",
      "echo: 6605  loss: 0.32856\n",
      "echo: 6606  loss: 0.32856\n",
      "echo: 6607  loss: 0.32856\n",
      "echo: 6608  loss: 0.32856\n",
      "echo: 6609  loss: 0.32856\n",
      "echo: 6610  loss: 0.32856\n",
      "echo: 6611  loss: 0.32856\n",
      "echo: 6612  loss: 0.32856\n",
      "echo: 6613  loss: 0.32856\n",
      "echo: 6614  loss: 0.32856\n",
      "echo: 6615  loss: 0.32856\n",
      "echo: 6616  loss: 0.32856\n",
      "echo: 6617  loss: 0.32856\n",
      "echo: 6618  loss: 0.32856\n",
      "echo: 6619  loss: 0.32856\n",
      "echo: 6620  loss: 0.32856\n",
      "echo: 6621  loss: 0.32856\n",
      "echo: 6622  loss: 0.32856\n",
      "echo: 6623  loss: 0.32856\n",
      "echo: 6624  loss: 0.32855\n",
      "echo: 6625  loss: 0.32855\n",
      "echo: 6626  loss: 0.32855\n",
      "echo: 6627  loss: 0.32855\n",
      "echo: 6628  loss: 0.32855\n",
      "echo: 6629  loss: 0.32855\n",
      "echo: 6630  loss: 0.32855\n",
      "echo: 6631  loss: 0.32855\n",
      "echo: 6632  loss: 0.32855\n",
      "echo: 6633  loss: 0.32855\n",
      "echo: 6634  loss: 0.32855\n",
      "echo: 6635  loss: 0.32855\n",
      "echo: 6636  loss: 0.32855\n",
      "echo: 6637  loss: 0.32855\n",
      "echo: 6638  loss: 0.32855\n",
      "echo: 6639  loss: 0.32855\n",
      "echo: 6640  loss: 0.32855\n",
      "echo: 6641  loss: 0.32855\n",
      "echo: 6642  loss: 0.32855\n",
      "echo: 6643  loss: 0.32855\n",
      "echo: 6644  loss: 0.32855\n",
      "echo: 6645  loss: 0.32855\n",
      "echo: 6646  loss: 0.32855\n",
      "echo: 6647  loss: 0.32855\n",
      "echo: 6648  loss: 0.32855\n",
      "echo: 6649  loss: 0.32855\n",
      "echo: 6650  loss: 0.32855\n",
      "echo: 6651  loss: 0.32855\n",
      "echo: 6652  loss: 0.32855\n",
      "echo: 6653  loss: 0.32855\n",
      "echo: 6654  loss: 0.32855\n",
      "echo: 6655  loss: 0.32855\n",
      "echo: 6656  loss: 0.32855\n",
      "echo: 6657  loss: 0.32855\n",
      "echo: 6658  loss: 0.32855\n",
      "echo: 6659  loss: 0.32855\n",
      "echo: 6660  loss: 0.32855\n",
      "echo: 6661  loss: 0.32855\n",
      "echo: 6662  loss: 0.32855\n",
      "echo: 6663  loss: 0.32855\n",
      "echo: 6664  loss: 0.32855\n",
      "echo: 6665  loss: 0.32855\n",
      "echo: 6666  loss: 0.32855\n",
      "echo: 6667  loss: 0.32855\n",
      "echo: 6668  loss: 0.32855\n",
      "echo: 6669  loss: 0.32855\n",
      "echo: 6670  loss: 0.32855\n",
      "echo: 6671  loss: 0.32855\n",
      "echo: 6672  loss: 0.32855\n",
      "echo: 6673  loss: 0.32855\n",
      "echo: 6674  loss: 0.32855\n",
      "echo: 6675  loss: 0.32855\n",
      "echo: 6676  loss: 0.32855\n",
      "echo: 6677  loss: 0.32855\n",
      "echo: 6678  loss: 0.32855\n",
      "echo: 6679  loss: 0.32855\n",
      "echo: 6680  loss: 0.32855\n",
      "echo: 6681  loss: 0.32855\n",
      "echo: 6682  loss: 0.32855\n",
      "echo: 6683  loss: 0.32855\n",
      "echo: 6684  loss: 0.32855\n",
      "echo: 6685  loss: 0.32855\n",
      "echo: 6686  loss: 0.32855\n",
      "echo: 6687  loss: 0.32855\n",
      "echo: 6688  loss: 0.32855\n",
      "echo: 6689  loss: 0.32854\n",
      "echo: 6690  loss: 0.32854\n",
      "echo: 6691  loss: 0.32854\n",
      "echo: 6692  loss: 0.32854\n",
      "echo: 6693  loss: 0.32854\n",
      "echo: 6694  loss: 0.32854\n",
      "echo: 6695  loss: 0.32854\n",
      "echo: 6696  loss: 0.32854\n",
      "echo: 6697  loss: 0.32854\n",
      "echo: 6698  loss: 0.32854\n",
      "echo: 6699  loss: 0.32854\n",
      "echo: 6700  loss: 0.32854\n",
      "echo: 6701  loss: 0.32854\n",
      "echo: 6702  loss: 0.32854\n",
      "echo: 6703  loss: 0.32854\n",
      "echo: 6704  loss: 0.32854\n",
      "echo: 6705  loss: 0.32854\n",
      "echo: 6706  loss: 0.32854\n",
      "echo: 6707  loss: 0.32854\n",
      "echo: 6708  loss: 0.32854\n",
      "echo: 6709  loss: 0.32854\n",
      "echo: 6710  loss: 0.32854\n",
      "echo: 6711  loss: 0.32854\n",
      "echo: 6712  loss: 0.32854\n",
      "echo: 6713  loss: 0.32854\n",
      "echo: 6714  loss: 0.32854\n",
      "echo: 6715  loss: 0.32854\n",
      "echo: 6716  loss: 0.32854\n",
      "echo: 6717  loss: 0.32854\n",
      "echo: 6718  loss: 0.32854\n",
      "echo: 6719  loss: 0.32854\n",
      "echo: 6720  loss: 0.32854\n",
      "echo: 6721  loss: 0.32854\n",
      "echo: 6722  loss: 0.32854\n",
      "echo: 6723  loss: 0.32854\n",
      "echo: 6724  loss: 0.32854\n",
      "echo: 6725  loss: 0.32854\n",
      "echo: 6726  loss: 0.32854\n",
      "echo: 6727  loss: 0.32854\n",
      "echo: 6728  loss: 0.32854\n",
      "echo: 6729  loss: 0.32854\n",
      "echo: 6730  loss: 0.32854\n",
      "echo: 6731  loss: 0.32854\n",
      "echo: 6732  loss: 0.32854\n",
      "echo: 6733  loss: 0.32854\n",
      "echo: 6734  loss: 0.32854\n",
      "echo: 6735  loss: 0.32854\n",
      "echo: 6736  loss: 0.32854\n",
      "echo: 6737  loss: 0.32854\n",
      "echo: 6738  loss: 0.32854\n",
      "echo: 6739  loss: 0.32854\n",
      "echo: 6740  loss: 0.32854\n",
      "echo: 6741  loss: 0.32854\n",
      "echo: 6742  loss: 0.32854\n",
      "echo: 6743  loss: 0.32854\n",
      "echo: 6744  loss: 0.32854\n",
      "echo: 6745  loss: 0.32854\n",
      "echo: 6746  loss: 0.32854\n",
      "echo: 6747  loss: 0.32854\n",
      "echo: 6748  loss: 0.32854\n",
      "echo: 6749  loss: 0.32854\n",
      "echo: 6750  loss: 0.32854\n",
      "echo: 6751  loss: 0.32854\n",
      "echo: 6752  loss: 0.32854\n",
      "echo: 6753  loss: 0.32854\n",
      "echo: 6754  loss: 0.32854\n",
      "echo: 6755  loss: 0.32853\n",
      "echo: 6756  loss: 0.32853\n",
      "echo: 6757  loss: 0.32853\n",
      "echo: 6758  loss: 0.32853\n",
      "echo: 6759  loss: 0.32853\n",
      "echo: 6760  loss: 0.32853\n",
      "echo: 6761  loss: 0.32853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 6762  loss: 0.32853\n",
      "echo: 6763  loss: 0.32853\n",
      "echo: 6764  loss: 0.32853\n",
      "echo: 6765  loss: 0.32853\n",
      "echo: 6766  loss: 0.32853\n",
      "echo: 6767  loss: 0.32853\n",
      "echo: 6768  loss: 0.32853\n",
      "echo: 6769  loss: 0.32853\n",
      "echo: 6770  loss: 0.32853\n",
      "echo: 6771  loss: 0.32853\n",
      "echo: 6772  loss: 0.32853\n",
      "echo: 6773  loss: 0.32853\n",
      "echo: 6774  loss: 0.32853\n",
      "echo: 6775  loss: 0.32853\n",
      "echo: 6776  loss: 0.32853\n",
      "echo: 6777  loss: 0.32853\n",
      "echo: 6778  loss: 0.32853\n",
      "echo: 6779  loss: 0.32853\n",
      "echo: 6780  loss: 0.32853\n",
      "echo: 6781  loss: 0.32853\n",
      "echo: 6782  loss: 0.32853\n",
      "echo: 6783  loss: 0.32853\n",
      "echo: 6784  loss: 0.32853\n",
      "echo: 6785  loss: 0.32853\n",
      "echo: 6786  loss: 0.32853\n",
      "echo: 6787  loss: 0.32853\n",
      "echo: 6788  loss: 0.32853\n",
      "echo: 6789  loss: 0.32853\n",
      "echo: 6790  loss: 0.32853\n",
      "echo: 6791  loss: 0.32853\n",
      "echo: 6792  loss: 0.32853\n",
      "echo: 6793  loss: 0.32853\n",
      "echo: 6794  loss: 0.32853\n",
      "echo: 6795  loss: 0.32853\n",
      "echo: 6796  loss: 0.32853\n",
      "echo: 6797  loss: 0.32853\n",
      "echo: 6798  loss: 0.32853\n",
      "echo: 6799  loss: 0.32853\n",
      "echo: 6800  loss: 0.32853\n",
      "echo: 6801  loss: 0.32853\n",
      "echo: 6802  loss: 0.32853\n",
      "echo: 6803  loss: 0.32853\n",
      "echo: 6804  loss: 0.32853\n",
      "echo: 6805  loss: 0.32853\n",
      "echo: 6806  loss: 0.32853\n",
      "echo: 6807  loss: 0.32853\n",
      "echo: 6808  loss: 0.32853\n",
      "echo: 6809  loss: 0.32853\n",
      "echo: 6810  loss: 0.32853\n",
      "echo: 6811  loss: 0.32853\n",
      "echo: 6812  loss: 0.32853\n",
      "echo: 6813  loss: 0.32853\n",
      "echo: 6814  loss: 0.32853\n",
      "echo: 6815  loss: 0.32853\n",
      "echo: 6816  loss: 0.32853\n",
      "echo: 6817  loss: 0.32853\n",
      "echo: 6818  loss: 0.32853\n",
      "echo: 6819  loss: 0.32853\n",
      "echo: 6820  loss: 0.32853\n",
      "echo: 6821  loss: 0.32853\n",
      "echo: 6822  loss: 0.32853\n",
      "echo: 6823  loss: 0.32853\n",
      "echo: 6824  loss: 0.32853\n",
      "echo: 6825  loss: 0.32852\n",
      "echo: 6826  loss: 0.32852\n",
      "echo: 6827  loss: 0.32852\n",
      "echo: 6828  loss: 0.32852\n",
      "echo: 6829  loss: 0.32852\n",
      "echo: 6830  loss: 0.32852\n",
      "echo: 6831  loss: 0.32852\n",
      "echo: 6832  loss: 0.32852\n",
      "echo: 6833  loss: 0.32852\n",
      "echo: 6834  loss: 0.32852\n",
      "echo: 6835  loss: 0.32852\n",
      "echo: 6836  loss: 0.32852\n",
      "echo: 6837  loss: 0.32852\n",
      "echo: 6838  loss: 0.32852\n",
      "echo: 6839  loss: 0.32852\n",
      "echo: 6840  loss: 0.32852\n",
      "echo: 6841  loss: 0.32852\n",
      "echo: 6842  loss: 0.32852\n",
      "echo: 6843  loss: 0.32852\n",
      "echo: 6844  loss: 0.32852\n",
      "echo: 6845  loss: 0.32852\n",
      "echo: 6846  loss: 0.32852\n",
      "echo: 6847  loss: 0.32852\n",
      "echo: 6848  loss: 0.32852\n",
      "echo: 6849  loss: 0.32852\n",
      "echo: 6850  loss: 0.32852\n",
      "echo: 6851  loss: 0.32852\n",
      "echo: 6852  loss: 0.32852\n",
      "echo: 6853  loss: 0.32852\n",
      "echo: 6854  loss: 0.32852\n",
      "echo: 6855  loss: 0.32852\n",
      "echo: 6856  loss: 0.32852\n",
      "echo: 6857  loss: 0.32852\n",
      "echo: 6858  loss: 0.32852\n",
      "echo: 6859  loss: 0.32852\n",
      "echo: 6860  loss: 0.32852\n",
      "echo: 6861  loss: 0.32852\n",
      "echo: 6862  loss: 0.32852\n",
      "echo: 6863  loss: 0.32852\n",
      "echo: 6864  loss: 0.32852\n",
      "echo: 6865  loss: 0.32852\n",
      "echo: 6866  loss: 0.32852\n",
      "echo: 6867  loss: 0.32852\n",
      "echo: 6868  loss: 0.32852\n",
      "echo: 6869  loss: 0.32852\n",
      "echo: 6870  loss: 0.32852\n",
      "echo: 6871  loss: 0.32852\n",
      "echo: 6872  loss: 0.32852\n",
      "echo: 6873  loss: 0.32852\n",
      "echo: 6874  loss: 0.32852\n",
      "echo: 6875  loss: 0.32852\n",
      "echo: 6876  loss: 0.32852\n",
      "echo: 6877  loss: 0.32852\n",
      "echo: 6878  loss: 0.32852\n",
      "echo: 6879  loss: 0.32852\n",
      "echo: 6880  loss: 0.32852\n",
      "echo: 6881  loss: 0.32852\n",
      "echo: 6882  loss: 0.32852\n",
      "echo: 6883  loss: 0.32852\n",
      "echo: 6884  loss: 0.32852\n",
      "echo: 6885  loss: 0.32852\n",
      "echo: 6886  loss: 0.32852\n",
      "echo: 6887  loss: 0.32852\n",
      "echo: 6888  loss: 0.32852\n",
      "echo: 6889  loss: 0.32852\n",
      "echo: 6890  loss: 0.32852\n",
      "echo: 6891  loss: 0.32852\n",
      "echo: 6892  loss: 0.32852\n",
      "echo: 6893  loss: 0.32852\n",
      "echo: 6894  loss: 0.32852\n",
      "echo: 6895  loss: 0.32852\n",
      "echo: 6896  loss: 0.32852\n",
      "echo: 6897  loss: 0.32851\n",
      "echo: 6898  loss: 0.32851\n",
      "echo: 6899  loss: 0.32851\n",
      "echo: 6900  loss: 0.32851\n",
      "echo: 6901  loss: 0.32851\n",
      "echo: 6902  loss: 0.32851\n",
      "echo: 6903  loss: 0.32851\n",
      "echo: 6904  loss: 0.32851\n",
      "echo: 6905  loss: 0.32851\n",
      "echo: 6906  loss: 0.32851\n",
      "echo: 6907  loss: 0.32851\n",
      "echo: 6908  loss: 0.32851\n",
      "echo: 6909  loss: 0.32851\n",
      "echo: 6910  loss: 0.32851\n",
      "echo: 6911  loss: 0.32851\n",
      "echo: 6912  loss: 0.32851\n",
      "echo: 6913  loss: 0.32851\n",
      "echo: 6914  loss: 0.32851\n",
      "echo: 6915  loss: 0.32851\n",
      "echo: 6916  loss: 0.32851\n",
      "echo: 6917  loss: 0.32851\n",
      "echo: 6918  loss: 0.32851\n",
      "echo: 6919  loss: 0.32851\n",
      "echo: 6920  loss: 0.32851\n",
      "echo: 6921  loss: 0.32851\n",
      "echo: 6922  loss: 0.32851\n",
      "echo: 6923  loss: 0.32851\n",
      "echo: 6924  loss: 0.32851\n",
      "echo: 6925  loss: 0.32851\n",
      "echo: 6926  loss: 0.32851\n",
      "echo: 6927  loss: 0.32851\n",
      "echo: 6928  loss: 0.32851\n",
      "echo: 6929  loss: 0.32851\n",
      "echo: 6930  loss: 0.32851\n",
      "echo: 6931  loss: 0.32851\n",
      "echo: 6932  loss: 0.32851\n",
      "echo: 6933  loss: 0.32851\n",
      "echo: 6934  loss: 0.32851\n",
      "echo: 6935  loss: 0.32851\n",
      "echo: 6936  loss: 0.32851\n",
      "echo: 6937  loss: 0.32851\n",
      "echo: 6938  loss: 0.32851\n",
      "echo: 6939  loss: 0.32851\n",
      "echo: 6940  loss: 0.32851\n",
      "echo: 6941  loss: 0.32851\n",
      "echo: 6942  loss: 0.32851\n",
      "echo: 6943  loss: 0.32851\n",
      "echo: 6944  loss: 0.32851\n",
      "echo: 6945  loss: 0.32851\n",
      "echo: 6946  loss: 0.32851\n",
      "echo: 6947  loss: 0.32851\n",
      "echo: 6948  loss: 0.32851\n",
      "echo: 6949  loss: 0.32851\n",
      "echo: 6950  loss: 0.32851\n",
      "echo: 6951  loss: 0.32851\n",
      "echo: 6952  loss: 0.32851\n",
      "echo: 6953  loss: 0.32851\n",
      "echo: 6954  loss: 0.32851\n",
      "echo: 6955  loss: 0.32851\n",
      "echo: 6956  loss: 0.32851\n",
      "echo: 6957  loss: 0.32851\n",
      "echo: 6958  loss: 0.32851\n",
      "echo: 6959  loss: 0.32851\n",
      "echo: 6960  loss: 0.32851\n",
      "echo: 6961  loss: 0.32851\n",
      "echo: 6962  loss: 0.32851\n",
      "echo: 6963  loss: 0.32851\n",
      "echo: 6964  loss: 0.32851\n",
      "echo: 6965  loss: 0.32851\n",
      "echo: 6966  loss: 0.32851\n",
      "echo: 6967  loss: 0.32851\n",
      "echo: 6968  loss: 0.32851\n",
      "echo: 6969  loss: 0.32851\n",
      "echo: 6970  loss: 0.32851\n",
      "echo: 6971  loss: 0.32851\n",
      "echo: 6972  loss: 0.3285\n",
      "echo: 6973  loss: 0.3285\n",
      "echo: 6974  loss: 0.3285\n",
      "echo: 6975  loss: 0.3285\n",
      "echo: 6976  loss: 0.3285\n",
      "echo: 6977  loss: 0.3285\n",
      "echo: 6978  loss: 0.3285\n",
      "echo: 6979  loss: 0.3285\n",
      "echo: 6980  loss: 0.3285\n",
      "echo: 6981  loss: 0.3285\n",
      "echo: 6982  loss: 0.3285\n",
      "echo: 6983  loss: 0.3285\n",
      "echo: 6984  loss: 0.3285\n",
      "echo: 6985  loss: 0.3285\n",
      "echo: 6986  loss: 0.3285\n",
      "echo: 6987  loss: 0.3285\n",
      "echo: 6988  loss: 0.3285\n",
      "echo: 6989  loss: 0.3285\n",
      "echo: 6990  loss: 0.3285\n",
      "echo: 6991  loss: 0.3285\n",
      "echo: 6992  loss: 0.3285\n",
      "echo: 6993  loss: 0.3285\n",
      "echo: 6994  loss: 0.3285\n",
      "echo: 6995  loss: 0.3285\n",
      "echo: 6996  loss: 0.3285\n",
      "echo: 6997  loss: 0.3285\n",
      "echo: 6998  loss: 0.3285\n",
      "echo: 6999  loss: 0.3285\n",
      "echo: 7000  loss: 0.3285\n",
      "echo: 7001  loss: 0.3285\n",
      "echo: 7002  loss: 0.3285\n",
      "echo: 7003  loss: 0.3285\n",
      "echo: 7004  loss: 0.3285\n",
      "echo: 7005  loss: 0.3285\n",
      "echo: 7006  loss: 0.3285\n",
      "echo: 7007  loss: 0.3285\n",
      "echo: 7008  loss: 0.3285\n",
      "echo: 7009  loss: 0.3285\n",
      "echo: 7010  loss: 0.3285\n",
      "echo: 7011  loss: 0.3285\n",
      "echo: 7012  loss: 0.3285\n",
      "echo: 7013  loss: 0.3285\n",
      "echo: 7014  loss: 0.3285\n",
      "echo: 7015  loss: 0.3285\n",
      "echo: 7016  loss: 0.3285\n",
      "echo: 7017  loss: 0.3285\n",
      "echo: 7018  loss: 0.3285\n",
      "echo: 7019  loss: 0.3285\n",
      "echo: 7020  loss: 0.3285\n",
      "echo: 7021  loss: 0.3285\n",
      "echo: 7022  loss: 0.3285\n",
      "echo: 7023  loss: 0.3285\n",
      "echo: 7024  loss: 0.3285\n",
      "echo: 7025  loss: 0.3285\n",
      "echo: 7026  loss: 0.3285\n",
      "echo: 7027  loss: 0.3285\n",
      "echo: 7028  loss: 0.3285\n",
      "echo: 7029  loss: 0.3285\n",
      "echo: 7030  loss: 0.3285\n",
      "echo: 7031  loss: 0.3285\n",
      "echo: 7032  loss: 0.3285\n",
      "echo: 7033  loss: 0.3285\n",
      "echo: 7034  loss: 0.3285\n",
      "echo: 7035  loss: 0.3285\n",
      "echo: 7036  loss: 0.3285\n",
      "echo: 7037  loss: 0.3285\n",
      "echo: 7038  loss: 0.3285\n",
      "echo: 7039  loss: 0.3285\n",
      "echo: 7040  loss: 0.3285\n",
      "echo: 7041  loss: 0.3285\n",
      "echo: 7042  loss: 0.3285\n",
      "echo: 7043  loss: 0.3285\n",
      "echo: 7044  loss: 0.3285\n",
      "echo: 7045  loss: 0.3285\n",
      "echo: 7046  loss: 0.3285\n",
      "echo: 7047  loss: 0.3285\n",
      "echo: 7048  loss: 0.3285\n",
      "echo: 7049  loss: 0.3285\n",
      "echo: 7050  loss: 0.3285\n",
      "echo: 7051  loss: 0.32849\n",
      "echo: 7052  loss: 0.32849\n",
      "echo: 7053  loss: 0.32849\n",
      "echo: 7054  loss: 0.32849\n",
      "echo: 7055  loss: 0.32849\n",
      "echo: 7056  loss: 0.32849\n",
      "echo: 7057  loss: 0.32849\n",
      "echo: 7058  loss: 0.32849\n",
      "echo: 7059  loss: 0.32849\n",
      "echo: 7060  loss: 0.32849\n",
      "echo: 7061  loss: 0.32849\n",
      "echo: 7062  loss: 0.32849\n",
      "echo: 7063  loss: 0.32849\n",
      "echo: 7064  loss: 0.32849\n",
      "echo: 7065  loss: 0.32849\n",
      "echo: 7066  loss: 0.32849\n",
      "echo: 7067  loss: 0.32849\n",
      "echo: 7068  loss: 0.32849\n",
      "echo: 7069  loss: 0.32849\n",
      "echo: 7070  loss: 0.32849\n",
      "echo: 7071  loss: 0.32849\n",
      "echo: 7072  loss: 0.32849\n",
      "echo: 7073  loss: 0.32849\n",
      "echo: 7074  loss: 0.32849\n",
      "echo: 7075  loss: 0.32849\n",
      "echo: 7076  loss: 0.32849\n",
      "echo: 7077  loss: 0.32849\n",
      "echo: 7078  loss: 0.32849\n",
      "echo: 7079  loss: 0.32849\n",
      "echo: 7080  loss: 0.32849\n",
      "echo: 7081  loss: 0.32849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 7082  loss: 0.32849\n",
      "echo: 7083  loss: 0.32849\n",
      "echo: 7084  loss: 0.32849\n",
      "echo: 7085  loss: 0.32849\n",
      "echo: 7086  loss: 0.32849\n",
      "echo: 7087  loss: 0.32849\n",
      "echo: 7088  loss: 0.32849\n",
      "echo: 7089  loss: 0.32849\n",
      "echo: 7090  loss: 0.32849\n",
      "echo: 7091  loss: 0.32849\n",
      "echo: 7092  loss: 0.32849\n",
      "echo: 7093  loss: 0.32849\n",
      "echo: 7094  loss: 0.32849\n",
      "echo: 7095  loss: 0.32849\n",
      "echo: 7096  loss: 0.32849\n",
      "echo: 7097  loss: 0.32849\n",
      "echo: 7098  loss: 0.32849\n",
      "echo: 7099  loss: 0.32849\n",
      "echo: 7100  loss: 0.32849\n",
      "echo: 7101  loss: 0.32849\n",
      "echo: 7102  loss: 0.32849\n",
      "echo: 7103  loss: 0.32849\n",
      "echo: 7104  loss: 0.32849\n",
      "echo: 7105  loss: 0.32849\n",
      "echo: 7106  loss: 0.32849\n",
      "echo: 7107  loss: 0.32849\n",
      "echo: 7108  loss: 0.32849\n",
      "echo: 7109  loss: 0.32849\n",
      "echo: 7110  loss: 0.32849\n",
      "echo: 7111  loss: 0.32849\n",
      "echo: 7112  loss: 0.32849\n",
      "echo: 7113  loss: 0.32849\n",
      "echo: 7114  loss: 0.32849\n",
      "echo: 7115  loss: 0.32849\n",
      "echo: 7116  loss: 0.32849\n",
      "echo: 7117  loss: 0.32849\n",
      "echo: 7118  loss: 0.32849\n",
      "echo: 7119  loss: 0.32849\n",
      "echo: 7120  loss: 0.32849\n",
      "echo: 7121  loss: 0.32849\n",
      "echo: 7122  loss: 0.32849\n",
      "echo: 7123  loss: 0.32849\n",
      "echo: 7124  loss: 0.32849\n",
      "echo: 7125  loss: 0.32849\n",
      "echo: 7126  loss: 0.32849\n",
      "echo: 7127  loss: 0.32849\n",
      "echo: 7128  loss: 0.32849\n",
      "echo: 7129  loss: 0.32849\n",
      "echo: 7130  loss: 0.32849\n",
      "echo: 7131  loss: 0.32849\n",
      "echo: 7132  loss: 0.32849\n",
      "echo: 7133  loss: 0.32848\n",
      "echo: 7134  loss: 0.32848\n",
      "echo: 7135  loss: 0.32848\n",
      "echo: 7136  loss: 0.32848\n",
      "echo: 7137  loss: 0.32848\n",
      "echo: 7138  loss: 0.32848\n",
      "echo: 7139  loss: 0.32848\n",
      "echo: 7140  loss: 0.32848\n",
      "echo: 7141  loss: 0.32848\n",
      "echo: 7142  loss: 0.32848\n",
      "echo: 7143  loss: 0.32848\n",
      "echo: 7144  loss: 0.32848\n",
      "echo: 7145  loss: 0.32848\n",
      "echo: 7146  loss: 0.32848\n",
      "echo: 7147  loss: 0.32848\n",
      "echo: 7148  loss: 0.32848\n",
      "echo: 7149  loss: 0.32848\n",
      "echo: 7150  loss: 0.32848\n",
      "echo: 7151  loss: 0.32848\n",
      "echo: 7152  loss: 0.32848\n",
      "echo: 7153  loss: 0.32848\n",
      "echo: 7154  loss: 0.32848\n",
      "echo: 7155  loss: 0.32848\n",
      "echo: 7156  loss: 0.32848\n",
      "echo: 7157  loss: 0.32848\n",
      "echo: 7158  loss: 0.32848\n",
      "echo: 7159  loss: 0.32848\n",
      "echo: 7160  loss: 0.32848\n",
      "echo: 7161  loss: 0.32848\n",
      "echo: 7162  loss: 0.32848\n",
      "echo: 7163  loss: 0.32848\n",
      "echo: 7164  loss: 0.32848\n",
      "echo: 7165  loss: 0.32848\n",
      "echo: 7166  loss: 0.32848\n",
      "echo: 7167  loss: 0.32848\n",
      "echo: 7168  loss: 0.32848\n",
      "echo: 7169  loss: 0.32848\n",
      "echo: 7170  loss: 0.32848\n",
      "echo: 7171  loss: 0.32848\n",
      "echo: 7172  loss: 0.32848\n",
      "echo: 7173  loss: 0.32848\n",
      "echo: 7174  loss: 0.32848\n",
      "echo: 7175  loss: 0.32848\n",
      "echo: 7176  loss: 0.32848\n",
      "echo: 7177  loss: 0.32848\n",
      "echo: 7178  loss: 0.32848\n",
      "echo: 7179  loss: 0.32848\n",
      "echo: 7180  loss: 0.32848\n",
      "echo: 7181  loss: 0.32848\n",
      "echo: 7182  loss: 0.32848\n",
      "echo: 7183  loss: 0.32848\n",
      "echo: 7184  loss: 0.32848\n",
      "echo: 7185  loss: 0.32848\n",
      "echo: 7186  loss: 0.32848\n",
      "echo: 7187  loss: 0.32848\n",
      "echo: 7188  loss: 0.32848\n",
      "echo: 7189  loss: 0.32848\n",
      "echo: 7190  loss: 0.32848\n",
      "echo: 7191  loss: 0.32848\n",
      "echo: 7192  loss: 0.32848\n",
      "echo: 7193  loss: 0.32848\n",
      "echo: 7194  loss: 0.32848\n",
      "echo: 7195  loss: 0.32848\n",
      "echo: 7196  loss: 0.32848\n",
      "echo: 7197  loss: 0.32848\n",
      "echo: 7198  loss: 0.32848\n",
      "echo: 7199  loss: 0.32848\n",
      "echo: 7200  loss: 0.32848\n",
      "echo: 7201  loss: 0.32848\n",
      "echo: 7202  loss: 0.32848\n",
      "echo: 7203  loss: 0.32848\n",
      "echo: 7204  loss: 0.32848\n",
      "echo: 7205  loss: 0.32848\n",
      "echo: 7206  loss: 0.32848\n",
      "echo: 7207  loss: 0.32848\n",
      "echo: 7208  loss: 0.32848\n",
      "echo: 7209  loss: 0.32848\n",
      "echo: 7210  loss: 0.32848\n",
      "echo: 7211  loss: 0.32848\n",
      "echo: 7212  loss: 0.32848\n",
      "echo: 7213  loss: 0.32848\n",
      "echo: 7214  loss: 0.32848\n",
      "echo: 7215  loss: 0.32848\n",
      "echo: 7216  loss: 0.32848\n",
      "echo: 7217  loss: 0.32848\n",
      "echo: 7218  loss: 0.32848\n",
      "echo: 7219  loss: 0.32847\n",
      "echo: 7220  loss: 0.32847\n",
      "echo: 7221  loss: 0.32847\n",
      "echo: 7222  loss: 0.32847\n",
      "echo: 7223  loss: 0.32847\n",
      "echo: 7224  loss: 0.32847\n",
      "echo: 7225  loss: 0.32847\n",
      "echo: 7226  loss: 0.32847\n",
      "echo: 7227  loss: 0.32847\n",
      "echo: 7228  loss: 0.32847\n",
      "echo: 7229  loss: 0.32847\n",
      "echo: 7230  loss: 0.32847\n",
      "echo: 7231  loss: 0.32847\n",
      "echo: 7232  loss: 0.32847\n",
      "echo: 7233  loss: 0.32847\n",
      "echo: 7234  loss: 0.32847\n",
      "echo: 7235  loss: 0.32847\n",
      "echo: 7236  loss: 0.32847\n",
      "echo: 7237  loss: 0.32847\n",
      "echo: 7238  loss: 0.32847\n",
      "echo: 7239  loss: 0.32847\n",
      "echo: 7240  loss: 0.32847\n",
      "echo: 7241  loss: 0.32847\n",
      "echo: 7242  loss: 0.32847\n",
      "echo: 7243  loss: 0.32847\n",
      "echo: 7244  loss: 0.32847\n",
      "echo: 7245  loss: 0.32847\n",
      "echo: 7246  loss: 0.32847\n",
      "echo: 7247  loss: 0.32847\n",
      "echo: 7248  loss: 0.32847\n",
      "echo: 7249  loss: 0.32847\n",
      "echo: 7250  loss: 0.32847\n",
      "echo: 7251  loss: 0.32847\n",
      "echo: 7252  loss: 0.32847\n",
      "echo: 7253  loss: 0.32847\n",
      "echo: 7254  loss: 0.32847\n",
      "echo: 7255  loss: 0.32847\n",
      "echo: 7256  loss: 0.32847\n",
      "echo: 7257  loss: 0.32847\n",
      "echo: 7258  loss: 0.32847\n",
      "echo: 7259  loss: 0.32847\n",
      "echo: 7260  loss: 0.32847\n",
      "echo: 7261  loss: 0.32847\n",
      "echo: 7262  loss: 0.32847\n",
      "echo: 7263  loss: 0.32847\n",
      "echo: 7264  loss: 0.32847\n",
      "echo: 7265  loss: 0.32847\n",
      "echo: 7266  loss: 0.32847\n",
      "echo: 7267  loss: 0.32847\n",
      "echo: 7268  loss: 0.32847\n",
      "echo: 7269  loss: 0.32847\n",
      "echo: 7270  loss: 0.32847\n",
      "echo: 7271  loss: 0.32847\n",
      "echo: 7272  loss: 0.32847\n",
      "echo: 7273  loss: 0.32847\n",
      "echo: 7274  loss: 0.32847\n",
      "echo: 7275  loss: 0.32847\n",
      "echo: 7276  loss: 0.32847\n",
      "echo: 7277  loss: 0.32847\n",
      "echo: 7278  loss: 0.32847\n",
      "echo: 7279  loss: 0.32847\n",
      "echo: 7280  loss: 0.32847\n",
      "echo: 7281  loss: 0.32847\n",
      "echo: 7282  loss: 0.32847\n",
      "echo: 7283  loss: 0.32847\n",
      "echo: 7284  loss: 0.32847\n",
      "echo: 7285  loss: 0.32847\n",
      "echo: 7286  loss: 0.32847\n",
      "echo: 7287  loss: 0.32847\n",
      "echo: 7288  loss: 0.32847\n",
      "echo: 7289  loss: 0.32847\n",
      "echo: 7290  loss: 0.32847\n",
      "echo: 7291  loss: 0.32847\n",
      "echo: 7292  loss: 0.32847\n",
      "echo: 7293  loss: 0.32847\n",
      "echo: 7294  loss: 0.32847\n",
      "echo: 7295  loss: 0.32847\n",
      "echo: 7296  loss: 0.32847\n",
      "echo: 7297  loss: 0.32847\n",
      "echo: 7298  loss: 0.32847\n",
      "echo: 7299  loss: 0.32847\n",
      "echo: 7300  loss: 0.32847\n",
      "echo: 7301  loss: 0.32847\n",
      "echo: 7302  loss: 0.32847\n",
      "echo: 7303  loss: 0.32847\n",
      "echo: 7304  loss: 0.32847\n",
      "echo: 7305  loss: 0.32847\n",
      "echo: 7306  loss: 0.32847\n",
      "echo: 7307  loss: 0.32847\n",
      "echo: 7308  loss: 0.32847\n",
      "echo: 7309  loss: 0.32847\n",
      "echo: 7310  loss: 0.32846\n",
      "echo: 7311  loss: 0.32846\n",
      "echo: 7312  loss: 0.32846\n",
      "echo: 7313  loss: 0.32846\n",
      "echo: 7314  loss: 0.32846\n",
      "echo: 7315  loss: 0.32846\n",
      "echo: 7316  loss: 0.32846\n",
      "echo: 7317  loss: 0.32846\n",
      "echo: 7318  loss: 0.32846\n",
      "echo: 7319  loss: 0.32846\n",
      "echo: 7320  loss: 0.32846\n",
      "echo: 7321  loss: 0.32846\n",
      "echo: 7322  loss: 0.32846\n",
      "echo: 7323  loss: 0.32846\n",
      "echo: 7324  loss: 0.32846\n",
      "echo: 7325  loss: 0.32846\n",
      "echo: 7326  loss: 0.32846\n",
      "echo: 7327  loss: 0.32846\n",
      "echo: 7328  loss: 0.32846\n",
      "echo: 7329  loss: 0.32846\n",
      "echo: 7330  loss: 0.32846\n",
      "echo: 7331  loss: 0.32846\n",
      "echo: 7332  loss: 0.32846\n",
      "echo: 7333  loss: 0.32846\n",
      "echo: 7334  loss: 0.32846\n",
      "echo: 7335  loss: 0.32846\n",
      "echo: 7336  loss: 0.32846\n",
      "echo: 7337  loss: 0.32846\n",
      "echo: 7338  loss: 0.32846\n",
      "echo: 7339  loss: 0.32846\n",
      "echo: 7340  loss: 0.32846\n",
      "echo: 7341  loss: 0.32846\n",
      "echo: 7342  loss: 0.32846\n",
      "echo: 7343  loss: 0.32846\n",
      "echo: 7344  loss: 0.32846\n",
      "echo: 7345  loss: 0.32846\n",
      "echo: 7346  loss: 0.32846\n",
      "echo: 7347  loss: 0.32846\n",
      "echo: 7348  loss: 0.32846\n",
      "echo: 7349  loss: 0.32846\n",
      "echo: 7350  loss: 0.32846\n",
      "echo: 7351  loss: 0.32846\n",
      "echo: 7352  loss: 0.32846\n",
      "echo: 7353  loss: 0.32846\n",
      "echo: 7354  loss: 0.32846\n",
      "echo: 7355  loss: 0.32846\n",
      "echo: 7356  loss: 0.32846\n",
      "echo: 7357  loss: 0.32846\n",
      "echo: 7358  loss: 0.32846\n",
      "echo: 7359  loss: 0.32846\n",
      "echo: 7360  loss: 0.32846\n",
      "echo: 7361  loss: 0.32846\n",
      "echo: 7362  loss: 0.32846\n",
      "echo: 7363  loss: 0.32846\n",
      "echo: 7364  loss: 0.32846\n",
      "echo: 7365  loss: 0.32846\n",
      "echo: 7366  loss: 0.32846\n",
      "echo: 7367  loss: 0.32846\n",
      "echo: 7368  loss: 0.32846\n",
      "echo: 7369  loss: 0.32846\n",
      "echo: 7370  loss: 0.32846\n",
      "echo: 7371  loss: 0.32846\n",
      "echo: 7372  loss: 0.32846\n",
      "echo: 7373  loss: 0.32846\n",
      "echo: 7374  loss: 0.32846\n",
      "echo: 7375  loss: 0.32846\n",
      "echo: 7376  loss: 0.32846\n",
      "echo: 7377  loss: 0.32846\n",
      "echo: 7378  loss: 0.32846\n",
      "echo: 7379  loss: 0.32846\n",
      "echo: 7380  loss: 0.32846\n",
      "echo: 7381  loss: 0.32846\n",
      "echo: 7382  loss: 0.32846\n",
      "echo: 7383  loss: 0.32846\n",
      "echo: 7384  loss: 0.32846\n",
      "echo: 7385  loss: 0.32846\n",
      "echo: 7386  loss: 0.32846\n",
      "echo: 7387  loss: 0.32846\n",
      "echo: 7388  loss: 0.32846\n",
      "echo: 7389  loss: 0.32846\n",
      "echo: 7390  loss: 0.32846\n",
      "echo: 7391  loss: 0.32846\n",
      "echo: 7392  loss: 0.32846\n",
      "echo: 7393  loss: 0.32846\n",
      "echo: 7394  loss: 0.32846\n",
      "echo: 7395  loss: 0.32846\n",
      "echo: 7396  loss: 0.32846\n",
      "echo: 7397  loss: 0.32846\n",
      "echo: 7398  loss: 0.32846\n",
      "echo: 7399  loss: 0.32846\n",
      "echo: 7400  loss: 0.32846\n",
      "echo: 7401  loss: 0.32846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 7402  loss: 0.32846\n",
      "echo: 7403  loss: 0.32846\n",
      "echo: 7404  loss: 0.32846\n",
      "echo: 7405  loss: 0.32846\n",
      "echo: 7406  loss: 0.32845\n",
      "echo: 7407  loss: 0.32845\n",
      "echo: 7408  loss: 0.32845\n",
      "echo: 7409  loss: 0.32845\n",
      "echo: 7410  loss: 0.32845\n",
      "echo: 7411  loss: 0.32845\n",
      "echo: 7412  loss: 0.32845\n",
      "echo: 7413  loss: 0.32845\n",
      "echo: 7414  loss: 0.32845\n",
      "echo: 7415  loss: 0.32845\n",
      "echo: 7416  loss: 0.32845\n",
      "echo: 7417  loss: 0.32845\n",
      "echo: 7418  loss: 0.32845\n",
      "echo: 7419  loss: 0.32845\n",
      "echo: 7420  loss: 0.32845\n",
      "echo: 7421  loss: 0.32845\n",
      "echo: 7422  loss: 0.32845\n",
      "echo: 7423  loss: 0.32845\n",
      "echo: 7424  loss: 0.32845\n",
      "echo: 7425  loss: 0.32845\n",
      "echo: 7426  loss: 0.32845\n",
      "echo: 7427  loss: 0.32845\n",
      "echo: 7428  loss: 0.32845\n",
      "echo: 7429  loss: 0.32845\n",
      "echo: 7430  loss: 0.32845\n",
      "echo: 7431  loss: 0.32845\n",
      "echo: 7432  loss: 0.32845\n",
      "echo: 7433  loss: 0.32845\n",
      "echo: 7434  loss: 0.32845\n",
      "echo: 7435  loss: 0.32845\n",
      "echo: 7436  loss: 0.32845\n",
      "echo: 7437  loss: 0.32845\n",
      "echo: 7438  loss: 0.32845\n",
      "echo: 7439  loss: 0.32845\n",
      "echo: 7440  loss: 0.32845\n",
      "echo: 7441  loss: 0.32845\n",
      "echo: 7442  loss: 0.32845\n",
      "echo: 7443  loss: 0.32845\n",
      "echo: 7444  loss: 0.32845\n",
      "echo: 7445  loss: 0.32845\n",
      "echo: 7446  loss: 0.32845\n",
      "echo: 7447  loss: 0.32845\n",
      "echo: 7448  loss: 0.32845\n",
      "echo: 7449  loss: 0.32845\n",
      "echo: 7450  loss: 0.32845\n",
      "echo: 7451  loss: 0.32845\n",
      "echo: 7452  loss: 0.32845\n",
      "echo: 7453  loss: 0.32845\n",
      "echo: 7454  loss: 0.32845\n",
      "echo: 7455  loss: 0.32845\n",
      "echo: 7456  loss: 0.32845\n",
      "echo: 7457  loss: 0.32845\n",
      "echo: 7458  loss: 0.32845\n",
      "echo: 7459  loss: 0.32845\n",
      "echo: 7460  loss: 0.32845\n",
      "echo: 7461  loss: 0.32845\n",
      "echo: 7462  loss: 0.32845\n",
      "echo: 7463  loss: 0.32845\n",
      "echo: 7464  loss: 0.32845\n",
      "echo: 7465  loss: 0.32845\n",
      "echo: 7466  loss: 0.32845\n",
      "echo: 7467  loss: 0.32845\n",
      "echo: 7468  loss: 0.32845\n",
      "echo: 7469  loss: 0.32845\n",
      "echo: 7470  loss: 0.32845\n",
      "echo: 7471  loss: 0.32845\n",
      "echo: 7472  loss: 0.32845\n",
      "echo: 7473  loss: 0.32845\n",
      "echo: 7474  loss: 0.32845\n",
      "echo: 7475  loss: 0.32845\n",
      "echo: 7476  loss: 0.32845\n",
      "echo: 7477  loss: 0.32845\n",
      "echo: 7478  loss: 0.32845\n",
      "echo: 7479  loss: 0.32845\n",
      "echo: 7480  loss: 0.32845\n",
      "echo: 7481  loss: 0.32845\n",
      "echo: 7482  loss: 0.32845\n",
      "echo: 7483  loss: 0.32845\n",
      "echo: 7484  loss: 0.32845\n",
      "echo: 7485  loss: 0.32845\n",
      "echo: 7486  loss: 0.32845\n",
      "echo: 7487  loss: 0.32845\n",
      "echo: 7488  loss: 0.32845\n",
      "echo: 7489  loss: 0.32845\n",
      "echo: 7490  loss: 0.32845\n",
      "echo: 7491  loss: 0.32845\n",
      "echo: 7492  loss: 0.32845\n",
      "echo: 7493  loss: 0.32845\n",
      "echo: 7494  loss: 0.32845\n",
      "echo: 7495  loss: 0.32845\n",
      "echo: 7496  loss: 0.32845\n",
      "echo: 7497  loss: 0.32845\n",
      "echo: 7498  loss: 0.32845\n",
      "echo: 7499  loss: 0.32845\n",
      "echo: 7500  loss: 0.32845\n",
      "echo: 7501  loss: 0.32845\n",
      "echo: 7502  loss: 0.32845\n",
      "echo: 7503  loss: 0.32845\n",
      "echo: 7504  loss: 0.32845\n",
      "echo: 7505  loss: 0.32845\n",
      "echo: 7506  loss: 0.32845\n",
      "echo: 7507  loss: 0.32844\n",
      "echo: 7508  loss: 0.32844\n",
      "echo: 7509  loss: 0.32844\n",
      "echo: 7510  loss: 0.32844\n",
      "echo: 7511  loss: 0.32844\n",
      "echo: 7512  loss: 0.32844\n",
      "echo: 7513  loss: 0.32844\n",
      "echo: 7514  loss: 0.32844\n",
      "echo: 7515  loss: 0.32844\n",
      "echo: 7516  loss: 0.32844\n",
      "echo: 7517  loss: 0.32844\n",
      "echo: 7518  loss: 0.32844\n",
      "echo: 7519  loss: 0.32844\n",
      "echo: 7520  loss: 0.32844\n",
      "echo: 7521  loss: 0.32844\n",
      "echo: 7522  loss: 0.32844\n",
      "echo: 7523  loss: 0.32844\n",
      "echo: 7524  loss: 0.32844\n",
      "echo: 7525  loss: 0.32844\n",
      "echo: 7526  loss: 0.32844\n",
      "echo: 7527  loss: 0.32844\n",
      "echo: 7528  loss: 0.32844\n",
      "echo: 7529  loss: 0.32844\n",
      "echo: 7530  loss: 0.32844\n",
      "echo: 7531  loss: 0.32844\n",
      "echo: 7532  loss: 0.32844\n",
      "echo: 7533  loss: 0.32844\n",
      "echo: 7534  loss: 0.32844\n",
      "echo: 7535  loss: 0.32844\n",
      "echo: 7536  loss: 0.32844\n",
      "echo: 7537  loss: 0.32844\n",
      "echo: 7538  loss: 0.32844\n",
      "echo: 7539  loss: 0.32844\n",
      "echo: 7540  loss: 0.32844\n",
      "echo: 7541  loss: 0.32844\n",
      "echo: 7542  loss: 0.32844\n",
      "echo: 7543  loss: 0.32844\n",
      "echo: 7544  loss: 0.32844\n",
      "echo: 7545  loss: 0.32844\n",
      "echo: 7546  loss: 0.32844\n",
      "echo: 7547  loss: 0.32844\n",
      "echo: 7548  loss: 0.32844\n",
      "echo: 7549  loss: 0.32844\n",
      "echo: 7550  loss: 0.32844\n",
      "echo: 7551  loss: 0.32844\n",
      "echo: 7552  loss: 0.32844\n",
      "echo: 7553  loss: 0.32844\n",
      "echo: 7554  loss: 0.32844\n",
      "echo: 7555  loss: 0.32844\n",
      "echo: 7556  loss: 0.32844\n",
      "echo: 7557  loss: 0.32844\n",
      "echo: 7558  loss: 0.32844\n",
      "echo: 7559  loss: 0.32844\n",
      "echo: 7560  loss: 0.32844\n",
      "echo: 7561  loss: 0.32844\n",
      "echo: 7562  loss: 0.32844\n",
      "echo: 7563  loss: 0.32844\n",
      "echo: 7564  loss: 0.32844\n",
      "echo: 7565  loss: 0.32844\n",
      "echo: 7566  loss: 0.32844\n",
      "echo: 7567  loss: 0.32844\n",
      "echo: 7568  loss: 0.32844\n",
      "echo: 7569  loss: 0.32844\n",
      "echo: 7570  loss: 0.32844\n",
      "echo: 7571  loss: 0.32844\n",
      "echo: 7572  loss: 0.32844\n",
      "echo: 7573  loss: 0.32844\n",
      "echo: 7574  loss: 0.32844\n",
      "echo: 7575  loss: 0.32844\n",
      "echo: 7576  loss: 0.32844\n",
      "echo: 7577  loss: 0.32844\n",
      "echo: 7578  loss: 0.32844\n",
      "echo: 7579  loss: 0.32844\n",
      "echo: 7580  loss: 0.32844\n",
      "echo: 7581  loss: 0.32844\n",
      "echo: 7582  loss: 0.32844\n",
      "echo: 7583  loss: 0.32844\n",
      "echo: 7584  loss: 0.32844\n",
      "echo: 7585  loss: 0.32844\n",
      "echo: 7586  loss: 0.32844\n",
      "echo: 7587  loss: 0.32844\n",
      "echo: 7588  loss: 0.32844\n",
      "echo: 7589  loss: 0.32844\n",
      "echo: 7590  loss: 0.32844\n",
      "echo: 7591  loss: 0.32844\n",
      "echo: 7592  loss: 0.32844\n",
      "echo: 7593  loss: 0.32844\n",
      "echo: 7594  loss: 0.32844\n",
      "echo: 7595  loss: 0.32844\n",
      "echo: 7596  loss: 0.32844\n",
      "echo: 7597  loss: 0.32844\n",
      "echo: 7598  loss: 0.32844\n",
      "echo: 7599  loss: 0.32844\n",
      "echo: 7600  loss: 0.32844\n",
      "echo: 7601  loss: 0.32844\n",
      "echo: 7602  loss: 0.32844\n",
      "echo: 7603  loss: 0.32844\n",
      "echo: 7604  loss: 0.32844\n",
      "echo: 7605  loss: 0.32844\n",
      "echo: 7606  loss: 0.32844\n",
      "echo: 7607  loss: 0.32844\n",
      "echo: 7608  loss: 0.32844\n",
      "echo: 7609  loss: 0.32844\n",
      "echo: 7610  loss: 0.32844\n",
      "echo: 7611  loss: 0.32844\n",
      "echo: 7612  loss: 0.32844\n",
      "echo: 7613  loss: 0.32844\n",
      "echo: 7614  loss: 0.32843\n",
      "echo: 7615  loss: 0.32843\n",
      "echo: 7616  loss: 0.32843\n",
      "echo: 7617  loss: 0.32843\n",
      "echo: 7618  loss: 0.32843\n",
      "echo: 7619  loss: 0.32843\n",
      "echo: 7620  loss: 0.32843\n",
      "echo: 7621  loss: 0.32843\n",
      "echo: 7622  loss: 0.32843\n",
      "echo: 7623  loss: 0.32843\n",
      "echo: 7624  loss: 0.32843\n",
      "echo: 7625  loss: 0.32843\n",
      "echo: 7626  loss: 0.32843\n",
      "echo: 7627  loss: 0.32843\n",
      "echo: 7628  loss: 0.32843\n",
      "echo: 7629  loss: 0.32843\n",
      "echo: 7630  loss: 0.32843\n",
      "echo: 7631  loss: 0.32843\n",
      "echo: 7632  loss: 0.32843\n",
      "echo: 7633  loss: 0.32843\n",
      "echo: 7634  loss: 0.32843\n",
      "echo: 7635  loss: 0.32843\n",
      "echo: 7636  loss: 0.32843\n",
      "echo: 7637  loss: 0.32843\n",
      "echo: 7638  loss: 0.32843\n",
      "echo: 7639  loss: 0.32843\n",
      "echo: 7640  loss: 0.32843\n",
      "echo: 7641  loss: 0.32843\n",
      "echo: 7642  loss: 0.32843\n",
      "echo: 7643  loss: 0.32843\n",
      "echo: 7644  loss: 0.32843\n",
      "echo: 7645  loss: 0.32843\n",
      "echo: 7646  loss: 0.32843\n",
      "echo: 7647  loss: 0.32843\n",
      "echo: 7648  loss: 0.32843\n",
      "echo: 7649  loss: 0.32843\n",
      "echo: 7650  loss: 0.32843\n",
      "echo: 7651  loss: 0.32843\n",
      "echo: 7652  loss: 0.32843\n",
      "echo: 7653  loss: 0.32843\n",
      "echo: 7654  loss: 0.32843\n",
      "echo: 7655  loss: 0.32843\n",
      "echo: 7656  loss: 0.32843\n",
      "echo: 7657  loss: 0.32843\n",
      "echo: 7658  loss: 0.32843\n",
      "echo: 7659  loss: 0.32843\n",
      "echo: 7660  loss: 0.32843\n",
      "echo: 7661  loss: 0.32843\n",
      "echo: 7662  loss: 0.32843\n",
      "echo: 7663  loss: 0.32843\n",
      "echo: 7664  loss: 0.32843\n",
      "echo: 7665  loss: 0.32843\n",
      "echo: 7666  loss: 0.32843\n",
      "echo: 7667  loss: 0.32843\n",
      "echo: 7668  loss: 0.32843\n",
      "echo: 7669  loss: 0.32843\n",
      "echo: 7670  loss: 0.32843\n",
      "echo: 7671  loss: 0.32843\n",
      "echo: 7672  loss: 0.32843\n",
      "echo: 7673  loss: 0.32843\n",
      "echo: 7674  loss: 0.32843\n",
      "echo: 7675  loss: 0.32843\n",
      "echo: 7676  loss: 0.32843\n",
      "echo: 7677  loss: 0.32843\n",
      "echo: 7678  loss: 0.32843\n",
      "echo: 7679  loss: 0.32843\n",
      "echo: 7680  loss: 0.32843\n",
      "echo: 7681  loss: 0.32843\n",
      "echo: 7682  loss: 0.32843\n",
      "echo: 7683  loss: 0.32843\n",
      "echo: 7684  loss: 0.32843\n",
      "echo: 7685  loss: 0.32843\n",
      "echo: 7686  loss: 0.32843\n",
      "echo: 7687  loss: 0.32843\n",
      "echo: 7688  loss: 0.32843\n",
      "echo: 7689  loss: 0.32843\n",
      "echo: 7690  loss: 0.32843\n",
      "echo: 7691  loss: 0.32843\n",
      "echo: 7692  loss: 0.32843\n",
      "echo: 7693  loss: 0.32843\n",
      "echo: 7694  loss: 0.32843\n",
      "echo: 7695  loss: 0.32843\n",
      "echo: 7696  loss: 0.32843\n",
      "echo: 7697  loss: 0.32843\n",
      "echo: 7698  loss: 0.32843\n",
      "echo: 7699  loss: 0.32843\n",
      "echo: 7700  loss: 0.32843\n",
      "echo: 7701  loss: 0.32843\n",
      "echo: 7702  loss: 0.32843\n",
      "echo: 7703  loss: 0.32843\n",
      "echo: 7704  loss: 0.32843\n",
      "echo: 7705  loss: 0.32843\n",
      "echo: 7706  loss: 0.32843\n",
      "echo: 7707  loss: 0.32843\n",
      "echo: 7708  loss: 0.32843\n",
      "echo: 7709  loss: 0.32843\n",
      "echo: 7710  loss: 0.32843\n",
      "echo: 7711  loss: 0.32843\n",
      "echo: 7712  loss: 0.32843\n",
      "echo: 7713  loss: 0.32843\n",
      "echo: 7714  loss: 0.32843\n",
      "echo: 7715  loss: 0.32843\n",
      "echo: 7716  loss: 0.32843\n",
      "echo: 7717  loss: 0.32843\n",
      "echo: 7718  loss: 0.32843\n",
      "echo: 7719  loss: 0.32843\n",
      "echo: 7720  loss: 0.32843\n",
      "echo: 7721  loss: 0.32843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 7722  loss: 0.32843\n",
      "echo: 7723  loss: 0.32843\n",
      "echo: 7724  loss: 0.32843\n",
      "echo: 7725  loss: 0.32843\n",
      "echo: 7726  loss: 0.32843\n",
      "echo: 7727  loss: 0.32842\n",
      "echo: 7728  loss: 0.32842\n",
      "echo: 7729  loss: 0.32842\n",
      "echo: 7730  loss: 0.32842\n",
      "echo: 7731  loss: 0.32842\n",
      "echo: 7732  loss: 0.32842\n",
      "echo: 7733  loss: 0.32842\n",
      "echo: 7734  loss: 0.32842\n",
      "echo: 7735  loss: 0.32842\n",
      "echo: 7736  loss: 0.32842\n",
      "echo: 7737  loss: 0.32842\n",
      "echo: 7738  loss: 0.32842\n",
      "echo: 7739  loss: 0.32842\n",
      "echo: 7740  loss: 0.32842\n",
      "echo: 7741  loss: 0.32842\n",
      "echo: 7742  loss: 0.32842\n",
      "echo: 7743  loss: 0.32842\n",
      "echo: 7744  loss: 0.32842\n",
      "echo: 7745  loss: 0.32842\n",
      "echo: 7746  loss: 0.32842\n",
      "echo: 7747  loss: 0.32842\n",
      "echo: 7748  loss: 0.32842\n",
      "echo: 7749  loss: 0.32842\n",
      "echo: 7750  loss: 0.32842\n",
      "echo: 7751  loss: 0.32842\n",
      "echo: 7752  loss: 0.32842\n",
      "echo: 7753  loss: 0.32842\n",
      "echo: 7754  loss: 0.32842\n",
      "echo: 7755  loss: 0.32842\n",
      "echo: 7756  loss: 0.32842\n",
      "echo: 7757  loss: 0.32842\n",
      "echo: 7758  loss: 0.32842\n",
      "echo: 7759  loss: 0.32842\n",
      "echo: 7760  loss: 0.32842\n",
      "echo: 7761  loss: 0.32842\n",
      "echo: 7762  loss: 0.32842\n",
      "echo: 7763  loss: 0.32842\n",
      "echo: 7764  loss: 0.32842\n",
      "echo: 7765  loss: 0.32842\n",
      "echo: 7766  loss: 0.32842\n",
      "echo: 7767  loss: 0.32842\n",
      "echo: 7768  loss: 0.32842\n",
      "echo: 7769  loss: 0.32842\n",
      "echo: 7770  loss: 0.32842\n",
      "echo: 7771  loss: 0.32842\n",
      "echo: 7772  loss: 0.32842\n",
      "echo: 7773  loss: 0.32842\n",
      "echo: 7774  loss: 0.32842\n",
      "echo: 7775  loss: 0.32842\n",
      "echo: 7776  loss: 0.32842\n",
      "echo: 7777  loss: 0.32842\n",
      "echo: 7778  loss: 0.32842\n",
      "echo: 7779  loss: 0.32842\n",
      "echo: 7780  loss: 0.32842\n",
      "echo: 7781  loss: 0.32842\n",
      "echo: 7782  loss: 0.32842\n",
      "echo: 7783  loss: 0.32842\n",
      "echo: 7784  loss: 0.32842\n",
      "echo: 7785  loss: 0.32842\n",
      "echo: 7786  loss: 0.32842\n",
      "echo: 7787  loss: 0.32842\n",
      "echo: 7788  loss: 0.32842\n",
      "echo: 7789  loss: 0.32842\n",
      "echo: 7790  loss: 0.32842\n",
      "echo: 7791  loss: 0.32842\n",
      "echo: 7792  loss: 0.32842\n",
      "echo: 7793  loss: 0.32842\n",
      "echo: 7794  loss: 0.32842\n",
      "echo: 7795  loss: 0.32842\n",
      "echo: 7796  loss: 0.32842\n",
      "echo: 7797  loss: 0.32842\n",
      "echo: 7798  loss: 0.32842\n",
      "echo: 7799  loss: 0.32842\n",
      "echo: 7800  loss: 0.32842\n",
      "echo: 7801  loss: 0.32842\n",
      "echo: 7802  loss: 0.32842\n",
      "echo: 7803  loss: 0.32842\n",
      "echo: 7804  loss: 0.32842\n",
      "echo: 7805  loss: 0.32842\n",
      "echo: 7806  loss: 0.32842\n",
      "echo: 7807  loss: 0.32842\n",
      "echo: 7808  loss: 0.32842\n",
      "echo: 7809  loss: 0.32842\n",
      "echo: 7810  loss: 0.32842\n",
      "echo: 7811  loss: 0.32842\n",
      "echo: 7812  loss: 0.32842\n",
      "echo: 7813  loss: 0.32842\n",
      "echo: 7814  loss: 0.32842\n",
      "echo: 7815  loss: 0.32842\n",
      "echo: 7816  loss: 0.32842\n",
      "echo: 7817  loss: 0.32842\n",
      "echo: 7818  loss: 0.32842\n",
      "echo: 7819  loss: 0.32842\n",
      "echo: 7820  loss: 0.32842\n",
      "echo: 7821  loss: 0.32842\n",
      "echo: 7822  loss: 0.32842\n",
      "echo: 7823  loss: 0.32842\n",
      "echo: 7824  loss: 0.32842\n",
      "echo: 7825  loss: 0.32842\n",
      "echo: 7826  loss: 0.32842\n",
      "echo: 7827  loss: 0.32842\n",
      "echo: 7828  loss: 0.32842\n",
      "echo: 7829  loss: 0.32842\n",
      "echo: 7830  loss: 0.32842\n",
      "echo: 7831  loss: 0.32842\n",
      "echo: 7832  loss: 0.32842\n",
      "echo: 7833  loss: 0.32842\n",
      "echo: 7834  loss: 0.32842\n",
      "echo: 7835  loss: 0.32842\n",
      "echo: 7836  loss: 0.32842\n",
      "echo: 7837  loss: 0.32842\n",
      "echo: 7838  loss: 0.32842\n",
      "echo: 7839  loss: 0.32842\n",
      "echo: 7840  loss: 0.32842\n",
      "echo: 7841  loss: 0.32842\n",
      "echo: 7842  loss: 0.32842\n",
      "echo: 7843  loss: 0.32842\n",
      "echo: 7844  loss: 0.32842\n",
      "echo: 7845  loss: 0.32842\n",
      "echo: 7846  loss: 0.32842\n",
      "echo: 7847  loss: 0.32842\n",
      "echo: 7848  loss: 0.32842\n",
      "echo: 7849  loss: 0.32841\n",
      "echo: 7850  loss: 0.32841\n",
      "echo: 7851  loss: 0.32841\n",
      "echo: 7852  loss: 0.32841\n",
      "echo: 7853  loss: 0.32841\n",
      "echo: 7854  loss: 0.32841\n",
      "echo: 7855  loss: 0.32841\n",
      "echo: 7856  loss: 0.32841\n",
      "echo: 7857  loss: 0.32841\n",
      "echo: 7858  loss: 0.32841\n",
      "echo: 7859  loss: 0.32841\n",
      "echo: 7860  loss: 0.32841\n",
      "echo: 7861  loss: 0.32841\n",
      "echo: 7862  loss: 0.32841\n",
      "echo: 7863  loss: 0.32841\n",
      "echo: 7864  loss: 0.32841\n",
      "echo: 7865  loss: 0.32841\n",
      "echo: 7866  loss: 0.32841\n",
      "echo: 7867  loss: 0.32841\n",
      "echo: 7868  loss: 0.32841\n",
      "echo: 7869  loss: 0.32841\n",
      "echo: 7870  loss: 0.32841\n",
      "echo: 7871  loss: 0.32841\n",
      "echo: 7872  loss: 0.32841\n",
      "echo: 7873  loss: 0.32841\n",
      "echo: 7874  loss: 0.32841\n",
      "echo: 7875  loss: 0.32841\n",
      "echo: 7876  loss: 0.32841\n",
      "echo: 7877  loss: 0.32841\n",
      "echo: 7878  loss: 0.32841\n",
      "echo: 7879  loss: 0.32841\n",
      "echo: 7880  loss: 0.32841\n",
      "echo: 7881  loss: 0.32841\n",
      "echo: 7882  loss: 0.32841\n",
      "echo: 7883  loss: 0.32841\n",
      "echo: 7884  loss: 0.32841\n",
      "echo: 7885  loss: 0.32841\n",
      "echo: 7886  loss: 0.32841\n",
      "echo: 7887  loss: 0.32841\n",
      "echo: 7888  loss: 0.32841\n",
      "echo: 7889  loss: 0.32841\n",
      "echo: 7890  loss: 0.32841\n",
      "echo: 7891  loss: 0.32841\n",
      "echo: 7892  loss: 0.32841\n",
      "echo: 7893  loss: 0.32841\n",
      "echo: 7894  loss: 0.32841\n",
      "echo: 7895  loss: 0.32841\n",
      "echo: 7896  loss: 0.32841\n",
      "echo: 7897  loss: 0.32841\n",
      "echo: 7898  loss: 0.32841\n",
      "echo: 7899  loss: 0.32841\n",
      "echo: 7900  loss: 0.32841\n",
      "echo: 7901  loss: 0.32841\n",
      "echo: 7902  loss: 0.32841\n",
      "echo: 7903  loss: 0.32841\n",
      "echo: 7904  loss: 0.32841\n",
      "echo: 7905  loss: 0.32841\n",
      "echo: 7906  loss: 0.32841\n",
      "echo: 7907  loss: 0.32841\n",
      "echo: 7908  loss: 0.32841\n",
      "echo: 7909  loss: 0.32841\n",
      "echo: 7910  loss: 0.32841\n",
      "echo: 7911  loss: 0.32841\n",
      "echo: 7912  loss: 0.32841\n",
      "echo: 7913  loss: 0.32841\n",
      "echo: 7914  loss: 0.32841\n",
      "echo: 7915  loss: 0.32841\n",
      "echo: 7916  loss: 0.32841\n",
      "echo: 7917  loss: 0.32841\n",
      "echo: 7918  loss: 0.32841\n",
      "echo: 7919  loss: 0.32841\n",
      "echo: 7920  loss: 0.32841\n",
      "echo: 7921  loss: 0.32841\n",
      "echo: 7922  loss: 0.32841\n",
      "echo: 7923  loss: 0.32841\n",
      "echo: 7924  loss: 0.32841\n",
      "echo: 7925  loss: 0.32841\n",
      "echo: 7926  loss: 0.32841\n",
      "echo: 7927  loss: 0.32841\n",
      "echo: 7928  loss: 0.32841\n",
      "echo: 7929  loss: 0.32841\n",
      "echo: 7930  loss: 0.32841\n",
      "echo: 7931  loss: 0.32841\n",
      "echo: 7932  loss: 0.32841\n",
      "echo: 7933  loss: 0.32841\n",
      "echo: 7934  loss: 0.32841\n",
      "echo: 7935  loss: 0.32841\n",
      "echo: 7936  loss: 0.32841\n",
      "echo: 7937  loss: 0.32841\n",
      "echo: 7938  loss: 0.32841\n",
      "echo: 7939  loss: 0.32841\n",
      "echo: 7940  loss: 0.32841\n",
      "echo: 7941  loss: 0.32841\n",
      "echo: 7942  loss: 0.32841\n",
      "echo: 7943  loss: 0.32841\n",
      "echo: 7944  loss: 0.32841\n",
      "echo: 7945  loss: 0.32841\n",
      "echo: 7946  loss: 0.32841\n",
      "echo: 7947  loss: 0.32841\n",
      "echo: 7948  loss: 0.32841\n",
      "echo: 7949  loss: 0.32841\n",
      "echo: 7950  loss: 0.32841\n",
      "echo: 7951  loss: 0.32841\n",
      "echo: 7952  loss: 0.32841\n",
      "echo: 7953  loss: 0.32841\n",
      "echo: 7954  loss: 0.32841\n",
      "echo: 7955  loss: 0.32841\n",
      "echo: 7956  loss: 0.32841\n",
      "echo: 7957  loss: 0.32841\n",
      "echo: 7958  loss: 0.32841\n",
      "echo: 7959  loss: 0.32841\n",
      "echo: 7960  loss: 0.32841\n",
      "echo: 7961  loss: 0.32841\n",
      "echo: 7962  loss: 0.32841\n",
      "echo: 7963  loss: 0.32841\n",
      "echo: 7964  loss: 0.32841\n",
      "echo: 7965  loss: 0.32841\n",
      "echo: 7966  loss: 0.32841\n",
      "echo: 7967  loss: 0.32841\n",
      "echo: 7968  loss: 0.32841\n",
      "echo: 7969  loss: 0.32841\n",
      "echo: 7970  loss: 0.32841\n",
      "echo: 7971  loss: 0.32841\n",
      "echo: 7972  loss: 0.32841\n",
      "echo: 7973  loss: 0.32841\n",
      "echo: 7974  loss: 0.32841\n",
      "echo: 7975  loss: 0.32841\n",
      "echo: 7976  loss: 0.32841\n",
      "echo: 7977  loss: 0.32841\n",
      "echo: 7978  loss: 0.3284\n",
      "echo: 7979  loss: 0.3284\n",
      "echo: 7980  loss: 0.3284\n",
      "echo: 7981  loss: 0.3284\n",
      "echo: 7982  loss: 0.3284\n",
      "echo: 7983  loss: 0.3284\n",
      "echo: 7984  loss: 0.3284\n",
      "echo: 7985  loss: 0.3284\n",
      "echo: 7986  loss: 0.3284\n",
      "echo: 7987  loss: 0.3284\n",
      "echo: 7988  loss: 0.3284\n",
      "echo: 7989  loss: 0.3284\n",
      "echo: 7990  loss: 0.3284\n",
      "echo: 7991  loss: 0.3284\n",
      "echo: 7992  loss: 0.3284\n",
      "echo: 7993  loss: 0.3284\n",
      "echo: 7994  loss: 0.3284\n",
      "echo: 7995  loss: 0.3284\n",
      "echo: 7996  loss: 0.3284\n",
      "echo: 7997  loss: 0.3284\n",
      "echo: 7998  loss: 0.3284\n",
      "echo: 7999  loss: 0.3284\n",
      "echo: 8000  loss: 0.3284\n",
      "echo: 8001  loss: 0.3284\n",
      "echo: 8002  loss: 0.3284\n",
      "echo: 8003  loss: 0.3284\n",
      "echo: 8004  loss: 0.3284\n",
      "echo: 8005  loss: 0.3284\n",
      "echo: 8006  loss: 0.3284\n",
      "echo: 8007  loss: 0.3284\n",
      "echo: 8008  loss: 0.3284\n",
      "echo: 8009  loss: 0.3284\n",
      "echo: 8010  loss: 0.3284\n",
      "echo: 8011  loss: 0.3284\n",
      "echo: 8012  loss: 0.3284\n",
      "echo: 8013  loss: 0.3284\n",
      "echo: 8014  loss: 0.3284\n",
      "echo: 8015  loss: 0.3284\n",
      "echo: 8016  loss: 0.3284\n",
      "echo: 8017  loss: 0.3284\n",
      "echo: 8018  loss: 0.3284\n",
      "echo: 8019  loss: 0.3284\n",
      "echo: 8020  loss: 0.3284\n",
      "echo: 8021  loss: 0.3284\n",
      "echo: 8022  loss: 0.3284\n",
      "echo: 8023  loss: 0.3284\n",
      "echo: 8024  loss: 0.3284\n",
      "echo: 8025  loss: 0.3284\n",
      "echo: 8026  loss: 0.3284\n",
      "echo: 8027  loss: 0.3284\n",
      "echo: 8028  loss: 0.3284\n",
      "echo: 8029  loss: 0.3284\n",
      "echo: 8030  loss: 0.3284\n",
      "echo: 8031  loss: 0.3284\n",
      "echo: 8032  loss: 0.3284\n",
      "echo: 8033  loss: 0.3284\n",
      "echo: 8034  loss: 0.3284\n",
      "echo: 8035  loss: 0.3284\n",
      "echo: 8036  loss: 0.3284\n",
      "echo: 8037  loss: 0.3284\n",
      "echo: 8038  loss: 0.3284\n",
      "echo: 8039  loss: 0.3284\n",
      "echo: 8040  loss: 0.3284\n",
      "echo: 8041  loss: 0.3284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 8042  loss: 0.3284\n",
      "echo: 8043  loss: 0.3284\n",
      "echo: 8044  loss: 0.3284\n",
      "echo: 8045  loss: 0.3284\n",
      "echo: 8046  loss: 0.3284\n",
      "echo: 8047  loss: 0.3284\n",
      "echo: 8048  loss: 0.3284\n",
      "echo: 8049  loss: 0.3284\n",
      "echo: 8050  loss: 0.3284\n",
      "echo: 8051  loss: 0.3284\n",
      "echo: 8052  loss: 0.3284\n",
      "echo: 8053  loss: 0.3284\n",
      "echo: 8054  loss: 0.3284\n",
      "echo: 8055  loss: 0.3284\n",
      "echo: 8056  loss: 0.3284\n",
      "echo: 8057  loss: 0.3284\n",
      "echo: 8058  loss: 0.3284\n",
      "echo: 8059  loss: 0.3284\n",
      "echo: 8060  loss: 0.3284\n",
      "echo: 8061  loss: 0.3284\n",
      "echo: 8062  loss: 0.3284\n",
      "echo: 8063  loss: 0.3284\n",
      "echo: 8064  loss: 0.3284\n",
      "echo: 8065  loss: 0.3284\n",
      "echo: 8066  loss: 0.3284\n",
      "echo: 8067  loss: 0.3284\n",
      "echo: 8068  loss: 0.3284\n",
      "echo: 8069  loss: 0.3284\n",
      "echo: 8070  loss: 0.3284\n",
      "echo: 8071  loss: 0.3284\n",
      "echo: 8072  loss: 0.3284\n",
      "echo: 8073  loss: 0.3284\n",
      "echo: 8074  loss: 0.3284\n",
      "echo: 8075  loss: 0.3284\n",
      "echo: 8076  loss: 0.3284\n",
      "echo: 8077  loss: 0.3284\n",
      "echo: 8078  loss: 0.3284\n",
      "echo: 8079  loss: 0.3284\n",
      "echo: 8080  loss: 0.3284\n",
      "echo: 8081  loss: 0.3284\n",
      "echo: 8082  loss: 0.3284\n",
      "echo: 8083  loss: 0.3284\n",
      "echo: 8084  loss: 0.3284\n",
      "echo: 8085  loss: 0.3284\n",
      "echo: 8086  loss: 0.3284\n",
      "echo: 8087  loss: 0.3284\n",
      "echo: 8088  loss: 0.3284\n",
      "echo: 8089  loss: 0.3284\n",
      "echo: 8090  loss: 0.3284\n",
      "echo: 8091  loss: 0.3284\n",
      "echo: 8092  loss: 0.3284\n",
      "echo: 8093  loss: 0.3284\n",
      "echo: 8094  loss: 0.3284\n",
      "echo: 8095  loss: 0.3284\n",
      "echo: 8096  loss: 0.3284\n",
      "echo: 8097  loss: 0.3284\n",
      "echo: 8098  loss: 0.3284\n",
      "echo: 8099  loss: 0.3284\n",
      "echo: 8100  loss: 0.3284\n",
      "echo: 8101  loss: 0.3284\n",
      "echo: 8102  loss: 0.3284\n",
      "echo: 8103  loss: 0.3284\n",
      "echo: 8104  loss: 0.3284\n",
      "echo: 8105  loss: 0.3284\n",
      "echo: 8106  loss: 0.3284\n",
      "echo: 8107  loss: 0.3284\n",
      "echo: 8108  loss: 0.3284\n",
      "echo: 8109  loss: 0.3284\n",
      "echo: 8110  loss: 0.3284\n",
      "echo: 8111  loss: 0.3284\n",
      "echo: 8112  loss: 0.3284\n",
      "echo: 8113  loss: 0.3284\n",
      "echo: 8114  loss: 0.3284\n",
      "echo: 8115  loss: 0.3284\n",
      "echo: 8116  loss: 0.3284\n",
      "echo: 8117  loss: 0.3284\n",
      "echo: 8118  loss: 0.3284\n",
      "echo: 8119  loss: 0.32839\n",
      "echo: 8120  loss: 0.32839\n",
      "echo: 8121  loss: 0.32839\n",
      "echo: 8122  loss: 0.32839\n",
      "echo: 8123  loss: 0.32839\n",
      "echo: 8124  loss: 0.32839\n",
      "echo: 8125  loss: 0.32839\n",
      "echo: 8126  loss: 0.32839\n",
      "echo: 8127  loss: 0.32839\n",
      "echo: 8128  loss: 0.32839\n",
      "echo: 8129  loss: 0.32839\n",
      "echo: 8130  loss: 0.32839\n",
      "echo: 8131  loss: 0.32839\n",
      "echo: 8132  loss: 0.32839\n",
      "echo: 8133  loss: 0.32839\n",
      "echo: 8134  loss: 0.32839\n",
      "echo: 8135  loss: 0.32839\n",
      "echo: 8136  loss: 0.32839\n",
      "echo: 8137  loss: 0.32839\n",
      "echo: 8138  loss: 0.32839\n",
      "echo: 8139  loss: 0.32839\n",
      "echo: 8140  loss: 0.32839\n",
      "echo: 8141  loss: 0.32839\n",
      "echo: 8142  loss: 0.32839\n",
      "echo: 8143  loss: 0.32839\n",
      "echo: 8144  loss: 0.32839\n",
      "echo: 8145  loss: 0.32839\n",
      "echo: 8146  loss: 0.32839\n",
      "echo: 8147  loss: 0.32839\n",
      "echo: 8148  loss: 0.32839\n",
      "echo: 8149  loss: 0.32839\n",
      "echo: 8150  loss: 0.32839\n",
      "echo: 8151  loss: 0.32839\n",
      "echo: 8152  loss: 0.32839\n",
      "echo: 8153  loss: 0.32839\n",
      "echo: 8154  loss: 0.32839\n",
      "echo: 8155  loss: 0.32839\n",
      "echo: 8156  loss: 0.32839\n",
      "echo: 8157  loss: 0.32839\n",
      "echo: 8158  loss: 0.32839\n",
      "echo: 8159  loss: 0.32839\n",
      "echo: 8160  loss: 0.32839\n",
      "echo: 8161  loss: 0.32839\n",
      "echo: 8162  loss: 0.32839\n",
      "echo: 8163  loss: 0.32839\n",
      "echo: 8164  loss: 0.32839\n",
      "echo: 8165  loss: 0.32839\n",
      "echo: 8166  loss: 0.32839\n",
      "echo: 8167  loss: 0.32839\n",
      "echo: 8168  loss: 0.32839\n",
      "echo: 8169  loss: 0.32839\n",
      "echo: 8170  loss: 0.32839\n",
      "echo: 8171  loss: 0.32839\n",
      "echo: 8172  loss: 0.32839\n",
      "echo: 8173  loss: 0.32839\n",
      "echo: 8174  loss: 0.32839\n",
      "echo: 8175  loss: 0.32839\n",
      "echo: 8176  loss: 0.32839\n",
      "echo: 8177  loss: 0.32839\n",
      "echo: 8178  loss: 0.32839\n",
      "echo: 8179  loss: 0.32839\n",
      "echo: 8180  loss: 0.32839\n",
      "echo: 8181  loss: 0.32839\n",
      "echo: 8182  loss: 0.32839\n",
      "echo: 8183  loss: 0.32839\n",
      "echo: 8184  loss: 0.32839\n",
      "echo: 8185  loss: 0.32839\n",
      "echo: 8186  loss: 0.32839\n",
      "echo: 8187  loss: 0.32839\n",
      "echo: 8188  loss: 0.32839\n",
      "echo: 8189  loss: 0.32839\n",
      "echo: 8190  loss: 0.32839\n",
      "echo: 8191  loss: 0.32839\n",
      "echo: 8192  loss: 0.32839\n",
      "echo: 8193  loss: 0.32839\n",
      "echo: 8194  loss: 0.32839\n",
      "echo: 8195  loss: 0.32839\n",
      "echo: 8196  loss: 0.32839\n",
      "echo: 8197  loss: 0.32839\n",
      "echo: 8198  loss: 0.32839\n",
      "echo: 8199  loss: 0.32839\n",
      "echo: 8200  loss: 0.32839\n",
      "echo: 8201  loss: 0.32839\n",
      "echo: 8202  loss: 0.32839\n",
      "echo: 8203  loss: 0.32839\n",
      "echo: 8204  loss: 0.32839\n",
      "echo: 8205  loss: 0.32839\n",
      "echo: 8206  loss: 0.32839\n",
      "echo: 8207  loss: 0.32839\n",
      "echo: 8208  loss: 0.32839\n",
      "echo: 8209  loss: 0.32839\n",
      "echo: 8210  loss: 0.32839\n",
      "echo: 8211  loss: 0.32839\n",
      "echo: 8212  loss: 0.32839\n",
      "echo: 8213  loss: 0.32839\n",
      "echo: 8214  loss: 0.32839\n",
      "echo: 8215  loss: 0.32839\n",
      "echo: 8216  loss: 0.32839\n",
      "echo: 8217  loss: 0.32839\n",
      "echo: 8218  loss: 0.32839\n",
      "echo: 8219  loss: 0.32839\n",
      "echo: 8220  loss: 0.32839\n",
      "echo: 8221  loss: 0.32839\n",
      "echo: 8222  loss: 0.32839\n",
      "echo: 8223  loss: 0.32839\n",
      "echo: 8224  loss: 0.32839\n",
      "echo: 8225  loss: 0.32839\n",
      "echo: 8226  loss: 0.32839\n",
      "echo: 8227  loss: 0.32839\n",
      "echo: 8228  loss: 0.32839\n",
      "echo: 8229  loss: 0.32839\n",
      "echo: 8230  loss: 0.32839\n",
      "echo: 8231  loss: 0.32839\n",
      "echo: 8232  loss: 0.32839\n",
      "echo: 8233  loss: 0.32839\n",
      "echo: 8234  loss: 0.32839\n",
      "echo: 8235  loss: 0.32839\n",
      "echo: 8236  loss: 0.32839\n",
      "echo: 8237  loss: 0.32839\n",
      "echo: 8238  loss: 0.32839\n",
      "echo: 8239  loss: 0.32839\n",
      "echo: 8240  loss: 0.32839\n",
      "echo: 8241  loss: 0.32839\n",
      "echo: 8242  loss: 0.32839\n",
      "echo: 8243  loss: 0.32839\n",
      "echo: 8244  loss: 0.32839\n",
      "echo: 8245  loss: 0.32839\n",
      "echo: 8246  loss: 0.32839\n",
      "echo: 8247  loss: 0.32839\n",
      "echo: 8248  loss: 0.32839\n",
      "echo: 8249  loss: 0.32839\n",
      "echo: 8250  loss: 0.32839\n",
      "echo: 8251  loss: 0.32839\n",
      "echo: 8252  loss: 0.32839\n",
      "echo: 8253  loss: 0.32839\n",
      "echo: 8254  loss: 0.32839\n",
      "echo: 8255  loss: 0.32839\n",
      "echo: 8256  loss: 0.32839\n",
      "echo: 8257  loss: 0.32839\n",
      "echo: 8258  loss: 0.32839\n",
      "echo: 8259  loss: 0.32839\n",
      "echo: 8260  loss: 0.32839\n",
      "echo: 8261  loss: 0.32839\n",
      "echo: 8262  loss: 0.32839\n",
      "echo: 8263  loss: 0.32839\n",
      "echo: 8264  loss: 0.32839\n",
      "echo: 8265  loss: 0.32839\n",
      "echo: 8266  loss: 0.32839\n",
      "echo: 8267  loss: 0.32839\n",
      "echo: 8268  loss: 0.32838\n",
      "echo: 8269  loss: 0.32838\n",
      "echo: 8270  loss: 0.32838\n",
      "echo: 8271  loss: 0.32838\n",
      "echo: 8272  loss: 0.32838\n",
      "echo: 8273  loss: 0.32838\n",
      "echo: 8274  loss: 0.32838\n",
      "echo: 8275  loss: 0.32838\n",
      "echo: 8276  loss: 0.32838\n",
      "echo: 8277  loss: 0.32838\n",
      "echo: 8278  loss: 0.32838\n",
      "echo: 8279  loss: 0.32838\n",
      "echo: 8280  loss: 0.32838\n",
      "echo: 8281  loss: 0.32838\n",
      "echo: 8282  loss: 0.32838\n",
      "echo: 8283  loss: 0.32838\n",
      "echo: 8284  loss: 0.32838\n",
      "echo: 8285  loss: 0.32838\n",
      "echo: 8286  loss: 0.32838\n",
      "echo: 8287  loss: 0.32838\n",
      "echo: 8288  loss: 0.32838\n",
      "echo: 8289  loss: 0.32838\n",
      "echo: 8290  loss: 0.32838\n",
      "echo: 8291  loss: 0.32838\n",
      "echo: 8292  loss: 0.32838\n",
      "echo: 8293  loss: 0.32838\n",
      "echo: 8294  loss: 0.32838\n",
      "echo: 8295  loss: 0.32838\n",
      "echo: 8296  loss: 0.32838\n",
      "echo: 8297  loss: 0.32838\n",
      "echo: 8298  loss: 0.32838\n",
      "echo: 8299  loss: 0.32838\n",
      "echo: 8300  loss: 0.32838\n",
      "echo: 8301  loss: 0.32838\n",
      "echo: 8302  loss: 0.32838\n",
      "echo: 8303  loss: 0.32838\n",
      "echo: 8304  loss: 0.32838\n",
      "echo: 8305  loss: 0.32838\n",
      "echo: 8306  loss: 0.32838\n",
      "echo: 8307  loss: 0.32838\n",
      "echo: 8308  loss: 0.32838\n",
      "echo: 8309  loss: 0.32838\n",
      "echo: 8310  loss: 0.32838\n",
      "echo: 8311  loss: 0.32838\n",
      "echo: 8312  loss: 0.32838\n",
      "echo: 8313  loss: 0.32838\n",
      "echo: 8314  loss: 0.32838\n",
      "echo: 8315  loss: 0.32838\n",
      "echo: 8316  loss: 0.32838\n",
      "echo: 8317  loss: 0.32838\n",
      "echo: 8318  loss: 0.32838\n",
      "echo: 8319  loss: 0.32838\n",
      "echo: 8320  loss: 0.32838\n",
      "echo: 8321  loss: 0.32838\n",
      "echo: 8322  loss: 0.32838\n",
      "echo: 8323  loss: 0.32838\n",
      "echo: 8324  loss: 0.32838\n",
      "echo: 8325  loss: 0.32838\n",
      "echo: 8326  loss: 0.32838\n",
      "echo: 8327  loss: 0.32838\n",
      "echo: 8328  loss: 0.32838\n",
      "echo: 8329  loss: 0.32838\n",
      "echo: 8330  loss: 0.32838\n",
      "echo: 8331  loss: 0.32838\n",
      "echo: 8332  loss: 0.32838\n",
      "echo: 8333  loss: 0.32838\n",
      "echo: 8334  loss: 0.32838\n",
      "echo: 8335  loss: 0.32838\n",
      "echo: 8336  loss: 0.32838\n",
      "echo: 8337  loss: 0.32838\n",
      "echo: 8338  loss: 0.32838\n",
      "echo: 8339  loss: 0.32838\n",
      "echo: 8340  loss: 0.32838\n",
      "echo: 8341  loss: 0.32838\n",
      "echo: 8342  loss: 0.32838\n",
      "echo: 8343  loss: 0.32838\n",
      "echo: 8344  loss: 0.32838\n",
      "echo: 8345  loss: 0.32838\n",
      "echo: 8346  loss: 0.32838\n",
      "echo: 8347  loss: 0.32838\n",
      "echo: 8348  loss: 0.32838\n",
      "echo: 8349  loss: 0.32838\n",
      "echo: 8350  loss: 0.32838\n",
      "echo: 8351  loss: 0.32838\n",
      "echo: 8352  loss: 0.32838\n",
      "echo: 8353  loss: 0.32838\n",
      "echo: 8354  loss: 0.32838\n",
      "echo: 8355  loss: 0.32838\n",
      "echo: 8356  loss: 0.32838\n",
      "echo: 8357  loss: 0.32838\n",
      "echo: 8358  loss: 0.32838\n",
      "echo: 8359  loss: 0.32838\n",
      "echo: 8360  loss: 0.32838\n",
      "echo: 8361  loss: 0.32838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 8362  loss: 0.32838\n",
      "echo: 8363  loss: 0.32838\n",
      "echo: 8364  loss: 0.32838\n",
      "echo: 8365  loss: 0.32838\n",
      "echo: 8366  loss: 0.32838\n",
      "echo: 8367  loss: 0.32838\n",
      "echo: 8368  loss: 0.32838\n",
      "echo: 8369  loss: 0.32838\n",
      "echo: 8370  loss: 0.32838\n",
      "echo: 8371  loss: 0.32838\n",
      "echo: 8372  loss: 0.32838\n",
      "echo: 8373  loss: 0.32838\n",
      "echo: 8374  loss: 0.32838\n",
      "echo: 8375  loss: 0.32838\n",
      "echo: 8376  loss: 0.32838\n",
      "echo: 8377  loss: 0.32838\n",
      "echo: 8378  loss: 0.32838\n",
      "echo: 8379  loss: 0.32838\n",
      "echo: 8380  loss: 0.32838\n",
      "echo: 8381  loss: 0.32838\n",
      "echo: 8382  loss: 0.32838\n",
      "echo: 8383  loss: 0.32838\n",
      "echo: 8384  loss: 0.32838\n",
      "echo: 8385  loss: 0.32838\n",
      "echo: 8386  loss: 0.32838\n",
      "echo: 8387  loss: 0.32838\n",
      "echo: 8388  loss: 0.32838\n",
      "echo: 8389  loss: 0.32838\n",
      "echo: 8390  loss: 0.32838\n",
      "echo: 8391  loss: 0.32838\n",
      "echo: 8392  loss: 0.32838\n",
      "echo: 8393  loss: 0.32838\n",
      "echo: 8394  loss: 0.32838\n",
      "echo: 8395  loss: 0.32838\n",
      "echo: 8396  loss: 0.32838\n",
      "echo: 8397  loss: 0.32838\n",
      "echo: 8398  loss: 0.32838\n",
      "echo: 8399  loss: 0.32838\n",
      "echo: 8400  loss: 0.32838\n",
      "echo: 8401  loss: 0.32838\n",
      "echo: 8402  loss: 0.32838\n",
      "echo: 8403  loss: 0.32838\n",
      "echo: 8404  loss: 0.32838\n",
      "echo: 8405  loss: 0.32838\n",
      "echo: 8406  loss: 0.32838\n",
      "echo: 8407  loss: 0.32838\n",
      "echo: 8408  loss: 0.32838\n",
      "echo: 8409  loss: 0.32838\n",
      "echo: 8410  loss: 0.32838\n",
      "echo: 8411  loss: 0.32838\n",
      "echo: 8412  loss: 0.32838\n",
      "echo: 8413  loss: 0.32838\n",
      "echo: 8414  loss: 0.32838\n",
      "echo: 8415  loss: 0.32838\n",
      "echo: 8416  loss: 0.32838\n",
      "echo: 8417  loss: 0.32838\n",
      "echo: 8418  loss: 0.32838\n",
      "echo: 8419  loss: 0.32838\n",
      "echo: 8420  loss: 0.32838\n",
      "echo: 8421  loss: 0.32838\n",
      "echo: 8422  loss: 0.32838\n",
      "echo: 8423  loss: 0.32838\n",
      "echo: 8424  loss: 0.32838\n",
      "echo: 8425  loss: 0.32838\n",
      "echo: 8426  loss: 0.32838\n",
      "echo: 8427  loss: 0.32838\n",
      "echo: 8428  loss: 0.32838\n",
      "echo: 8429  loss: 0.32838\n",
      "echo: 8430  loss: 0.32838\n",
      "echo: 8431  loss: 0.32838\n",
      "echo: 8432  loss: 0.32838\n",
      "echo: 8433  loss: 0.32837\n",
      "echo: 8434  loss: 0.32837\n",
      "echo: 8435  loss: 0.32837\n",
      "echo: 8436  loss: 0.32837\n",
      "echo: 8437  loss: 0.32837\n",
      "echo: 8438  loss: 0.32837\n",
      "echo: 8439  loss: 0.32837\n",
      "echo: 8440  loss: 0.32837\n",
      "echo: 8441  loss: 0.32837\n",
      "echo: 8442  loss: 0.32837\n",
      "echo: 8443  loss: 0.32837\n",
      "echo: 8444  loss: 0.32837\n",
      "echo: 8445  loss: 0.32837\n",
      "echo: 8446  loss: 0.32837\n",
      "echo: 8447  loss: 0.32837\n",
      "echo: 8448  loss: 0.32837\n",
      "echo: 8449  loss: 0.32837\n",
      "echo: 8450  loss: 0.32837\n",
      "echo: 8451  loss: 0.32837\n",
      "echo: 8452  loss: 0.32837\n",
      "echo: 8453  loss: 0.32837\n",
      "echo: 8454  loss: 0.32837\n",
      "echo: 8455  loss: 0.32837\n",
      "echo: 8456  loss: 0.32837\n",
      "echo: 8457  loss: 0.32837\n",
      "echo: 8458  loss: 0.32837\n",
      "echo: 8459  loss: 0.32837\n",
      "echo: 8460  loss: 0.32837\n",
      "echo: 8461  loss: 0.32837\n",
      "echo: 8462  loss: 0.32837\n",
      "echo: 8463  loss: 0.32837\n",
      "echo: 8464  loss: 0.32837\n",
      "echo: 8465  loss: 0.32837\n",
      "echo: 8466  loss: 0.32837\n",
      "echo: 8467  loss: 0.32837\n",
      "echo: 8468  loss: 0.32837\n",
      "echo: 8469  loss: 0.32837\n",
      "echo: 8470  loss: 0.32837\n",
      "echo: 8471  loss: 0.32837\n",
      "echo: 8472  loss: 0.32837\n",
      "echo: 8473  loss: 0.32837\n",
      "echo: 8474  loss: 0.32837\n",
      "echo: 8475  loss: 0.32837\n",
      "echo: 8476  loss: 0.32837\n",
      "echo: 8477  loss: 0.32837\n",
      "echo: 8478  loss: 0.32837\n",
      "echo: 8479  loss: 0.32837\n",
      "echo: 8480  loss: 0.32837\n",
      "echo: 8481  loss: 0.32837\n",
      "echo: 8482  loss: 0.32837\n",
      "echo: 8483  loss: 0.32837\n",
      "echo: 8484  loss: 0.32837\n",
      "echo: 8485  loss: 0.32837\n",
      "echo: 8486  loss: 0.32837\n",
      "echo: 8487  loss: 0.32837\n",
      "echo: 8488  loss: 0.32837\n",
      "echo: 8489  loss: 0.32837\n",
      "echo: 8490  loss: 0.32837\n",
      "echo: 8491  loss: 0.32837\n",
      "echo: 8492  loss: 0.32837\n",
      "echo: 8493  loss: 0.32837\n",
      "echo: 8494  loss: 0.32837\n",
      "echo: 8495  loss: 0.32837\n",
      "echo: 8496  loss: 0.32837\n",
      "echo: 8497  loss: 0.32837\n",
      "echo: 8498  loss: 0.32837\n",
      "echo: 8499  loss: 0.32837\n",
      "echo: 8500  loss: 0.32837\n",
      "echo: 8501  loss: 0.32837\n",
      "echo: 8502  loss: 0.32837\n",
      "echo: 8503  loss: 0.32837\n",
      "echo: 8504  loss: 0.32837\n",
      "echo: 8505  loss: 0.32837\n",
      "echo: 8506  loss: 0.32837\n",
      "echo: 8507  loss: 0.32837\n",
      "echo: 8508  loss: 0.32837\n",
      "echo: 8509  loss: 0.32837\n",
      "echo: 8510  loss: 0.32837\n",
      "echo: 8511  loss: 0.32837\n",
      "echo: 8512  loss: 0.32837\n",
      "echo: 8513  loss: 0.32837\n",
      "echo: 8514  loss: 0.32837\n",
      "echo: 8515  loss: 0.32837\n",
      "echo: 8516  loss: 0.32837\n",
      "echo: 8517  loss: 0.32837\n",
      "echo: 8518  loss: 0.32837\n",
      "echo: 8519  loss: 0.32837\n",
      "echo: 8520  loss: 0.32837\n",
      "echo: 8521  loss: 0.32837\n",
      "echo: 8522  loss: 0.32837\n",
      "echo: 8523  loss: 0.32837\n",
      "echo: 8524  loss: 0.32837\n",
      "echo: 8525  loss: 0.32837\n",
      "echo: 8526  loss: 0.32837\n",
      "echo: 8527  loss: 0.32837\n",
      "echo: 8528  loss: 0.32837\n",
      "echo: 8529  loss: 0.32837\n",
      "echo: 8530  loss: 0.32837\n",
      "echo: 8531  loss: 0.32837\n",
      "echo: 8532  loss: 0.32837\n",
      "echo: 8533  loss: 0.32837\n",
      "echo: 8534  loss: 0.32837\n",
      "echo: 8535  loss: 0.32837\n",
      "echo: 8536  loss: 0.32837\n",
      "echo: 8537  loss: 0.32837\n",
      "echo: 8538  loss: 0.32837\n",
      "echo: 8539  loss: 0.32837\n",
      "echo: 8540  loss: 0.32837\n",
      "echo: 8541  loss: 0.32837\n",
      "echo: 8542  loss: 0.32837\n",
      "echo: 8543  loss: 0.32837\n",
      "echo: 8544  loss: 0.32837\n",
      "echo: 8545  loss: 0.32837\n",
      "echo: 8546  loss: 0.32837\n",
      "echo: 8547  loss: 0.32837\n",
      "echo: 8548  loss: 0.32837\n",
      "echo: 8549  loss: 0.32837\n",
      "echo: 8550  loss: 0.32837\n",
      "echo: 8551  loss: 0.32837\n",
      "echo: 8552  loss: 0.32837\n",
      "echo: 8553  loss: 0.32837\n",
      "echo: 8554  loss: 0.32837\n",
      "echo: 8555  loss: 0.32837\n",
      "echo: 8556  loss: 0.32837\n",
      "echo: 8557  loss: 0.32837\n",
      "echo: 8558  loss: 0.32837\n",
      "echo: 8559  loss: 0.32837\n",
      "echo: 8560  loss: 0.32837\n",
      "echo: 8561  loss: 0.32837\n",
      "echo: 8562  loss: 0.32837\n",
      "echo: 8563  loss: 0.32837\n",
      "echo: 8564  loss: 0.32837\n",
      "echo: 8565  loss: 0.32837\n",
      "echo: 8566  loss: 0.32837\n",
      "echo: 8567  loss: 0.32837\n",
      "echo: 8568  loss: 0.32837\n",
      "echo: 8569  loss: 0.32837\n",
      "echo: 8570  loss: 0.32837\n",
      "echo: 8571  loss: 0.32837\n",
      "echo: 8572  loss: 0.32837\n",
      "echo: 8573  loss: 0.32837\n",
      "echo: 8574  loss: 0.32837\n",
      "echo: 8575  loss: 0.32837\n",
      "echo: 8576  loss: 0.32837\n",
      "echo: 8577  loss: 0.32837\n",
      "echo: 8578  loss: 0.32837\n",
      "echo: 8579  loss: 0.32837\n",
      "echo: 8580  loss: 0.32837\n",
      "echo: 8581  loss: 0.32837\n",
      "echo: 8582  loss: 0.32837\n",
      "echo: 8583  loss: 0.32837\n",
      "echo: 8584  loss: 0.32837\n",
      "echo: 8585  loss: 0.32837\n",
      "echo: 8586  loss: 0.32837\n",
      "echo: 8587  loss: 0.32837\n",
      "echo: 8588  loss: 0.32837\n",
      "echo: 8589  loss: 0.32837\n",
      "echo: 8590  loss: 0.32837\n",
      "echo: 8591  loss: 0.32837\n",
      "echo: 8592  loss: 0.32837\n",
      "echo: 8593  loss: 0.32837\n",
      "echo: 8594  loss: 0.32837\n",
      "echo: 8595  loss: 0.32837\n",
      "echo: 8596  loss: 0.32837\n",
      "echo: 8597  loss: 0.32837\n",
      "echo: 8598  loss: 0.32837\n",
      "echo: 8599  loss: 0.32837\n",
      "echo: 8600  loss: 0.32837\n",
      "echo: 8601  loss: 0.32837\n",
      "echo: 8602  loss: 0.32837\n",
      "echo: 8603  loss: 0.32837\n",
      "echo: 8604  loss: 0.32837\n",
      "echo: 8605  loss: 0.32837\n",
      "echo: 8606  loss: 0.32837\n",
      "echo: 8607  loss: 0.32837\n",
      "echo: 8608  loss: 0.32837\n",
      "echo: 8609  loss: 0.32837\n",
      "echo: 8610  loss: 0.32837\n",
      "echo: 8611  loss: 0.32837\n",
      "echo: 8612  loss: 0.32837\n",
      "echo: 8613  loss: 0.32837\n",
      "echo: 8614  loss: 0.32836\n",
      "echo: 8615  loss: 0.32836\n",
      "echo: 8616  loss: 0.32836\n",
      "echo: 8617  loss: 0.32836\n",
      "echo: 8618  loss: 0.32836\n",
      "echo: 8619  loss: 0.32836\n",
      "echo: 8620  loss: 0.32836\n",
      "echo: 8621  loss: 0.32836\n",
      "echo: 8622  loss: 0.32836\n",
      "echo: 8623  loss: 0.32836\n",
      "echo: 8624  loss: 0.32836\n",
      "echo: 8625  loss: 0.32836\n",
      "echo: 8626  loss: 0.32836\n",
      "echo: 8627  loss: 0.32836\n",
      "echo: 8628  loss: 0.32836\n",
      "echo: 8629  loss: 0.32836\n",
      "echo: 8630  loss: 0.32836\n",
      "echo: 8631  loss: 0.32836\n",
      "echo: 8632  loss: 0.32836\n",
      "echo: 8633  loss: 0.32836\n",
      "echo: 8634  loss: 0.32836\n",
      "echo: 8635  loss: 0.32836\n",
      "echo: 8636  loss: 0.32836\n",
      "echo: 8637  loss: 0.32836\n",
      "echo: 8638  loss: 0.32836\n",
      "echo: 8639  loss: 0.32836\n",
      "echo: 8640  loss: 0.32836\n",
      "echo: 8641  loss: 0.32836\n",
      "echo: 8642  loss: 0.32836\n",
      "echo: 8643  loss: 0.32836\n",
      "echo: 8644  loss: 0.32836\n",
      "echo: 8645  loss: 0.32836\n",
      "echo: 8646  loss: 0.32836\n",
      "echo: 8647  loss: 0.32836\n",
      "echo: 8648  loss: 0.32836\n",
      "echo: 8649  loss: 0.32836\n",
      "echo: 8650  loss: 0.32836\n",
      "echo: 8651  loss: 0.32836\n",
      "echo: 8652  loss: 0.32836\n",
      "echo: 8653  loss: 0.32836\n",
      "echo: 8654  loss: 0.32836\n",
      "echo: 8655  loss: 0.32836\n",
      "echo: 8656  loss: 0.32836\n",
      "echo: 8657  loss: 0.32836\n",
      "echo: 8658  loss: 0.32836\n",
      "echo: 8659  loss: 0.32836\n",
      "echo: 8660  loss: 0.32836\n",
      "echo: 8661  loss: 0.32836\n",
      "echo: 8662  loss: 0.32836\n",
      "echo: 8663  loss: 0.32836\n",
      "echo: 8664  loss: 0.32836\n",
      "echo: 8665  loss: 0.32836\n",
      "echo: 8666  loss: 0.32836\n",
      "echo: 8667  loss: 0.32836\n",
      "echo: 8668  loss: 0.32836\n",
      "echo: 8669  loss: 0.32836\n",
      "echo: 8670  loss: 0.32836\n",
      "echo: 8671  loss: 0.32836\n",
      "echo: 8672  loss: 0.32836\n",
      "echo: 8673  loss: 0.32836\n",
      "echo: 8674  loss: 0.32836\n",
      "echo: 8675  loss: 0.32836\n",
      "echo: 8676  loss: 0.32836\n",
      "echo: 8677  loss: 0.32836\n",
      "echo: 8678  loss: 0.32836\n",
      "echo: 8679  loss: 0.32836\n",
      "echo: 8680  loss: 0.32836\n",
      "echo: 8681  loss: 0.32836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 8682  loss: 0.32836\n",
      "echo: 8683  loss: 0.32836\n",
      "echo: 8684  loss: 0.32836\n",
      "echo: 8685  loss: 0.32836\n",
      "echo: 8686  loss: 0.32836\n",
      "echo: 8687  loss: 0.32836\n",
      "echo: 8688  loss: 0.32836\n",
      "echo: 8689  loss: 0.32836\n",
      "echo: 8690  loss: 0.32836\n",
      "echo: 8691  loss: 0.32836\n",
      "echo: 8692  loss: 0.32836\n",
      "echo: 8693  loss: 0.32836\n",
      "echo: 8694  loss: 0.32836\n",
      "echo: 8695  loss: 0.32836\n",
      "echo: 8696  loss: 0.32836\n",
      "echo: 8697  loss: 0.32836\n",
      "echo: 8698  loss: 0.32836\n",
      "echo: 8699  loss: 0.32836\n",
      "echo: 8700  loss: 0.32836\n",
      "echo: 8701  loss: 0.32836\n",
      "echo: 8702  loss: 0.32836\n",
      "echo: 8703  loss: 0.32836\n",
      "echo: 8704  loss: 0.32836\n",
      "echo: 8705  loss: 0.32836\n",
      "echo: 8706  loss: 0.32836\n",
      "echo: 8707  loss: 0.32836\n",
      "echo: 8708  loss: 0.32836\n",
      "echo: 8709  loss: 0.32836\n",
      "echo: 8710  loss: 0.32836\n",
      "echo: 8711  loss: 0.32836\n",
      "echo: 8712  loss: 0.32836\n",
      "echo: 8713  loss: 0.32836\n",
      "echo: 8714  loss: 0.32836\n",
      "echo: 8715  loss: 0.32836\n",
      "echo: 8716  loss: 0.32836\n",
      "echo: 8717  loss: 0.32836\n",
      "echo: 8718  loss: 0.32836\n",
      "echo: 8719  loss: 0.32836\n",
      "echo: 8720  loss: 0.32836\n",
      "echo: 8721  loss: 0.32836\n",
      "echo: 8722  loss: 0.32836\n",
      "echo: 8723  loss: 0.32836\n",
      "echo: 8724  loss: 0.32836\n",
      "echo: 8725  loss: 0.32836\n",
      "echo: 8726  loss: 0.32836\n",
      "echo: 8727  loss: 0.32836\n",
      "echo: 8728  loss: 0.32836\n",
      "echo: 8729  loss: 0.32836\n",
      "echo: 8730  loss: 0.32836\n",
      "echo: 8731  loss: 0.32836\n",
      "echo: 8732  loss: 0.32836\n",
      "echo: 8733  loss: 0.32836\n",
      "echo: 8734  loss: 0.32836\n",
      "echo: 8735  loss: 0.32836\n",
      "echo: 8736  loss: 0.32836\n",
      "echo: 8737  loss: 0.32836\n",
      "echo: 8738  loss: 0.32836\n",
      "echo: 8739  loss: 0.32836\n",
      "echo: 8740  loss: 0.32836\n",
      "echo: 8741  loss: 0.32836\n",
      "echo: 8742  loss: 0.32836\n",
      "echo: 8743  loss: 0.32836\n",
      "echo: 8744  loss: 0.32836\n",
      "echo: 8745  loss: 0.32836\n",
      "echo: 8746  loss: 0.32836\n",
      "echo: 8747  loss: 0.32836\n",
      "echo: 8748  loss: 0.32836\n",
      "echo: 8749  loss: 0.32836\n",
      "echo: 8750  loss: 0.32836\n",
      "echo: 8751  loss: 0.32836\n",
      "echo: 8752  loss: 0.32836\n",
      "echo: 8753  loss: 0.32836\n",
      "echo: 8754  loss: 0.32836\n",
      "echo: 8755  loss: 0.32836\n",
      "echo: 8756  loss: 0.32836\n",
      "echo: 8757  loss: 0.32836\n",
      "echo: 8758  loss: 0.32836\n",
      "echo: 8759  loss: 0.32836\n",
      "echo: 8760  loss: 0.32836\n",
      "echo: 8761  loss: 0.32836\n",
      "echo: 8762  loss: 0.32836\n",
      "echo: 8763  loss: 0.32836\n",
      "echo: 8764  loss: 0.32836\n",
      "echo: 8765  loss: 0.32836\n",
      "echo: 8766  loss: 0.32836\n",
      "echo: 8767  loss: 0.32836\n",
      "echo: 8768  loss: 0.32836\n",
      "echo: 8769  loss: 0.32836\n",
      "echo: 8770  loss: 0.32836\n",
      "echo: 8771  loss: 0.32836\n",
      "echo: 8772  loss: 0.32836\n",
      "echo: 8773  loss: 0.32836\n",
      "echo: 8774  loss: 0.32836\n",
      "echo: 8775  loss: 0.32836\n",
      "echo: 8776  loss: 0.32836\n",
      "echo: 8777  loss: 0.32836\n",
      "echo: 8778  loss: 0.32836\n",
      "echo: 8779  loss: 0.32836\n",
      "echo: 8780  loss: 0.32836\n",
      "echo: 8781  loss: 0.32836\n",
      "echo: 8782  loss: 0.32836\n",
      "echo: 8783  loss: 0.32836\n",
      "echo: 8784  loss: 0.32836\n",
      "echo: 8785  loss: 0.32836\n",
      "echo: 8786  loss: 0.32836\n",
      "echo: 8787  loss: 0.32836\n",
      "echo: 8788  loss: 0.32836\n",
      "echo: 8789  loss: 0.32836\n",
      "echo: 8790  loss: 0.32836\n",
      "echo: 8791  loss: 0.32836\n",
      "echo: 8792  loss: 0.32836\n",
      "echo: 8793  loss: 0.32836\n",
      "echo: 8794  loss: 0.32836\n",
      "echo: 8795  loss: 0.32836\n",
      "echo: 8796  loss: 0.32836\n",
      "echo: 8797  loss: 0.32836\n",
      "echo: 8798  loss: 0.32836\n",
      "echo: 8799  loss: 0.32836\n",
      "echo: 8800  loss: 0.32836\n",
      "echo: 8801  loss: 0.32836\n",
      "echo: 8802  loss: 0.32836\n",
      "echo: 8803  loss: 0.32836\n",
      "echo: 8804  loss: 0.32836\n",
      "echo: 8805  loss: 0.32836\n",
      "echo: 8806  loss: 0.32836\n",
      "echo: 8807  loss: 0.32836\n",
      "echo: 8808  loss: 0.32836\n",
      "echo: 8809  loss: 0.32836\n",
      "echo: 8810  loss: 0.32836\n",
      "echo: 8811  loss: 0.32836\n",
      "echo: 8812  loss: 0.32835\n",
      "echo: 8813  loss: 0.32835\n",
      "echo: 8814  loss: 0.32835\n",
      "echo: 8815  loss: 0.32835\n",
      "echo: 8816  loss: 0.32835\n",
      "echo: 8817  loss: 0.32835\n",
      "echo: 8818  loss: 0.32835\n",
      "echo: 8819  loss: 0.32835\n",
      "echo: 8820  loss: 0.32835\n",
      "echo: 8821  loss: 0.32835\n",
      "echo: 8822  loss: 0.32835\n",
      "echo: 8823  loss: 0.32835\n",
      "echo: 8824  loss: 0.32835\n",
      "echo: 8825  loss: 0.32835\n",
      "echo: 8826  loss: 0.32835\n",
      "echo: 8827  loss: 0.32835\n",
      "echo: 8828  loss: 0.32835\n",
      "echo: 8829  loss: 0.32835\n",
      "echo: 8830  loss: 0.32835\n",
      "echo: 8831  loss: 0.32835\n",
      "echo: 8832  loss: 0.32835\n",
      "echo: 8833  loss: 0.32835\n",
      "echo: 8834  loss: 0.32835\n",
      "echo: 8835  loss: 0.32835\n",
      "echo: 8836  loss: 0.32835\n",
      "echo: 8837  loss: 0.32835\n",
      "echo: 8838  loss: 0.32835\n",
      "echo: 8839  loss: 0.32835\n",
      "echo: 8840  loss: 0.32835\n",
      "echo: 8841  loss: 0.32835\n",
      "echo: 8842  loss: 0.32835\n",
      "echo: 8843  loss: 0.32835\n",
      "echo: 8844  loss: 0.32835\n",
      "echo: 8845  loss: 0.32835\n",
      "echo: 8846  loss: 0.32835\n",
      "echo: 8847  loss: 0.32835\n",
      "echo: 8848  loss: 0.32835\n",
      "echo: 8849  loss: 0.32835\n",
      "echo: 8850  loss: 0.32835\n",
      "echo: 8851  loss: 0.32835\n",
      "echo: 8852  loss: 0.32835\n",
      "echo: 8853  loss: 0.32835\n",
      "echo: 8854  loss: 0.32835\n",
      "echo: 8855  loss: 0.32835\n",
      "echo: 8856  loss: 0.32835\n",
      "echo: 8857  loss: 0.32835\n",
      "echo: 8858  loss: 0.32835\n",
      "echo: 8859  loss: 0.32835\n",
      "echo: 8860  loss: 0.32835\n",
      "echo: 8861  loss: 0.32835\n",
      "echo: 8862  loss: 0.32835\n",
      "echo: 8863  loss: 0.32835\n",
      "echo: 8864  loss: 0.32835\n",
      "echo: 8865  loss: 0.32835\n",
      "echo: 8866  loss: 0.32835\n",
      "echo: 8867  loss: 0.32835\n",
      "echo: 8868  loss: 0.32835\n",
      "echo: 8869  loss: 0.32835\n",
      "echo: 8870  loss: 0.32835\n",
      "echo: 8871  loss: 0.32835\n",
      "echo: 8872  loss: 0.32835\n",
      "echo: 8873  loss: 0.32835\n",
      "echo: 8874  loss: 0.32835\n",
      "echo: 8875  loss: 0.32835\n",
      "echo: 8876  loss: 0.32835\n",
      "echo: 8877  loss: 0.32835\n",
      "echo: 8878  loss: 0.32835\n",
      "echo: 8879  loss: 0.32835\n",
      "echo: 8880  loss: 0.32835\n",
      "echo: 8881  loss: 0.32835\n",
      "echo: 8882  loss: 0.32835\n",
      "echo: 8883  loss: 0.32835\n",
      "echo: 8884  loss: 0.32835\n",
      "echo: 8885  loss: 0.32835\n",
      "echo: 8886  loss: 0.32835\n",
      "echo: 8887  loss: 0.32835\n",
      "echo: 8888  loss: 0.32835\n",
      "echo: 8889  loss: 0.32835\n",
      "echo: 8890  loss: 0.32835\n",
      "echo: 8891  loss: 0.32835\n",
      "echo: 8892  loss: 0.32835\n",
      "echo: 8893  loss: 0.32835\n",
      "echo: 8894  loss: 0.32835\n",
      "echo: 8895  loss: 0.32835\n",
      "echo: 8896  loss: 0.32835\n",
      "echo: 8897  loss: 0.32835\n",
      "echo: 8898  loss: 0.32835\n",
      "echo: 8899  loss: 0.32835\n",
      "echo: 8900  loss: 0.32835\n",
      "echo: 8901  loss: 0.32835\n",
      "echo: 8902  loss: 0.32835\n",
      "echo: 8903  loss: 0.32835\n",
      "echo: 8904  loss: 0.32835\n",
      "echo: 8905  loss: 0.32835\n",
      "echo: 8906  loss: 0.32835\n",
      "echo: 8907  loss: 0.32835\n",
      "echo: 8908  loss: 0.32835\n",
      "echo: 8909  loss: 0.32835\n",
      "echo: 8910  loss: 0.32835\n",
      "echo: 8911  loss: 0.32835\n",
      "echo: 8912  loss: 0.32835\n",
      "echo: 8913  loss: 0.32835\n",
      "echo: 8914  loss: 0.32835\n",
      "echo: 8915  loss: 0.32835\n",
      "echo: 8916  loss: 0.32835\n",
      "echo: 8917  loss: 0.32835\n",
      "echo: 8918  loss: 0.32835\n",
      "echo: 8919  loss: 0.32835\n",
      "echo: 8920  loss: 0.32835\n",
      "echo: 8921  loss: 0.32835\n",
      "echo: 8922  loss: 0.32835\n",
      "echo: 8923  loss: 0.32835\n",
      "echo: 8924  loss: 0.32835\n",
      "echo: 8925  loss: 0.32835\n",
      "echo: 8926  loss: 0.32835\n",
      "echo: 8927  loss: 0.32835\n",
      "echo: 8928  loss: 0.32835\n",
      "echo: 8929  loss: 0.32835\n",
      "echo: 8930  loss: 0.32835\n",
      "echo: 8931  loss: 0.32835\n",
      "echo: 8932  loss: 0.32835\n",
      "echo: 8933  loss: 0.32835\n",
      "echo: 8934  loss: 0.32835\n",
      "echo: 8935  loss: 0.32835\n",
      "echo: 8936  loss: 0.32835\n",
      "echo: 8937  loss: 0.32835\n",
      "echo: 8938  loss: 0.32835\n",
      "echo: 8939  loss: 0.32835\n",
      "echo: 8940  loss: 0.32835\n",
      "echo: 8941  loss: 0.32835\n",
      "echo: 8942  loss: 0.32835\n",
      "echo: 8943  loss: 0.32835\n",
      "echo: 8944  loss: 0.32835\n",
      "echo: 8945  loss: 0.32835\n",
      "echo: 8946  loss: 0.32835\n",
      "echo: 8947  loss: 0.32835\n",
      "echo: 8948  loss: 0.32835\n",
      "echo: 8949  loss: 0.32835\n",
      "echo: 8950  loss: 0.32835\n",
      "echo: 8951  loss: 0.32835\n",
      "echo: 8952  loss: 0.32835\n",
      "echo: 8953  loss: 0.32835\n",
      "echo: 8954  loss: 0.32835\n",
      "echo: 8955  loss: 0.32835\n",
      "echo: 8956  loss: 0.32835\n",
      "echo: 8957  loss: 0.32835\n",
      "echo: 8958  loss: 0.32835\n",
      "echo: 8959  loss: 0.32835\n",
      "echo: 8960  loss: 0.32835\n",
      "echo: 8961  loss: 0.32835\n",
      "echo: 8962  loss: 0.32835\n",
      "echo: 8963  loss: 0.32835\n",
      "echo: 8964  loss: 0.32835\n",
      "echo: 8965  loss: 0.32835\n",
      "echo: 8966  loss: 0.32835\n",
      "echo: 8967  loss: 0.32835\n",
      "echo: 8968  loss: 0.32835\n",
      "echo: 8969  loss: 0.32835\n",
      "echo: 8970  loss: 0.32835\n",
      "echo: 8971  loss: 0.32835\n",
      "echo: 8972  loss: 0.32835\n",
      "echo: 8973  loss: 0.32835\n",
      "echo: 8974  loss: 0.32835\n",
      "echo: 8975  loss: 0.32835\n",
      "echo: 8976  loss: 0.32835\n",
      "echo: 8977  loss: 0.32835\n",
      "echo: 8978  loss: 0.32835\n",
      "echo: 8979  loss: 0.32835\n",
      "echo: 8980  loss: 0.32835\n",
      "echo: 8981  loss: 0.32835\n",
      "echo: 8982  loss: 0.32835\n",
      "echo: 8983  loss: 0.32835\n",
      "echo: 8984  loss: 0.32835\n",
      "echo: 8985  loss: 0.32835\n",
      "echo: 8986  loss: 0.32835\n",
      "echo: 8987  loss: 0.32835\n",
      "echo: 8988  loss: 0.32835\n",
      "echo: 8989  loss: 0.32835\n",
      "echo: 8990  loss: 0.32835\n",
      "echo: 8991  loss: 0.32835\n",
      "echo: 8992  loss: 0.32835\n",
      "echo: 8993  loss: 0.32835\n",
      "echo: 8994  loss: 0.32835\n",
      "echo: 8995  loss: 0.32835\n",
      "echo: 8996  loss: 0.32835\n",
      "echo: 8997  loss: 0.32835\n",
      "echo: 8998  loss: 0.32835\n",
      "echo: 8999  loss: 0.32835\n",
      "echo: 9000  loss: 0.32835\n",
      "echo: 9001  loss: 0.32835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 9002  loss: 0.32835\n",
      "echo: 9003  loss: 0.32835\n",
      "echo: 9004  loss: 0.32835\n",
      "echo: 9005  loss: 0.32835\n",
      "echo: 9006  loss: 0.32835\n",
      "echo: 9007  loss: 0.32835\n",
      "echo: 9008  loss: 0.32835\n",
      "echo: 9009  loss: 0.32835\n",
      "echo: 9010  loss: 0.32835\n",
      "echo: 9011  loss: 0.32835\n",
      "echo: 9012  loss: 0.32835\n",
      "echo: 9013  loss: 0.32835\n",
      "echo: 9014  loss: 0.32835\n",
      "echo: 9015  loss: 0.32835\n",
      "echo: 9016  loss: 0.32835\n",
      "echo: 9017  loss: 0.32835\n",
      "echo: 9018  loss: 0.32835\n",
      "echo: 9019  loss: 0.32835\n",
      "echo: 9020  loss: 0.32835\n",
      "echo: 9021  loss: 0.32835\n",
      "echo: 9022  loss: 0.32835\n",
      "echo: 9023  loss: 0.32835\n",
      "echo: 9024  loss: 0.32835\n",
      "echo: 9025  loss: 0.32835\n",
      "echo: 9026  loss: 0.32835\n",
      "echo: 9027  loss: 0.32835\n",
      "echo: 9028  loss: 0.32835\n",
      "echo: 9029  loss: 0.32835\n",
      "echo: 9030  loss: 0.32835\n",
      "echo: 9031  loss: 0.32835\n",
      "echo: 9032  loss: 0.32835\n",
      "echo: 9033  loss: 0.32835\n",
      "echo: 9034  loss: 0.32835\n",
      "echo: 9035  loss: 0.32835\n",
      "echo: 9036  loss: 0.32834\n",
      "echo: 9037  loss: 0.32834\n",
      "echo: 9038  loss: 0.32834\n",
      "echo: 9039  loss: 0.32834\n",
      "echo: 9040  loss: 0.32834\n",
      "echo: 9041  loss: 0.32834\n",
      "echo: 9042  loss: 0.32834\n",
      "echo: 9043  loss: 0.32834\n",
      "echo: 9044  loss: 0.32834\n",
      "echo: 9045  loss: 0.32834\n",
      "echo: 9046  loss: 0.32834\n",
      "echo: 9047  loss: 0.32834\n",
      "echo: 9048  loss: 0.32834\n",
      "echo: 9049  loss: 0.32834\n",
      "echo: 9050  loss: 0.32834\n",
      "echo: 9051  loss: 0.32834\n",
      "echo: 9052  loss: 0.32834\n",
      "echo: 9053  loss: 0.32834\n",
      "echo: 9054  loss: 0.32834\n",
      "echo: 9055  loss: 0.32834\n",
      "echo: 9056  loss: 0.32834\n",
      "echo: 9057  loss: 0.32834\n",
      "echo: 9058  loss: 0.32834\n",
      "echo: 9059  loss: 0.32834\n",
      "echo: 9060  loss: 0.32834\n",
      "echo: 9061  loss: 0.32834\n",
      "echo: 9062  loss: 0.32834\n",
      "echo: 9063  loss: 0.32834\n",
      "echo: 9064  loss: 0.32834\n",
      "echo: 9065  loss: 0.32834\n",
      "echo: 9066  loss: 0.32834\n",
      "echo: 9067  loss: 0.32834\n",
      "echo: 9068  loss: 0.32834\n",
      "echo: 9069  loss: 0.32834\n",
      "echo: 9070  loss: 0.32834\n",
      "echo: 9071  loss: 0.32834\n",
      "echo: 9072  loss: 0.32834\n",
      "echo: 9073  loss: 0.32834\n",
      "echo: 9074  loss: 0.32834\n",
      "echo: 9075  loss: 0.32834\n",
      "echo: 9076  loss: 0.32834\n",
      "echo: 9077  loss: 0.32834\n",
      "echo: 9078  loss: 0.32834\n",
      "echo: 9079  loss: 0.32834\n",
      "echo: 9080  loss: 0.32834\n",
      "echo: 9081  loss: 0.32834\n",
      "echo: 9082  loss: 0.32834\n",
      "echo: 9083  loss: 0.32834\n",
      "echo: 9084  loss: 0.32834\n",
      "echo: 9085  loss: 0.32834\n",
      "echo: 9086  loss: 0.32834\n",
      "echo: 9087  loss: 0.32834\n",
      "echo: 9088  loss: 0.32834\n",
      "echo: 9089  loss: 0.32834\n",
      "echo: 9090  loss: 0.32834\n",
      "echo: 9091  loss: 0.32834\n",
      "echo: 9092  loss: 0.32834\n",
      "echo: 9093  loss: 0.32834\n",
      "echo: 9094  loss: 0.32834\n",
      "echo: 9095  loss: 0.32834\n",
      "echo: 9096  loss: 0.32834\n",
      "echo: 9097  loss: 0.32834\n",
      "echo: 9098  loss: 0.32834\n",
      "echo: 9099  loss: 0.32834\n",
      "echo: 9100  loss: 0.32834\n",
      "echo: 9101  loss: 0.32834\n",
      "echo: 9102  loss: 0.32834\n",
      "echo: 9103  loss: 0.32834\n",
      "echo: 9104  loss: 0.32834\n",
      "echo: 9105  loss: 0.32834\n",
      "echo: 9106  loss: 0.32834\n",
      "echo: 9107  loss: 0.32834\n",
      "echo: 9108  loss: 0.32834\n",
      "echo: 9109  loss: 0.32834\n",
      "echo: 9110  loss: 0.32834\n",
      "echo: 9111  loss: 0.32834\n",
      "echo: 9112  loss: 0.32834\n",
      "echo: 9113  loss: 0.32834\n",
      "echo: 9114  loss: 0.32834\n",
      "echo: 9115  loss: 0.32834\n",
      "echo: 9116  loss: 0.32834\n",
      "echo: 9117  loss: 0.32834\n",
      "echo: 9118  loss: 0.32834\n",
      "echo: 9119  loss: 0.32834\n",
      "echo: 9120  loss: 0.32834\n",
      "echo: 9121  loss: 0.32834\n",
      "echo: 9122  loss: 0.32834\n",
      "echo: 9123  loss: 0.32834\n",
      "echo: 9124  loss: 0.32834\n",
      "echo: 9125  loss: 0.32834\n",
      "echo: 9126  loss: 0.32834\n",
      "echo: 9127  loss: 0.32834\n",
      "echo: 9128  loss: 0.32834\n",
      "echo: 9129  loss: 0.32834\n",
      "echo: 9130  loss: 0.32834\n",
      "echo: 9131  loss: 0.32834\n",
      "echo: 9132  loss: 0.32834\n",
      "echo: 9133  loss: 0.32834\n",
      "echo: 9134  loss: 0.32834\n",
      "echo: 9135  loss: 0.32834\n",
      "echo: 9136  loss: 0.32834\n",
      "echo: 9137  loss: 0.32834\n",
      "echo: 9138  loss: 0.32834\n",
      "echo: 9139  loss: 0.32834\n",
      "echo: 9140  loss: 0.32834\n",
      "echo: 9141  loss: 0.32834\n",
      "echo: 9142  loss: 0.32834\n",
      "echo: 9143  loss: 0.32834\n",
      "echo: 9144  loss: 0.32834\n",
      "echo: 9145  loss: 0.32834\n",
      "echo: 9146  loss: 0.32834\n",
      "echo: 9147  loss: 0.32834\n",
      "echo: 9148  loss: 0.32834\n",
      "echo: 9149  loss: 0.32834\n",
      "echo: 9150  loss: 0.32834\n",
      "echo: 9151  loss: 0.32834\n",
      "echo: 9152  loss: 0.32834\n",
      "echo: 9153  loss: 0.32834\n",
      "echo: 9154  loss: 0.32834\n",
      "echo: 9155  loss: 0.32834\n",
      "echo: 9156  loss: 0.32834\n",
      "echo: 9157  loss: 0.32834\n",
      "echo: 9158  loss: 0.32834\n",
      "echo: 9159  loss: 0.32834\n",
      "echo: 9160  loss: 0.32834\n",
      "echo: 9161  loss: 0.32834\n",
      "echo: 9162  loss: 0.32834\n",
      "echo: 9163  loss: 0.32834\n",
      "echo: 9164  loss: 0.32834\n",
      "echo: 9165  loss: 0.32834\n",
      "echo: 9166  loss: 0.32834\n",
      "echo: 9167  loss: 0.32834\n",
      "echo: 9168  loss: 0.32834\n",
      "echo: 9169  loss: 0.32834\n",
      "echo: 9170  loss: 0.32834\n",
      "echo: 9171  loss: 0.32834\n",
      "echo: 9172  loss: 0.32834\n",
      "echo: 9173  loss: 0.32834\n",
      "echo: 9174  loss: 0.32834\n",
      "echo: 9175  loss: 0.32834\n",
      "echo: 9176  loss: 0.32834\n",
      "echo: 9177  loss: 0.32834\n",
      "echo: 9178  loss: 0.32834\n",
      "echo: 9179  loss: 0.32834\n",
      "echo: 9180  loss: 0.32834\n",
      "echo: 9181  loss: 0.32834\n",
      "echo: 9182  loss: 0.32834\n",
      "echo: 9183  loss: 0.32834\n",
      "echo: 9184  loss: 0.32834\n",
      "echo: 9185  loss: 0.32834\n",
      "echo: 9186  loss: 0.32834\n",
      "echo: 9187  loss: 0.32834\n",
      "echo: 9188  loss: 0.32834\n",
      "echo: 9189  loss: 0.32834\n",
      "echo: 9190  loss: 0.32834\n",
      "echo: 9191  loss: 0.32834\n",
      "echo: 9192  loss: 0.32834\n",
      "echo: 9193  loss: 0.32834\n",
      "echo: 9194  loss: 0.32834\n",
      "echo: 9195  loss: 0.32834\n",
      "echo: 9196  loss: 0.32834\n",
      "echo: 9197  loss: 0.32834\n",
      "echo: 9198  loss: 0.32834\n",
      "echo: 9199  loss: 0.32834\n",
      "echo: 9200  loss: 0.32834\n",
      "echo: 9201  loss: 0.32834\n",
      "echo: 9202  loss: 0.32834\n",
      "echo: 9203  loss: 0.32834\n",
      "echo: 9204  loss: 0.32834\n",
      "echo: 9205  loss: 0.32834\n",
      "echo: 9206  loss: 0.32834\n",
      "echo: 9207  loss: 0.32834\n",
      "echo: 9208  loss: 0.32834\n",
      "echo: 9209  loss: 0.32834\n",
      "echo: 9210  loss: 0.32834\n",
      "echo: 9211  loss: 0.32834\n",
      "echo: 9212  loss: 0.32834\n",
      "echo: 9213  loss: 0.32834\n",
      "echo: 9214  loss: 0.32834\n",
      "echo: 9215  loss: 0.32834\n",
      "echo: 9216  loss: 0.32834\n",
      "echo: 9217  loss: 0.32834\n",
      "echo: 9218  loss: 0.32834\n",
      "echo: 9219  loss: 0.32834\n",
      "echo: 9220  loss: 0.32834\n",
      "echo: 9221  loss: 0.32834\n",
      "echo: 9222  loss: 0.32834\n",
      "echo: 9223  loss: 0.32834\n",
      "echo: 9224  loss: 0.32834\n",
      "echo: 9225  loss: 0.32834\n",
      "echo: 9226  loss: 0.32834\n",
      "echo: 9227  loss: 0.32834\n",
      "echo: 9228  loss: 0.32834\n",
      "echo: 9229  loss: 0.32834\n",
      "echo: 9230  loss: 0.32834\n",
      "echo: 9231  loss: 0.32834\n",
      "echo: 9232  loss: 0.32834\n",
      "echo: 9233  loss: 0.32834\n",
      "echo: 9234  loss: 0.32834\n",
      "echo: 9235  loss: 0.32834\n",
      "echo: 9236  loss: 0.32834\n",
      "echo: 9237  loss: 0.32834\n",
      "echo: 9238  loss: 0.32834\n",
      "echo: 9239  loss: 0.32834\n",
      "echo: 9240  loss: 0.32834\n",
      "echo: 9241  loss: 0.32834\n",
      "echo: 9242  loss: 0.32834\n",
      "echo: 9243  loss: 0.32834\n",
      "echo: 9244  loss: 0.32834\n",
      "echo: 9245  loss: 0.32834\n",
      "echo: 9246  loss: 0.32834\n",
      "echo: 9247  loss: 0.32834\n",
      "echo: 9248  loss: 0.32834\n",
      "echo: 9249  loss: 0.32834\n",
      "echo: 9250  loss: 0.32834\n",
      "echo: 9251  loss: 0.32834\n",
      "echo: 9252  loss: 0.32834\n",
      "echo: 9253  loss: 0.32834\n",
      "echo: 9254  loss: 0.32834\n",
      "echo: 9255  loss: 0.32834\n",
      "echo: 9256  loss: 0.32834\n",
      "echo: 9257  loss: 0.32834\n",
      "echo: 9258  loss: 0.32834\n",
      "echo: 9259  loss: 0.32834\n",
      "echo: 9260  loss: 0.32834\n",
      "echo: 9261  loss: 0.32834\n",
      "echo: 9262  loss: 0.32834\n",
      "echo: 9263  loss: 0.32834\n",
      "echo: 9264  loss: 0.32834\n",
      "echo: 9265  loss: 0.32834\n",
      "echo: 9266  loss: 0.32834\n",
      "echo: 9267  loss: 0.32834\n",
      "echo: 9268  loss: 0.32834\n",
      "echo: 9269  loss: 0.32834\n",
      "echo: 9270  loss: 0.32834\n",
      "echo: 9271  loss: 0.32834\n",
      "echo: 9272  loss: 0.32834\n",
      "echo: 9273  loss: 0.32834\n",
      "echo: 9274  loss: 0.32834\n",
      "echo: 9275  loss: 0.32834\n",
      "echo: 9276  loss: 0.32834\n",
      "echo: 9277  loss: 0.32834\n",
      "echo: 9278  loss: 0.32834\n",
      "echo: 9279  loss: 0.32834\n",
      "echo: 9280  loss: 0.32834\n",
      "echo: 9281  loss: 0.32834\n",
      "echo: 9282  loss: 0.32834\n",
      "echo: 9283  loss: 0.32834\n",
      "echo: 9284  loss: 0.32834\n",
      "echo: 9285  loss: 0.32834\n",
      "echo: 9286  loss: 0.32834\n",
      "echo: 9287  loss: 0.32833\n",
      "echo: 9288  loss: 0.32833\n",
      "echo: 9289  loss: 0.32833\n",
      "echo: 9290  loss: 0.32833\n",
      "echo: 9291  loss: 0.32833\n",
      "echo: 9292  loss: 0.32833\n",
      "echo: 9293  loss: 0.32833\n",
      "echo: 9294  loss: 0.32833\n",
      "echo: 9295  loss: 0.32833\n",
      "echo: 9296  loss: 0.32833\n",
      "echo: 9297  loss: 0.32833\n",
      "echo: 9298  loss: 0.32833\n",
      "echo: 9299  loss: 0.32833\n",
      "echo: 9300  loss: 0.32833\n",
      "echo: 9301  loss: 0.32833\n",
      "echo: 9302  loss: 0.32833\n",
      "echo: 9303  loss: 0.32833\n",
      "echo: 9304  loss: 0.32833\n",
      "echo: 9305  loss: 0.32833\n",
      "echo: 9306  loss: 0.32833\n",
      "echo: 9307  loss: 0.32833\n",
      "echo: 9308  loss: 0.32833\n",
      "echo: 9309  loss: 0.32833\n",
      "echo: 9310  loss: 0.32833\n",
      "echo: 9311  loss: 0.32833\n",
      "echo: 9312  loss: 0.32833\n",
      "echo: 9313  loss: 0.32833\n",
      "echo: 9314  loss: 0.32833\n",
      "echo: 9315  loss: 0.32833\n",
      "echo: 9316  loss: 0.32833\n",
      "echo: 9317  loss: 0.32833\n",
      "echo: 9318  loss: 0.32833\n",
      "echo: 9319  loss: 0.32833\n",
      "echo: 9320  loss: 0.32833\n",
      "echo: 9321  loss: 0.32833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 9322  loss: 0.32833\n",
      "echo: 9323  loss: 0.32833\n",
      "echo: 9324  loss: 0.32833\n",
      "echo: 9325  loss: 0.32833\n",
      "echo: 9326  loss: 0.32833\n",
      "echo: 9327  loss: 0.32833\n",
      "echo: 9328  loss: 0.32833\n",
      "echo: 9329  loss: 0.32833\n",
      "echo: 9330  loss: 0.32833\n",
      "echo: 9331  loss: 0.32833\n",
      "echo: 9332  loss: 0.32833\n",
      "echo: 9333  loss: 0.32833\n",
      "echo: 9334  loss: 0.32833\n",
      "echo: 9335  loss: 0.32833\n",
      "echo: 9336  loss: 0.32833\n",
      "echo: 9337  loss: 0.32833\n",
      "echo: 9338  loss: 0.32833\n",
      "echo: 9339  loss: 0.32833\n",
      "echo: 9340  loss: 0.32833\n",
      "echo: 9341  loss: 0.32833\n",
      "echo: 9342  loss: 0.32833\n",
      "echo: 9343  loss: 0.32833\n",
      "echo: 9344  loss: 0.32833\n",
      "echo: 9345  loss: 0.32833\n",
      "echo: 9346  loss: 0.32833\n",
      "echo: 9347  loss: 0.32833\n",
      "echo: 9348  loss: 0.32833\n",
      "echo: 9349  loss: 0.32833\n",
      "echo: 9350  loss: 0.32833\n",
      "echo: 9351  loss: 0.32833\n",
      "echo: 9352  loss: 0.32833\n",
      "echo: 9353  loss: 0.32833\n",
      "echo: 9354  loss: 0.32833\n",
      "echo: 9355  loss: 0.32833\n",
      "echo: 9356  loss: 0.32833\n",
      "echo: 9357  loss: 0.32833\n",
      "echo: 9358  loss: 0.32833\n",
      "echo: 9359  loss: 0.32833\n",
      "echo: 9360  loss: 0.32833\n",
      "echo: 9361  loss: 0.32833\n",
      "echo: 9362  loss: 0.32833\n",
      "echo: 9363  loss: 0.32833\n",
      "echo: 9364  loss: 0.32833\n",
      "echo: 9365  loss: 0.32833\n",
      "echo: 9366  loss: 0.32833\n",
      "echo: 9367  loss: 0.32833\n",
      "echo: 9368  loss: 0.32833\n",
      "echo: 9369  loss: 0.32833\n",
      "echo: 9370  loss: 0.32833\n",
      "echo: 9371  loss: 0.32833\n",
      "echo: 9372  loss: 0.32833\n",
      "echo: 9373  loss: 0.32833\n",
      "echo: 9374  loss: 0.32833\n",
      "echo: 9375  loss: 0.32833\n",
      "echo: 9376  loss: 0.32833\n",
      "echo: 9377  loss: 0.32833\n",
      "echo: 9378  loss: 0.32833\n",
      "echo: 9379  loss: 0.32833\n",
      "echo: 9380  loss: 0.32833\n",
      "echo: 9381  loss: 0.32833\n",
      "echo: 9382  loss: 0.32833\n",
      "echo: 9383  loss: 0.32833\n",
      "echo: 9384  loss: 0.32833\n",
      "echo: 9385  loss: 0.32833\n",
      "echo: 9386  loss: 0.32833\n",
      "echo: 9387  loss: 0.32833\n",
      "echo: 9388  loss: 0.32833\n",
      "echo: 9389  loss: 0.32833\n",
      "echo: 9390  loss: 0.32833\n",
      "echo: 9391  loss: 0.32833\n",
      "echo: 9392  loss: 0.32833\n",
      "echo: 9393  loss: 0.32833\n",
      "echo: 9394  loss: 0.32833\n",
      "echo: 9395  loss: 0.32833\n",
      "echo: 9396  loss: 0.32833\n",
      "echo: 9397  loss: 0.32833\n",
      "echo: 9398  loss: 0.32833\n",
      "echo: 9399  loss: 0.32833\n",
      "echo: 9400  loss: 0.32833\n",
      "echo: 9401  loss: 0.32833\n",
      "echo: 9402  loss: 0.32833\n",
      "echo: 9403  loss: 0.32833\n",
      "echo: 9404  loss: 0.32833\n",
      "echo: 9405  loss: 0.32833\n",
      "echo: 9406  loss: 0.32833\n",
      "echo: 9407  loss: 0.32833\n",
      "echo: 9408  loss: 0.32833\n",
      "echo: 9409  loss: 0.32833\n",
      "echo: 9410  loss: 0.32833\n",
      "echo: 9411  loss: 0.32833\n",
      "echo: 9412  loss: 0.32833\n",
      "echo: 9413  loss: 0.32833\n",
      "echo: 9414  loss: 0.32833\n",
      "echo: 9415  loss: 0.32833\n",
      "echo: 9416  loss: 0.32833\n",
      "echo: 9417  loss: 0.32833\n",
      "echo: 9418  loss: 0.32833\n",
      "echo: 9419  loss: 0.32833\n",
      "echo: 9420  loss: 0.32833\n",
      "echo: 9421  loss: 0.32833\n",
      "echo: 9422  loss: 0.32833\n",
      "echo: 9423  loss: 0.32833\n",
      "echo: 9424  loss: 0.32833\n",
      "echo: 9425  loss: 0.32833\n",
      "echo: 9426  loss: 0.32833\n",
      "echo: 9427  loss: 0.32833\n",
      "echo: 9428  loss: 0.32833\n",
      "echo: 9429  loss: 0.32833\n",
      "echo: 9430  loss: 0.32833\n",
      "echo: 9431  loss: 0.32833\n",
      "echo: 9432  loss: 0.32833\n",
      "echo: 9433  loss: 0.32833\n",
      "echo: 9434  loss: 0.32833\n",
      "echo: 9435  loss: 0.32833\n",
      "echo: 9436  loss: 0.32833\n",
      "echo: 9437  loss: 0.32833\n",
      "echo: 9438  loss: 0.32833\n",
      "echo: 9439  loss: 0.32833\n",
      "echo: 9440  loss: 0.32833\n",
      "echo: 9441  loss: 0.32833\n",
      "echo: 9442  loss: 0.32833\n",
      "echo: 9443  loss: 0.32833\n",
      "echo: 9444  loss: 0.32833\n",
      "echo: 9445  loss: 0.32833\n",
      "echo: 9446  loss: 0.32833\n",
      "echo: 9447  loss: 0.32833\n",
      "echo: 9448  loss: 0.32833\n",
      "echo: 9449  loss: 0.32833\n",
      "echo: 9450  loss: 0.32833\n",
      "echo: 9451  loss: 0.32833\n",
      "echo: 9452  loss: 0.32833\n",
      "echo: 9453  loss: 0.32833\n",
      "echo: 9454  loss: 0.32833\n",
      "echo: 9455  loss: 0.32833\n",
      "echo: 9456  loss: 0.32833\n",
      "echo: 9457  loss: 0.32833\n",
      "echo: 9458  loss: 0.32833\n",
      "echo: 9459  loss: 0.32833\n",
      "echo: 9460  loss: 0.32833\n",
      "echo: 9461  loss: 0.32833\n",
      "echo: 9462  loss: 0.32833\n",
      "echo: 9463  loss: 0.32833\n",
      "echo: 9464  loss: 0.32833\n",
      "echo: 9465  loss: 0.32833\n",
      "echo: 9466  loss: 0.32833\n",
      "echo: 9467  loss: 0.32833\n",
      "echo: 9468  loss: 0.32833\n",
      "echo: 9469  loss: 0.32833\n",
      "echo: 9470  loss: 0.32833\n",
      "echo: 9471  loss: 0.32833\n",
      "echo: 9472  loss: 0.32833\n",
      "echo: 9473  loss: 0.32833\n",
      "echo: 9474  loss: 0.32833\n",
      "echo: 9475  loss: 0.32833\n",
      "echo: 9476  loss: 0.32833\n",
      "echo: 9477  loss: 0.32833\n",
      "echo: 9478  loss: 0.32833\n",
      "echo: 9479  loss: 0.32833\n",
      "echo: 9480  loss: 0.32833\n",
      "echo: 9481  loss: 0.32833\n",
      "echo: 9482  loss: 0.32833\n",
      "echo: 9483  loss: 0.32833\n",
      "echo: 9484  loss: 0.32833\n",
      "echo: 9485  loss: 0.32833\n",
      "echo: 9486  loss: 0.32833\n",
      "echo: 9487  loss: 0.32833\n",
      "echo: 9488  loss: 0.32833\n",
      "echo: 9489  loss: 0.32833\n",
      "echo: 9490  loss: 0.32833\n",
      "echo: 9491  loss: 0.32833\n",
      "echo: 9492  loss: 0.32833\n",
      "echo: 9493  loss: 0.32833\n",
      "echo: 9494  loss: 0.32833\n",
      "echo: 9495  loss: 0.32833\n",
      "echo: 9496  loss: 0.32833\n",
      "echo: 9497  loss: 0.32833\n",
      "echo: 9498  loss: 0.32833\n",
      "echo: 9499  loss: 0.32833\n",
      "echo: 9500  loss: 0.32833\n",
      "echo: 9501  loss: 0.32833\n",
      "echo: 9502  loss: 0.32833\n",
      "echo: 9503  loss: 0.32833\n",
      "echo: 9504  loss: 0.32833\n",
      "echo: 9505  loss: 0.32833\n",
      "echo: 9506  loss: 0.32833\n",
      "echo: 9507  loss: 0.32833\n",
      "echo: 9508  loss: 0.32833\n",
      "echo: 9509  loss: 0.32833\n",
      "echo: 9510  loss: 0.32833\n",
      "echo: 9511  loss: 0.32833\n",
      "echo: 9512  loss: 0.32833\n",
      "echo: 9513  loss: 0.32833\n",
      "echo: 9514  loss: 0.32833\n",
      "echo: 9515  loss: 0.32833\n",
      "echo: 9516  loss: 0.32833\n",
      "echo: 9517  loss: 0.32833\n",
      "echo: 9518  loss: 0.32833\n",
      "echo: 9519  loss: 0.32833\n",
      "echo: 9520  loss: 0.32833\n",
      "echo: 9521  loss: 0.32833\n",
      "echo: 9522  loss: 0.32833\n",
      "echo: 9523  loss: 0.32833\n",
      "echo: 9524  loss: 0.32833\n",
      "echo: 9525  loss: 0.32833\n",
      "echo: 9526  loss: 0.32833\n",
      "echo: 9527  loss: 0.32833\n",
      "echo: 9528  loss: 0.32833\n",
      "echo: 9529  loss: 0.32833\n",
      "echo: 9530  loss: 0.32833\n",
      "echo: 9531  loss: 0.32833\n",
      "echo: 9532  loss: 0.32833\n",
      "echo: 9533  loss: 0.32833\n",
      "echo: 9534  loss: 0.32833\n",
      "echo: 9535  loss: 0.32833\n",
      "echo: 9536  loss: 0.32833\n",
      "echo: 9537  loss: 0.32833\n",
      "echo: 9538  loss: 0.32833\n",
      "echo: 9539  loss: 0.32833\n",
      "echo: 9540  loss: 0.32833\n",
      "echo: 9541  loss: 0.32833\n",
      "echo: 9542  loss: 0.32833\n",
      "echo: 9543  loss: 0.32833\n",
      "echo: 9544  loss: 0.32833\n",
      "echo: 9545  loss: 0.32833\n",
      "echo: 9546  loss: 0.32833\n",
      "echo: 9547  loss: 0.32833\n",
      "echo: 9548  loss: 0.32833\n",
      "echo: 9549  loss: 0.32833\n",
      "echo: 9550  loss: 0.32833\n",
      "echo: 9551  loss: 0.32833\n",
      "echo: 9552  loss: 0.32833\n",
      "echo: 9553  loss: 0.32833\n",
      "echo: 9554  loss: 0.32833\n",
      "echo: 9555  loss: 0.32833\n",
      "echo: 9556  loss: 0.32833\n",
      "echo: 9557  loss: 0.32833\n",
      "echo: 9558  loss: 0.32833\n",
      "echo: 9559  loss: 0.32833\n",
      "echo: 9560  loss: 0.32833\n",
      "echo: 9561  loss: 0.32833\n",
      "echo: 9562  loss: 0.32833\n",
      "echo: 9563  loss: 0.32833\n",
      "echo: 9564  loss: 0.32833\n",
      "echo: 9565  loss: 0.32833\n",
      "echo: 9566  loss: 0.32833\n",
      "echo: 9567  loss: 0.32833\n",
      "echo: 9568  loss: 0.32833\n",
      "echo: 9569  loss: 0.32833\n",
      "echo: 9570  loss: 0.32833\n",
      "echo: 9571  loss: 0.32833\n",
      "echo: 9572  loss: 0.32833\n",
      "echo: 9573  loss: 0.32833\n",
      "echo: 9574  loss: 0.32833\n",
      "echo: 9575  loss: 0.32833\n",
      "echo: 9576  loss: 0.32833\n",
      "echo: 9577  loss: 0.32832\n",
      "echo: 9578  loss: 0.32832\n",
      "echo: 9579  loss: 0.32832\n",
      "echo: 9580  loss: 0.32832\n",
      "echo: 9581  loss: 0.32832\n",
      "echo: 9582  loss: 0.32832\n",
      "echo: 9583  loss: 0.32832\n",
      "echo: 9584  loss: 0.32832\n",
      "echo: 9585  loss: 0.32832\n",
      "echo: 9586  loss: 0.32832\n",
      "echo: 9587  loss: 0.32832\n",
      "echo: 9588  loss: 0.32832\n",
      "echo: 9589  loss: 0.32832\n",
      "echo: 9590  loss: 0.32832\n",
      "echo: 9591  loss: 0.32832\n",
      "echo: 9592  loss: 0.32832\n",
      "echo: 9593  loss: 0.32832\n",
      "echo: 9594  loss: 0.32832\n",
      "echo: 9595  loss: 0.32832\n",
      "echo: 9596  loss: 0.32832\n",
      "echo: 9597  loss: 0.32832\n",
      "echo: 9598  loss: 0.32832\n",
      "echo: 9599  loss: 0.32832\n",
      "echo: 9600  loss: 0.32832\n",
      "echo: 9601  loss: 0.32832\n",
      "echo: 9602  loss: 0.32832\n",
      "echo: 9603  loss: 0.32832\n",
      "echo: 9604  loss: 0.32832\n",
      "echo: 9605  loss: 0.32832\n",
      "echo: 9606  loss: 0.32832\n",
      "echo: 9607  loss: 0.32832\n",
      "echo: 9608  loss: 0.32832\n",
      "echo: 9609  loss: 0.32832\n",
      "echo: 9610  loss: 0.32832\n",
      "echo: 9611  loss: 0.32832\n",
      "echo: 9612  loss: 0.32832\n",
      "echo: 9613  loss: 0.32832\n",
      "echo: 9614  loss: 0.32832\n",
      "echo: 9615  loss: 0.32832\n",
      "echo: 9616  loss: 0.32832\n",
      "echo: 9617  loss: 0.32832\n",
      "echo: 9618  loss: 0.32832\n",
      "echo: 9619  loss: 0.32832\n",
      "echo: 9620  loss: 0.32832\n",
      "echo: 9621  loss: 0.32832\n",
      "echo: 9622  loss: 0.32832\n",
      "echo: 9623  loss: 0.32832\n",
      "echo: 9624  loss: 0.32832\n",
      "echo: 9625  loss: 0.32832\n",
      "echo: 9626  loss: 0.32832\n",
      "echo: 9627  loss: 0.32832\n",
      "echo: 9628  loss: 0.32832\n",
      "echo: 9629  loss: 0.32832\n",
      "echo: 9630  loss: 0.32832\n",
      "echo: 9631  loss: 0.32832\n",
      "echo: 9632  loss: 0.32832\n",
      "echo: 9633  loss: 0.32832\n",
      "echo: 9634  loss: 0.32832\n",
      "echo: 9635  loss: 0.32832\n",
      "echo: 9636  loss: 0.32832\n",
      "echo: 9637  loss: 0.32832\n",
      "echo: 9638  loss: 0.32832\n",
      "echo: 9639  loss: 0.32832\n",
      "echo: 9640  loss: 0.32832\n",
      "echo: 9641  loss: 0.32832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 9642  loss: 0.32832\n",
      "echo: 9643  loss: 0.32832\n",
      "echo: 9644  loss: 0.32832\n",
      "echo: 9645  loss: 0.32832\n",
      "echo: 9646  loss: 0.32832\n",
      "echo: 9647  loss: 0.32832\n",
      "echo: 9648  loss: 0.32832\n",
      "echo: 9649  loss: 0.32832\n",
      "echo: 9650  loss: 0.32832\n",
      "echo: 9651  loss: 0.32832\n",
      "echo: 9652  loss: 0.32832\n",
      "echo: 9653  loss: 0.32832\n",
      "echo: 9654  loss: 0.32832\n",
      "echo: 9655  loss: 0.32832\n",
      "echo: 9656  loss: 0.32832\n",
      "echo: 9657  loss: 0.32832\n",
      "echo: 9658  loss: 0.32832\n",
      "echo: 9659  loss: 0.32832\n",
      "echo: 9660  loss: 0.32832\n",
      "echo: 9661  loss: 0.32832\n",
      "echo: 9662  loss: 0.32832\n",
      "echo: 9663  loss: 0.32832\n",
      "echo: 9664  loss: 0.32832\n",
      "echo: 9665  loss: 0.32832\n",
      "echo: 9666  loss: 0.32832\n",
      "echo: 9667  loss: 0.32832\n",
      "echo: 9668  loss: 0.32832\n",
      "echo: 9669  loss: 0.32832\n",
      "echo: 9670  loss: 0.32832\n",
      "echo: 9671  loss: 0.32832\n",
      "echo: 9672  loss: 0.32832\n",
      "echo: 9673  loss: 0.32832\n",
      "echo: 9674  loss: 0.32832\n",
      "echo: 9675  loss: 0.32832\n",
      "echo: 9676  loss: 0.32832\n",
      "echo: 9677  loss: 0.32832\n",
      "echo: 9678  loss: 0.32832\n",
      "echo: 9679  loss: 0.32832\n",
      "echo: 9680  loss: 0.32832\n",
      "echo: 9681  loss: 0.32832\n",
      "echo: 9682  loss: 0.32832\n",
      "echo: 9683  loss: 0.32832\n",
      "echo: 9684  loss: 0.32832\n",
      "echo: 9685  loss: 0.32832\n",
      "echo: 9686  loss: 0.32832\n",
      "echo: 9687  loss: 0.32832\n",
      "echo: 9688  loss: 0.32832\n",
      "echo: 9689  loss: 0.32832\n",
      "echo: 9690  loss: 0.32832\n",
      "echo: 9691  loss: 0.32832\n",
      "echo: 9692  loss: 0.32832\n",
      "echo: 9693  loss: 0.32832\n",
      "echo: 9694  loss: 0.32832\n",
      "echo: 9695  loss: 0.32832\n",
      "echo: 9696  loss: 0.32832\n",
      "echo: 9697  loss: 0.32832\n",
      "echo: 9698  loss: 0.32832\n",
      "echo: 9699  loss: 0.32832\n",
      "echo: 9700  loss: 0.32832\n",
      "echo: 9701  loss: 0.32832\n",
      "echo: 9702  loss: 0.32832\n",
      "echo: 9703  loss: 0.32832\n",
      "echo: 9704  loss: 0.32832\n",
      "echo: 9705  loss: 0.32832\n",
      "echo: 9706  loss: 0.32832\n",
      "echo: 9707  loss: 0.32832\n",
      "echo: 9708  loss: 0.32832\n",
      "echo: 9709  loss: 0.32832\n",
      "echo: 9710  loss: 0.32832\n",
      "echo: 9711  loss: 0.32832\n",
      "echo: 9712  loss: 0.32832\n",
      "echo: 9713  loss: 0.32832\n",
      "echo: 9714  loss: 0.32832\n",
      "echo: 9715  loss: 0.32832\n",
      "echo: 9716  loss: 0.32832\n",
      "echo: 9717  loss: 0.32832\n",
      "echo: 9718  loss: 0.32832\n",
      "echo: 9719  loss: 0.32832\n",
      "echo: 9720  loss: 0.32832\n",
      "echo: 9721  loss: 0.32832\n",
      "echo: 9722  loss: 0.32832\n",
      "echo: 9723  loss: 0.32832\n",
      "echo: 9724  loss: 0.32832\n",
      "echo: 9725  loss: 0.32832\n",
      "echo: 9726  loss: 0.32832\n",
      "echo: 9727  loss: 0.32832\n",
      "echo: 9728  loss: 0.32832\n",
      "echo: 9729  loss: 0.32832\n",
      "echo: 9730  loss: 0.32832\n",
      "echo: 9731  loss: 0.32832\n",
      "echo: 9732  loss: 0.32832\n",
      "echo: 9733  loss: 0.32832\n",
      "echo: 9734  loss: 0.32832\n",
      "echo: 9735  loss: 0.32832\n",
      "echo: 9736  loss: 0.32832\n",
      "echo: 9737  loss: 0.32832\n",
      "echo: 9738  loss: 0.32832\n",
      "echo: 9739  loss: 0.32832\n",
      "echo: 9740  loss: 0.32832\n",
      "echo: 9741  loss: 0.32832\n",
      "echo: 9742  loss: 0.32832\n",
      "echo: 9743  loss: 0.32832\n",
      "echo: 9744  loss: 0.32832\n",
      "echo: 9745  loss: 0.32832\n",
      "echo: 9746  loss: 0.32832\n",
      "echo: 9747  loss: 0.32832\n",
      "echo: 9748  loss: 0.32832\n",
      "echo: 9749  loss: 0.32832\n",
      "echo: 9750  loss: 0.32832\n",
      "echo: 9751  loss: 0.32832\n",
      "echo: 9752  loss: 0.32832\n",
      "echo: 9753  loss: 0.32832\n",
      "echo: 9754  loss: 0.32832\n",
      "echo: 9755  loss: 0.32832\n",
      "echo: 9756  loss: 0.32832\n",
      "echo: 9757  loss: 0.32832\n",
      "echo: 9758  loss: 0.32832\n",
      "echo: 9759  loss: 0.32832\n",
      "echo: 9760  loss: 0.32832\n",
      "echo: 9761  loss: 0.32832\n",
      "echo: 9762  loss: 0.32832\n",
      "echo: 9763  loss: 0.32832\n",
      "echo: 9764  loss: 0.32832\n",
      "echo: 9765  loss: 0.32832\n",
      "echo: 9766  loss: 0.32832\n",
      "echo: 9767  loss: 0.32832\n",
      "echo: 9768  loss: 0.32832\n",
      "echo: 9769  loss: 0.32832\n",
      "echo: 9770  loss: 0.32832\n",
      "echo: 9771  loss: 0.32832\n",
      "echo: 9772  loss: 0.32832\n",
      "echo: 9773  loss: 0.32832\n",
      "echo: 9774  loss: 0.32832\n",
      "echo: 9775  loss: 0.32832\n",
      "echo: 9776  loss: 0.32832\n",
      "echo: 9777  loss: 0.32832\n",
      "echo: 9778  loss: 0.32832\n",
      "echo: 9779  loss: 0.32832\n",
      "echo: 9780  loss: 0.32832\n",
      "echo: 9781  loss: 0.32832\n",
      "echo: 9782  loss: 0.32832\n",
      "echo: 9783  loss: 0.32832\n",
      "echo: 9784  loss: 0.32832\n",
      "echo: 9785  loss: 0.32832\n",
      "echo: 9786  loss: 0.32832\n",
      "echo: 9787  loss: 0.32832\n",
      "echo: 9788  loss: 0.32832\n",
      "echo: 9789  loss: 0.32832\n",
      "echo: 9790  loss: 0.32832\n",
      "echo: 9791  loss: 0.32832\n",
      "echo: 9792  loss: 0.32832\n",
      "echo: 9793  loss: 0.32832\n",
      "echo: 9794  loss: 0.32832\n",
      "echo: 9795  loss: 0.32832\n",
      "echo: 9796  loss: 0.32832\n",
      "echo: 9797  loss: 0.32832\n",
      "echo: 9798  loss: 0.32832\n",
      "echo: 9799  loss: 0.32832\n",
      "echo: 9800  loss: 0.32832\n",
      "echo: 9801  loss: 0.32832\n",
      "echo: 9802  loss: 0.32832\n",
      "echo: 9803  loss: 0.32832\n",
      "echo: 9804  loss: 0.32832\n",
      "echo: 9805  loss: 0.32832\n",
      "echo: 9806  loss: 0.32832\n",
      "echo: 9807  loss: 0.32832\n",
      "echo: 9808  loss: 0.32832\n",
      "echo: 9809  loss: 0.32832\n",
      "echo: 9810  loss: 0.32832\n",
      "echo: 9811  loss: 0.32832\n",
      "echo: 9812  loss: 0.32832\n",
      "echo: 9813  loss: 0.32832\n",
      "echo: 9814  loss: 0.32832\n",
      "echo: 9815  loss: 0.32832\n",
      "echo: 9816  loss: 0.32832\n",
      "echo: 9817  loss: 0.32832\n",
      "echo: 9818  loss: 0.32832\n",
      "echo: 9819  loss: 0.32832\n",
      "echo: 9820  loss: 0.32832\n",
      "echo: 9821  loss: 0.32832\n",
      "echo: 9822  loss: 0.32832\n",
      "echo: 9823  loss: 0.32832\n",
      "echo: 9824  loss: 0.32832\n",
      "echo: 9825  loss: 0.32832\n",
      "echo: 9826  loss: 0.32832\n",
      "echo: 9827  loss: 0.32832\n",
      "echo: 9828  loss: 0.32832\n",
      "echo: 9829  loss: 0.32832\n",
      "echo: 9830  loss: 0.32832\n",
      "echo: 9831  loss: 0.32832\n",
      "echo: 9832  loss: 0.32832\n",
      "echo: 9833  loss: 0.32832\n",
      "echo: 9834  loss: 0.32832\n",
      "echo: 9835  loss: 0.32832\n",
      "echo: 9836  loss: 0.32832\n",
      "echo: 9837  loss: 0.32832\n",
      "echo: 9838  loss: 0.32832\n",
      "echo: 9839  loss: 0.32832\n",
      "echo: 9840  loss: 0.32832\n",
      "echo: 9841  loss: 0.32832\n",
      "echo: 9842  loss: 0.32832\n",
      "echo: 9843  loss: 0.32832\n",
      "echo: 9844  loss: 0.32832\n",
      "echo: 9845  loss: 0.32832\n",
      "echo: 9846  loss: 0.32832\n",
      "echo: 9847  loss: 0.32832\n",
      "echo: 9848  loss: 0.32832\n",
      "echo: 9849  loss: 0.32832\n",
      "echo: 9850  loss: 0.32832\n",
      "echo: 9851  loss: 0.32832\n",
      "echo: 9852  loss: 0.32832\n",
      "echo: 9853  loss: 0.32832\n",
      "echo: 9854  loss: 0.32832\n",
      "echo: 9855  loss: 0.32832\n",
      "echo: 9856  loss: 0.32832\n",
      "echo: 9857  loss: 0.32832\n",
      "echo: 9858  loss: 0.32832\n",
      "echo: 9859  loss: 0.32832\n",
      "echo: 9860  loss: 0.32832\n",
      "echo: 9861  loss: 0.32832\n",
      "echo: 9862  loss: 0.32832\n",
      "echo: 9863  loss: 0.32832\n",
      "echo: 9864  loss: 0.32832\n",
      "echo: 9865  loss: 0.32832\n",
      "echo: 9866  loss: 0.32832\n",
      "echo: 9867  loss: 0.32832\n",
      "echo: 9868  loss: 0.32832\n",
      "echo: 9869  loss: 0.32832\n",
      "echo: 9870  loss: 0.32832\n",
      "echo: 9871  loss: 0.32832\n",
      "echo: 9872  loss: 0.32832\n",
      "echo: 9873  loss: 0.32832\n",
      "echo: 9874  loss: 0.32832\n",
      "echo: 9875  loss: 0.32832\n",
      "echo: 9876  loss: 0.32832\n",
      "echo: 9877  loss: 0.32832\n",
      "echo: 9878  loss: 0.32832\n",
      "echo: 9879  loss: 0.32832\n",
      "echo: 9880  loss: 0.32832\n",
      "echo: 9881  loss: 0.32832\n",
      "echo: 9882  loss: 0.32832\n",
      "echo: 9883  loss: 0.32832\n",
      "echo: 9884  loss: 0.32832\n",
      "echo: 9885  loss: 0.32832\n",
      "echo: 9886  loss: 0.32832\n",
      "echo: 9887  loss: 0.32832\n",
      "echo: 9888  loss: 0.32832\n",
      "echo: 9889  loss: 0.32832\n",
      "echo: 9890  loss: 0.32832\n",
      "echo: 9891  loss: 0.32832\n",
      "echo: 9892  loss: 0.32832\n",
      "echo: 9893  loss: 0.32832\n",
      "echo: 9894  loss: 0.32832\n",
      "echo: 9895  loss: 0.32832\n",
      "echo: 9896  loss: 0.32832\n",
      "echo: 9897  loss: 0.32832\n",
      "echo: 9898  loss: 0.32832\n",
      "echo: 9899  loss: 0.32832\n",
      "echo: 9900  loss: 0.32832\n",
      "echo: 9901  loss: 0.32832\n",
      "echo: 9902  loss: 0.32832\n",
      "echo: 9903  loss: 0.32832\n",
      "echo: 9904  loss: 0.32832\n",
      "echo: 9905  loss: 0.32832\n",
      "echo: 9906  loss: 0.32832\n",
      "echo: 9907  loss: 0.32832\n",
      "echo: 9908  loss: 0.32832\n",
      "echo: 9909  loss: 0.32832\n",
      "echo: 9910  loss: 0.32832\n",
      "echo: 9911  loss: 0.32832\n",
      "echo: 9912  loss: 0.32832\n",
      "echo: 9913  loss: 0.32832\n",
      "echo: 9914  loss: 0.32832\n",
      "echo: 9915  loss: 0.32832\n",
      "echo: 9916  loss: 0.32832\n",
      "echo: 9917  loss: 0.32832\n",
      "echo: 9918  loss: 0.32831\n",
      "echo: 9919  loss: 0.32831\n",
      "echo: 9920  loss: 0.32831\n",
      "echo: 9921  loss: 0.32831\n",
      "echo: 9922  loss: 0.32831\n",
      "echo: 9923  loss: 0.32831\n",
      "echo: 9924  loss: 0.32831\n",
      "echo: 9925  loss: 0.32831\n",
      "echo: 9926  loss: 0.32831\n",
      "echo: 9927  loss: 0.32831\n",
      "echo: 9928  loss: 0.32831\n",
      "echo: 9929  loss: 0.32831\n",
      "echo: 9930  loss: 0.32831\n",
      "echo: 9931  loss: 0.32831\n",
      "echo: 9932  loss: 0.32831\n",
      "echo: 9933  loss: 0.32831\n",
      "echo: 9934  loss: 0.32831\n",
      "echo: 9935  loss: 0.32831\n",
      "echo: 9936  loss: 0.32831\n",
      "echo: 9937  loss: 0.32831\n",
      "echo: 9938  loss: 0.32831\n",
      "echo: 9939  loss: 0.32831\n",
      "echo: 9940  loss: 0.32831\n",
      "echo: 9941  loss: 0.32831\n",
      "echo: 9942  loss: 0.32831\n",
      "echo: 9943  loss: 0.32831\n",
      "echo: 9944  loss: 0.32831\n",
      "echo: 9945  loss: 0.32831\n",
      "echo: 9946  loss: 0.32831\n",
      "echo: 9947  loss: 0.32831\n",
      "echo: 9948  loss: 0.32831\n",
      "echo: 9949  loss: 0.32831\n",
      "echo: 9950  loss: 0.32831\n",
      "echo: 9951  loss: 0.32831\n",
      "echo: 9952  loss: 0.32831\n",
      "echo: 9953  loss: 0.32831\n",
      "echo: 9954  loss: 0.32831\n",
      "echo: 9955  loss: 0.32831\n",
      "echo: 9956  loss: 0.32831\n",
      "echo: 9957  loss: 0.32831\n",
      "echo: 9958  loss: 0.32831\n",
      "echo: 9959  loss: 0.32831\n",
      "echo: 9960  loss: 0.32831\n",
      "echo: 9961  loss: 0.32831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 9962  loss: 0.32831\n",
      "echo: 9963  loss: 0.32831\n",
      "echo: 9964  loss: 0.32831\n",
      "echo: 9965  loss: 0.32831\n",
      "echo: 9966  loss: 0.32831\n",
      "echo: 9967  loss: 0.32831\n",
      "echo: 9968  loss: 0.32831\n",
      "echo: 9969  loss: 0.32831\n",
      "echo: 9970  loss: 0.32831\n",
      "echo: 9971  loss: 0.32831\n",
      "echo: 9972  loss: 0.32831\n",
      "echo: 9973  loss: 0.32831\n",
      "echo: 9974  loss: 0.32831\n",
      "echo: 9975  loss: 0.32831\n",
      "echo: 9976  loss: 0.32831\n",
      "echo: 9977  loss: 0.32831\n",
      "echo: 9978  loss: 0.32831\n",
      "echo: 9979  loss: 0.32831\n",
      "echo: 9980  loss: 0.32831\n",
      "echo: 9981  loss: 0.32831\n",
      "echo: 9982  loss: 0.32831\n",
      "echo: 9983  loss: 0.32831\n",
      "echo: 9984  loss: 0.32831\n",
      "echo: 9985  loss: 0.32831\n",
      "echo: 9986  loss: 0.32831\n",
      "echo: 9987  loss: 0.32831\n",
      "echo: 9988  loss: 0.32831\n",
      "echo: 9989  loss: 0.32831\n",
      "echo: 9990  loss: 0.32831\n",
      "echo: 9991  loss: 0.32831\n",
      "echo: 9992  loss: 0.32831\n",
      "echo: 9993  loss: 0.32831\n",
      "echo: 9994  loss: 0.32831\n",
      "echo: 9995  loss: 0.32831\n",
      "echo: 9996  loss: 0.32831\n",
      "echo: 9997  loss: 0.32831\n",
      "echo: 9998  loss: 0.32831\n",
      "echo: 9999  loss: 0.32831\n",
      "echo: 10000  loss: 0.32831\n",
      "echo: 10001  loss: 0.32831\n",
      "echo: 10002  loss: 0.32831\n",
      "echo: 10003  loss: 0.32831\n",
      "echo: 10004  loss: 0.32831\n",
      "echo: 10005  loss: 0.32831\n",
      "echo: 10006  loss: 0.32831\n",
      "echo: 10007  loss: 0.32831\n",
      "echo: 10008  loss: 0.32831\n",
      "echo: 10009  loss: 0.32831\n",
      "echo: 10010  loss: 0.32831\n",
      "echo: 10011  loss: 0.32831\n",
      "echo: 10012  loss: 0.32831\n",
      "echo: 10013  loss: 0.32831\n",
      "echo: 10014  loss: 0.32831\n",
      "echo: 10015  loss: 0.32831\n",
      "echo: 10016  loss: 0.32831\n",
      "echo: 10017  loss: 0.32831\n",
      "echo: 10018  loss: 0.32831\n",
      "echo: 10019  loss: 0.32831\n",
      "echo: 10020  loss: 0.32831\n",
      "echo: 10021  loss: 0.32831\n",
      "echo: 10022  loss: 0.32831\n",
      "echo: 10023  loss: 0.32831\n",
      "echo: 10024  loss: 0.32831\n",
      "echo: 10025  loss: 0.32831\n",
      "echo: 10026  loss: 0.32831\n",
      "echo: 10027  loss: 0.32831\n",
      "echo: 10028  loss: 0.32831\n",
      "echo: 10029  loss: 0.32831\n",
      "echo: 10030  loss: 0.32831\n",
      "echo: 10031  loss: 0.32831\n",
      "echo: 10032  loss: 0.32831\n",
      "echo: 10033  loss: 0.32831\n",
      "echo: 10034  loss: 0.32831\n",
      "echo: 10035  loss: 0.32831\n",
      "echo: 10036  loss: 0.32831\n",
      "echo: 10037  loss: 0.32831\n",
      "echo: 10038  loss: 0.32831\n",
      "echo: 10039  loss: 0.32831\n",
      "echo: 10040  loss: 0.32831\n",
      "echo: 10041  loss: 0.32831\n",
      "echo: 10042  loss: 0.32831\n",
      "echo: 10043  loss: 0.32831\n",
      "echo: 10044  loss: 0.32831\n",
      "echo: 10045  loss: 0.32831\n",
      "echo: 10046  loss: 0.32831\n",
      "echo: 10047  loss: 0.32831\n",
      "echo: 10048  loss: 0.32831\n",
      "echo: 10049  loss: 0.32831\n",
      "echo: 10050  loss: 0.32831\n",
      "echo: 10051  loss: 0.32831\n",
      "echo: 10052  loss: 0.32831\n",
      "echo: 10053  loss: 0.32831\n",
      "echo: 10054  loss: 0.32831\n",
      "echo: 10055  loss: 0.32831\n",
      "echo: 10056  loss: 0.32831\n",
      "echo: 10057  loss: 0.32831\n",
      "echo: 10058  loss: 0.32831\n",
      "echo: 10059  loss: 0.32831\n",
      "echo: 10060  loss: 0.32831\n",
      "echo: 10061  loss: 0.32831\n",
      "echo: 10062  loss: 0.32831\n",
      "echo: 10063  loss: 0.32831\n",
      "echo: 10064  loss: 0.32831\n",
      "echo: 10065  loss: 0.32831\n",
      "echo: 10066  loss: 0.32831\n",
      "echo: 10067  loss: 0.32831\n",
      "echo: 10068  loss: 0.32831\n",
      "echo: 10069  loss: 0.32831\n",
      "echo: 10070  loss: 0.32831\n",
      "echo: 10071  loss: 0.32831\n",
      "echo: 10072  loss: 0.32831\n",
      "echo: 10073  loss: 0.32831\n",
      "echo: 10074  loss: 0.32831\n",
      "echo: 10075  loss: 0.32831\n",
      "echo: 10076  loss: 0.32831\n",
      "echo: 10077  loss: 0.32831\n",
      "echo: 10078  loss: 0.32831\n",
      "echo: 10079  loss: 0.32831\n",
      "echo: 10080  loss: 0.32831\n",
      "echo: 10081  loss: 0.32831\n",
      "echo: 10082  loss: 0.32831\n",
      "echo: 10083  loss: 0.32831\n",
      "echo: 10084  loss: 0.32831\n",
      "echo: 10085  loss: 0.32831\n",
      "echo: 10086  loss: 0.32831\n",
      "echo: 10087  loss: 0.32831\n",
      "echo: 10088  loss: 0.32831\n",
      "echo: 10089  loss: 0.32831\n",
      "echo: 10090  loss: 0.32831\n",
      "echo: 10091  loss: 0.32831\n",
      "echo: 10092  loss: 0.32831\n",
      "echo: 10093  loss: 0.32831\n",
      "echo: 10094  loss: 0.32831\n",
      "echo: 10095  loss: 0.32831\n",
      "echo: 10096  loss: 0.32831\n",
      "echo: 10097  loss: 0.32831\n",
      "echo: 10098  loss: 0.32831\n",
      "echo: 10099  loss: 0.32831\n",
      "echo: 10100  loss: 0.32831\n",
      "echo: 10101  loss: 0.32831\n",
      "echo: 10102  loss: 0.32831\n",
      "echo: 10103  loss: 0.32831\n",
      "echo: 10104  loss: 0.32831\n",
      "echo: 10105  loss: 0.32831\n",
      "echo: 10106  loss: 0.32831\n",
      "echo: 10107  loss: 0.32831\n",
      "echo: 10108  loss: 0.32831\n",
      "echo: 10109  loss: 0.32831\n",
      "echo: 10110  loss: 0.32831\n",
      "echo: 10111  loss: 0.32831\n",
      "echo: 10112  loss: 0.32831\n",
      "echo: 10113  loss: 0.32831\n",
      "echo: 10114  loss: 0.32831\n",
      "echo: 10115  loss: 0.32831\n",
      "echo: 10116  loss: 0.32831\n",
      "echo: 10117  loss: 0.32831\n",
      "echo: 10118  loss: 0.32831\n",
      "echo: 10119  loss: 0.32831\n",
      "echo: 10120  loss: 0.32831\n",
      "echo: 10121  loss: 0.32831\n",
      "echo: 10122  loss: 0.32831\n",
      "echo: 10123  loss: 0.32831\n",
      "echo: 10124  loss: 0.32831\n",
      "echo: 10125  loss: 0.32831\n",
      "echo: 10126  loss: 0.32831\n",
      "echo: 10127  loss: 0.32831\n",
      "echo: 10128  loss: 0.32831\n",
      "echo: 10129  loss: 0.32831\n",
      "echo: 10130  loss: 0.32831\n",
      "echo: 10131  loss: 0.32831\n",
      "echo: 10132  loss: 0.32831\n",
      "echo: 10133  loss: 0.32831\n",
      "echo: 10134  loss: 0.32831\n",
      "echo: 10135  loss: 0.32831\n",
      "echo: 10136  loss: 0.32831\n",
      "echo: 10137  loss: 0.32831\n",
      "echo: 10138  loss: 0.32831\n",
      "echo: 10139  loss: 0.32831\n",
      "echo: 10140  loss: 0.32831\n",
      "echo: 10141  loss: 0.32831\n",
      "echo: 10142  loss: 0.32831\n",
      "echo: 10143  loss: 0.32831\n",
      "echo: 10144  loss: 0.32831\n",
      "echo: 10145  loss: 0.32831\n",
      "echo: 10146  loss: 0.32831\n",
      "echo: 10147  loss: 0.32831\n",
      "echo: 10148  loss: 0.32831\n",
      "echo: 10149  loss: 0.32831\n",
      "echo: 10150  loss: 0.32831\n",
      "echo: 10151  loss: 0.32831\n",
      "echo: 10152  loss: 0.32831\n",
      "echo: 10153  loss: 0.32831\n",
      "echo: 10154  loss: 0.32831\n",
      "echo: 10155  loss: 0.32831\n",
      "echo: 10156  loss: 0.32831\n",
      "echo: 10157  loss: 0.32831\n",
      "echo: 10158  loss: 0.32831\n",
      "echo: 10159  loss: 0.32831\n",
      "echo: 10160  loss: 0.32831\n",
      "echo: 10161  loss: 0.32831\n",
      "echo: 10162  loss: 0.32831\n",
      "echo: 10163  loss: 0.32831\n",
      "echo: 10164  loss: 0.32831\n",
      "echo: 10165  loss: 0.32831\n",
      "echo: 10166  loss: 0.32831\n",
      "echo: 10167  loss: 0.32831\n",
      "echo: 10168  loss: 0.32831\n",
      "echo: 10169  loss: 0.32831\n",
      "echo: 10170  loss: 0.32831\n",
      "echo: 10171  loss: 0.32831\n",
      "echo: 10172  loss: 0.32831\n",
      "echo: 10173  loss: 0.32831\n",
      "echo: 10174  loss: 0.32831\n",
      "echo: 10175  loss: 0.32831\n",
      "echo: 10176  loss: 0.32831\n",
      "echo: 10177  loss: 0.32831\n",
      "echo: 10178  loss: 0.32831\n",
      "echo: 10179  loss: 0.32831\n",
      "echo: 10180  loss: 0.32831\n",
      "echo: 10181  loss: 0.32831\n",
      "echo: 10182  loss: 0.32831\n",
      "echo: 10183  loss: 0.32831\n",
      "echo: 10184  loss: 0.32831\n",
      "echo: 10185  loss: 0.32831\n",
      "echo: 10186  loss: 0.32831\n",
      "echo: 10187  loss: 0.32831\n",
      "echo: 10188  loss: 0.32831\n",
      "echo: 10189  loss: 0.32831\n",
      "echo: 10190  loss: 0.32831\n",
      "echo: 10191  loss: 0.32831\n",
      "echo: 10192  loss: 0.32831\n",
      "echo: 10193  loss: 0.32831\n",
      "echo: 10194  loss: 0.32831\n",
      "echo: 10195  loss: 0.32831\n",
      "echo: 10196  loss: 0.32831\n",
      "echo: 10197  loss: 0.32831\n",
      "echo: 10198  loss: 0.32831\n",
      "echo: 10199  loss: 0.32831\n",
      "echo: 10200  loss: 0.32831\n",
      "echo: 10201  loss: 0.32831\n",
      "echo: 10202  loss: 0.32831\n",
      "echo: 10203  loss: 0.32831\n",
      "echo: 10204  loss: 0.32831\n",
      "echo: 10205  loss: 0.32831\n",
      "echo: 10206  loss: 0.32831\n",
      "echo: 10207  loss: 0.32831\n",
      "echo: 10208  loss: 0.32831\n",
      "echo: 10209  loss: 0.32831\n",
      "echo: 10210  loss: 0.32831\n",
      "echo: 10211  loss: 0.32831\n",
      "echo: 10212  loss: 0.32831\n",
      "echo: 10213  loss: 0.32831\n",
      "echo: 10214  loss: 0.32831\n",
      "echo: 10215  loss: 0.32831\n",
      "echo: 10216  loss: 0.32831\n",
      "echo: 10217  loss: 0.32831\n",
      "echo: 10218  loss: 0.32831\n",
      "echo: 10219  loss: 0.32831\n",
      "echo: 10220  loss: 0.32831\n",
      "echo: 10221  loss: 0.32831\n",
      "echo: 10222  loss: 0.32831\n",
      "echo: 10223  loss: 0.32831\n",
      "echo: 10224  loss: 0.32831\n",
      "echo: 10225  loss: 0.32831\n",
      "echo: 10226  loss: 0.32831\n",
      "echo: 10227  loss: 0.32831\n",
      "echo: 10228  loss: 0.32831\n",
      "echo: 10229  loss: 0.32831\n",
      "echo: 10230  loss: 0.32831\n",
      "echo: 10231  loss: 0.32831\n",
      "echo: 10232  loss: 0.32831\n",
      "echo: 10233  loss: 0.32831\n",
      "echo: 10234  loss: 0.32831\n",
      "echo: 10235  loss: 0.32831\n",
      "echo: 10236  loss: 0.32831\n",
      "echo: 10237  loss: 0.32831\n",
      "echo: 10238  loss: 0.32831\n",
      "echo: 10239  loss: 0.32831\n",
      "echo: 10240  loss: 0.32831\n",
      "echo: 10241  loss: 0.32831\n",
      "echo: 10242  loss: 0.32831\n",
      "echo: 10243  loss: 0.32831\n",
      "echo: 10244  loss: 0.32831\n",
      "echo: 10245  loss: 0.32831\n",
      "echo: 10246  loss: 0.32831\n",
      "echo: 10247  loss: 0.32831\n",
      "echo: 10248  loss: 0.32831\n",
      "echo: 10249  loss: 0.32831\n",
      "echo: 10250  loss: 0.32831\n",
      "echo: 10251  loss: 0.32831\n",
      "echo: 10252  loss: 0.32831\n",
      "echo: 10253  loss: 0.32831\n",
      "echo: 10254  loss: 0.32831\n",
      "echo: 10255  loss: 0.32831\n",
      "echo: 10256  loss: 0.32831\n",
      "echo: 10257  loss: 0.32831\n",
      "echo: 10258  loss: 0.32831\n",
      "echo: 10259  loss: 0.32831\n",
      "echo: 10260  loss: 0.32831\n",
      "echo: 10261  loss: 0.32831\n",
      "echo: 10262  loss: 0.32831\n",
      "echo: 10263  loss: 0.32831\n",
      "echo: 10264  loss: 0.32831\n",
      "echo: 10265  loss: 0.32831\n",
      "echo: 10266  loss: 0.32831\n",
      "echo: 10267  loss: 0.32831\n",
      "echo: 10268  loss: 0.32831\n",
      "echo: 10269  loss: 0.32831\n",
      "echo: 10270  loss: 0.32831\n",
      "echo: 10271  loss: 0.32831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 10272  loss: 0.32831\n",
      "echo: 10273  loss: 0.32831\n",
      "echo: 10274  loss: 0.32831\n",
      "echo: 10275  loss: 0.32831\n",
      "echo: 10276  loss: 0.32831\n",
      "echo: 10277  loss: 0.32831\n",
      "echo: 10278  loss: 0.32831\n",
      "echo: 10279  loss: 0.32831\n",
      "echo: 10280  loss: 0.32831\n",
      "echo: 10281  loss: 0.32831\n",
      "echo: 10282  loss: 0.32831\n",
      "echo: 10283  loss: 0.32831\n",
      "echo: 10284  loss: 0.32831\n",
      "echo: 10285  loss: 0.32831\n",
      "echo: 10286  loss: 0.32831\n",
      "echo: 10287  loss: 0.32831\n",
      "echo: 10288  loss: 0.32831\n",
      "echo: 10289  loss: 0.32831\n",
      "echo: 10290  loss: 0.32831\n",
      "echo: 10291  loss: 0.32831\n",
      "echo: 10292  loss: 0.32831\n",
      "echo: 10293  loss: 0.32831\n",
      "echo: 10294  loss: 0.32831\n",
      "echo: 10295  loss: 0.32831\n",
      "echo: 10296  loss: 0.32831\n",
      "echo: 10297  loss: 0.32831\n",
      "echo: 10298  loss: 0.32831\n",
      "echo: 10299  loss: 0.32831\n",
      "echo: 10300  loss: 0.32831\n",
      "echo: 10301  loss: 0.32831\n",
      "echo: 10302  loss: 0.32831\n",
      "echo: 10303  loss: 0.32831\n",
      "echo: 10304  loss: 0.32831\n",
      "echo: 10305  loss: 0.32831\n",
      "echo: 10306  loss: 0.32831\n",
      "echo: 10307  loss: 0.32831\n",
      "echo: 10308  loss: 0.32831\n",
      "echo: 10309  loss: 0.32831\n",
      "echo: 10310  loss: 0.32831\n",
      "echo: 10311  loss: 0.32831\n",
      "echo: 10312  loss: 0.32831\n",
      "echo: 10313  loss: 0.32831\n",
      "echo: 10314  loss: 0.32831\n",
      "echo: 10315  loss: 0.32831\n",
      "echo: 10316  loss: 0.32831\n",
      "echo: 10317  loss: 0.32831\n",
      "echo: 10318  loss: 0.32831\n",
      "echo: 10319  loss: 0.32831\n",
      "echo: 10320  loss: 0.32831\n",
      "echo: 10321  loss: 0.32831\n",
      "echo: 10322  loss: 0.32831\n",
      "echo: 10323  loss: 0.32831\n",
      "echo: 10324  loss: 0.32831\n",
      "echo: 10325  loss: 0.32831\n",
      "echo: 10326  loss: 0.32831\n",
      "echo: 10327  loss: 0.32831\n",
      "echo: 10328  loss: 0.32831\n",
      "echo: 10329  loss: 0.32831\n",
      "echo: 10330  loss: 0.32831\n",
      "echo: 10331  loss: 0.32831\n",
      "echo: 10332  loss: 0.3283\n",
      "echo: 10333  loss: 0.3283\n",
      "echo: 10334  loss: 0.3283\n",
      "echo: 10335  loss: 0.3283\n",
      "echo: 10336  loss: 0.3283\n",
      "echo: 10337  loss: 0.3283\n",
      "echo: 10338  loss: 0.3283\n",
      "echo: 10339  loss: 0.3283\n",
      "echo: 10340  loss: 0.3283\n",
      "echo: 10341  loss: 0.3283\n",
      "echo: 10342  loss: 0.3283\n",
      "echo: 10343  loss: 0.3283\n",
      "echo: 10344  loss: 0.3283\n",
      "echo: 10345  loss: 0.3283\n",
      "echo: 10346  loss: 0.3283\n",
      "echo: 10347  loss: 0.3283\n",
      "echo: 10348  loss: 0.3283\n",
      "echo: 10349  loss: 0.3283\n",
      "echo: 10350  loss: 0.3283\n",
      "echo: 10351  loss: 0.3283\n",
      "echo: 10352  loss: 0.3283\n",
      "echo: 10353  loss: 0.3283\n",
      "echo: 10354  loss: 0.3283\n",
      "echo: 10355  loss: 0.3283\n",
      "echo: 10356  loss: 0.3283\n",
      "echo: 10357  loss: 0.3283\n",
      "echo: 10358  loss: 0.3283\n",
      "echo: 10359  loss: 0.3283\n",
      "echo: 10360  loss: 0.3283\n",
      "echo: 10361  loss: 0.3283\n",
      "echo: 10362  loss: 0.3283\n",
      "echo: 10363  loss: 0.3283\n",
      "echo: 10364  loss: 0.3283\n",
      "echo: 10365  loss: 0.3283\n",
      "echo: 10366  loss: 0.3283\n",
      "echo: 10367  loss: 0.3283\n",
      "echo: 10368  loss: 0.3283\n",
      "echo: 10369  loss: 0.3283\n",
      "echo: 10370  loss: 0.3283\n",
      "echo: 10371  loss: 0.3283\n",
      "echo: 10372  loss: 0.3283\n",
      "echo: 10373  loss: 0.3283\n",
      "echo: 10374  loss: 0.3283\n",
      "echo: 10375  loss: 0.3283\n",
      "echo: 10376  loss: 0.3283\n",
      "echo: 10377  loss: 0.3283\n",
      "echo: 10378  loss: 0.3283\n",
      "echo: 10379  loss: 0.3283\n",
      "echo: 10380  loss: 0.3283\n",
      "echo: 10381  loss: 0.3283\n",
      "echo: 10382  loss: 0.3283\n",
      "echo: 10383  loss: 0.3283\n",
      "echo: 10384  loss: 0.3283\n",
      "echo: 10385  loss: 0.3283\n",
      "echo: 10386  loss: 0.3283\n",
      "echo: 10387  loss: 0.3283\n",
      "echo: 10388  loss: 0.3283\n",
      "echo: 10389  loss: 0.3283\n",
      "echo: 10390  loss: 0.3283\n",
      "echo: 10391  loss: 0.3283\n",
      "echo: 10392  loss: 0.3283\n",
      "echo: 10393  loss: 0.3283\n",
      "echo: 10394  loss: 0.3283\n",
      "echo: 10395  loss: 0.3283\n",
      "echo: 10396  loss: 0.3283\n",
      "echo: 10397  loss: 0.3283\n",
      "echo: 10398  loss: 0.3283\n",
      "echo: 10399  loss: 0.3283\n",
      "echo: 10400  loss: 0.3283\n",
      "echo: 10401  loss: 0.3283\n",
      "echo: 10402  loss: 0.3283\n",
      "echo: 10403  loss: 0.3283\n",
      "echo: 10404  loss: 0.3283\n",
      "echo: 10405  loss: 0.3283\n",
      "echo: 10406  loss: 0.3283\n",
      "echo: 10407  loss: 0.3283\n",
      "echo: 10408  loss: 0.3283\n",
      "echo: 10409  loss: 0.3283\n",
      "echo: 10410  loss: 0.3283\n",
      "echo: 10411  loss: 0.3283\n",
      "echo: 10412  loss: 0.3283\n",
      "echo: 10413  loss: 0.3283\n",
      "echo: 10414  loss: 0.3283\n",
      "echo: 10415  loss: 0.3283\n",
      "echo: 10416  loss: 0.3283\n",
      "echo: 10417  loss: 0.3283\n",
      "echo: 10418  loss: 0.3283\n",
      "echo: 10419  loss: 0.3283\n",
      "echo: 10420  loss: 0.3283\n",
      "echo: 10421  loss: 0.3283\n",
      "echo: 10422  loss: 0.3283\n",
      "echo: 10423  loss: 0.3283\n",
      "echo: 10424  loss: 0.3283\n",
      "echo: 10425  loss: 0.3283\n",
      "echo: 10426  loss: 0.3283\n",
      "echo: 10427  loss: 0.3283\n",
      "echo: 10428  loss: 0.3283\n",
      "echo: 10429  loss: 0.3283\n",
      "echo: 10430  loss: 0.3283\n",
      "echo: 10431  loss: 0.3283\n",
      "echo: 10432  loss: 0.3283\n",
      "echo: 10433  loss: 0.3283\n",
      "echo: 10434  loss: 0.3283\n",
      "echo: 10435  loss: 0.3283\n",
      "echo: 10436  loss: 0.3283\n",
      "echo: 10437  loss: 0.3283\n",
      "echo: 10438  loss: 0.3283\n",
      "echo: 10439  loss: 0.3283\n",
      "echo: 10440  loss: 0.3283\n",
      "echo: 10441  loss: 0.3283\n",
      "echo: 10442  loss: 0.3283\n",
      "echo: 10443  loss: 0.3283\n",
      "echo: 10444  loss: 0.3283\n",
      "echo: 10445  loss: 0.3283\n",
      "echo: 10446  loss: 0.3283\n",
      "echo: 10447  loss: 0.3283\n",
      "echo: 10448  loss: 0.3283\n",
      "echo: 10449  loss: 0.3283\n",
      "echo: 10450  loss: 0.3283\n",
      "echo: 10451  loss: 0.3283\n",
      "echo: 10452  loss: 0.3283\n",
      "echo: 10453  loss: 0.3283\n",
      "echo: 10454  loss: 0.3283\n",
      "echo: 10455  loss: 0.3283\n",
      "echo: 10456  loss: 0.3283\n",
      "echo: 10457  loss: 0.3283\n",
      "echo: 10458  loss: 0.3283\n",
      "echo: 10459  loss: 0.3283\n",
      "echo: 10460  loss: 0.3283\n",
      "echo: 10461  loss: 0.3283\n",
      "echo: 10462  loss: 0.3283\n",
      "echo: 10463  loss: 0.3283\n",
      "echo: 10464  loss: 0.3283\n",
      "echo: 10465  loss: 0.3283\n",
      "echo: 10466  loss: 0.3283\n",
      "echo: 10467  loss: 0.3283\n",
      "echo: 10468  loss: 0.3283\n",
      "echo: 10469  loss: 0.3283\n",
      "echo: 10470  loss: 0.3283\n",
      "echo: 10471  loss: 0.3283\n",
      "echo: 10472  loss: 0.3283\n",
      "echo: 10473  loss: 0.3283\n",
      "echo: 10474  loss: 0.3283\n",
      "echo: 10475  loss: 0.3283\n",
      "echo: 10476  loss: 0.3283\n",
      "echo: 10477  loss: 0.3283\n",
      "echo: 10478  loss: 0.3283\n",
      "echo: 10479  loss: 0.3283\n",
      "echo: 10480  loss: 0.3283\n",
      "echo: 10481  loss: 0.3283\n",
      "echo: 10482  loss: 0.3283\n",
      "echo: 10483  loss: 0.3283\n",
      "echo: 10484  loss: 0.3283\n",
      "echo: 10485  loss: 0.3283\n",
      "echo: 10486  loss: 0.3283\n",
      "echo: 10487  loss: 0.3283\n",
      "echo: 10488  loss: 0.3283\n",
      "echo: 10489  loss: 0.3283\n",
      "echo: 10490  loss: 0.3283\n",
      "echo: 10491  loss: 0.3283\n",
      "echo: 10492  loss: 0.3283\n",
      "echo: 10493  loss: 0.3283\n",
      "echo: 10494  loss: 0.3283\n",
      "echo: 10495  loss: 0.3283\n",
      "echo: 10496  loss: 0.3283\n",
      "echo: 10497  loss: 0.3283\n",
      "echo: 10498  loss: 0.3283\n",
      "echo: 10499  loss: 0.3283\n",
      "echo: 10500  loss: 0.3283\n",
      "echo: 10501  loss: 0.3283\n",
      "echo: 10502  loss: 0.3283\n",
      "echo: 10503  loss: 0.3283\n",
      "echo: 10504  loss: 0.3283\n",
      "echo: 10505  loss: 0.3283\n",
      "echo: 10506  loss: 0.3283\n",
      "echo: 10507  loss: 0.3283\n",
      "echo: 10508  loss: 0.3283\n",
      "echo: 10509  loss: 0.3283\n",
      "echo: 10510  loss: 0.3283\n",
      "echo: 10511  loss: 0.3283\n",
      "echo: 10512  loss: 0.3283\n",
      "echo: 10513  loss: 0.3283\n",
      "echo: 10514  loss: 0.3283\n",
      "echo: 10515  loss: 0.3283\n",
      "echo: 10516  loss: 0.3283\n",
      "echo: 10517  loss: 0.3283\n",
      "echo: 10518  loss: 0.3283\n",
      "echo: 10519  loss: 0.3283\n",
      "echo: 10520  loss: 0.3283\n",
      "echo: 10521  loss: 0.3283\n",
      "echo: 10522  loss: 0.3283\n",
      "echo: 10523  loss: 0.3283\n",
      "echo: 10524  loss: 0.3283\n",
      "echo: 10525  loss: 0.3283\n",
      "echo: 10526  loss: 0.3283\n",
      "echo: 10527  loss: 0.3283\n",
      "echo: 10528  loss: 0.3283\n",
      "echo: 10529  loss: 0.3283\n",
      "echo: 10530  loss: 0.3283\n",
      "echo: 10531  loss: 0.3283\n",
      "echo: 10532  loss: 0.3283\n",
      "echo: 10533  loss: 0.3283\n",
      "echo: 10534  loss: 0.3283\n",
      "echo: 10535  loss: 0.3283\n",
      "echo: 10536  loss: 0.3283\n",
      "echo: 10537  loss: 0.3283\n",
      "echo: 10538  loss: 0.3283\n",
      "echo: 10539  loss: 0.3283\n",
      "echo: 10540  loss: 0.3283\n",
      "echo: 10541  loss: 0.3283\n",
      "echo: 10542  loss: 0.3283\n",
      "echo: 10543  loss: 0.3283\n",
      "echo: 10544  loss: 0.3283\n",
      "echo: 10545  loss: 0.3283\n",
      "echo: 10546  loss: 0.3283\n",
      "echo: 10547  loss: 0.3283\n",
      "echo: 10548  loss: 0.3283\n",
      "echo: 10549  loss: 0.3283\n",
      "echo: 10550  loss: 0.3283\n",
      "echo: 10551  loss: 0.3283\n",
      "echo: 10552  loss: 0.3283\n",
      "echo: 10553  loss: 0.3283\n",
      "echo: 10554  loss: 0.3283\n",
      "echo: 10555  loss: 0.3283\n",
      "echo: 10556  loss: 0.3283\n",
      "echo: 10557  loss: 0.3283\n",
      "echo: 10558  loss: 0.3283\n",
      "echo: 10559  loss: 0.3283\n",
      "echo: 10560  loss: 0.3283\n",
      "echo: 10561  loss: 0.3283\n",
      "echo: 10562  loss: 0.3283\n",
      "echo: 10563  loss: 0.3283\n",
      "echo: 10564  loss: 0.3283\n",
      "echo: 10565  loss: 0.3283\n",
      "echo: 10566  loss: 0.3283\n",
      "echo: 10567  loss: 0.3283\n",
      "echo: 10568  loss: 0.3283\n",
      "echo: 10569  loss: 0.3283\n",
      "echo: 10570  loss: 0.3283\n",
      "echo: 10571  loss: 0.3283\n",
      "echo: 10572  loss: 0.3283\n",
      "echo: 10573  loss: 0.3283\n",
      "echo: 10574  loss: 0.3283\n",
      "echo: 10575  loss: 0.3283\n",
      "echo: 10576  loss: 0.3283\n",
      "echo: 10577  loss: 0.3283\n",
      "echo: 10578  loss: 0.3283\n",
      "echo: 10579  loss: 0.3283\n",
      "echo: 10580  loss: 0.3283\n",
      "echo: 10581  loss: 0.3283\n",
      "echo: 10582  loss: 0.3283\n",
      "echo: 10583  loss: 0.3283\n",
      "echo: 10584  loss: 0.3283\n",
      "echo: 10585  loss: 0.3283\n",
      "echo: 10586  loss: 0.3283\n",
      "echo: 10587  loss: 0.3283\n",
      "echo: 10588  loss: 0.3283\n",
      "echo: 10589  loss: 0.3283\n",
      "echo: 10590  loss: 0.3283\n",
      "echo: 10591  loss: 0.3283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 10592  loss: 0.3283\n",
      "echo: 10593  loss: 0.3283\n",
      "echo: 10594  loss: 0.3283\n",
      "echo: 10595  loss: 0.3283\n",
      "echo: 10596  loss: 0.3283\n",
      "echo: 10597  loss: 0.3283\n",
      "echo: 10598  loss: 0.3283\n",
      "echo: 10599  loss: 0.3283\n",
      "echo: 10600  loss: 0.3283\n",
      "echo: 10601  loss: 0.3283\n",
      "echo: 10602  loss: 0.3283\n",
      "echo: 10603  loss: 0.3283\n",
      "echo: 10604  loss: 0.3283\n",
      "echo: 10605  loss: 0.3283\n",
      "echo: 10606  loss: 0.3283\n",
      "echo: 10607  loss: 0.3283\n",
      "echo: 10608  loss: 0.3283\n",
      "echo: 10609  loss: 0.3283\n",
      "echo: 10610  loss: 0.3283\n",
      "echo: 10611  loss: 0.3283\n",
      "echo: 10612  loss: 0.3283\n",
      "echo: 10613  loss: 0.3283\n",
      "echo: 10614  loss: 0.3283\n",
      "echo: 10615  loss: 0.3283\n",
      "echo: 10616  loss: 0.3283\n",
      "echo: 10617  loss: 0.3283\n",
      "echo: 10618  loss: 0.3283\n",
      "echo: 10619  loss: 0.3283\n",
      "echo: 10620  loss: 0.3283\n",
      "echo: 10621  loss: 0.3283\n",
      "echo: 10622  loss: 0.3283\n",
      "echo: 10623  loss: 0.3283\n",
      "echo: 10624  loss: 0.3283\n",
      "echo: 10625  loss: 0.3283\n",
      "echo: 10626  loss: 0.3283\n",
      "echo: 10627  loss: 0.3283\n",
      "echo: 10628  loss: 0.3283\n",
      "echo: 10629  loss: 0.3283\n",
      "echo: 10630  loss: 0.3283\n",
      "echo: 10631  loss: 0.3283\n",
      "echo: 10632  loss: 0.3283\n",
      "echo: 10633  loss: 0.3283\n",
      "echo: 10634  loss: 0.3283\n",
      "echo: 10635  loss: 0.3283\n",
      "echo: 10636  loss: 0.3283\n",
      "echo: 10637  loss: 0.3283\n",
      "echo: 10638  loss: 0.3283\n",
      "echo: 10639  loss: 0.3283\n",
      "echo: 10640  loss: 0.3283\n",
      "echo: 10641  loss: 0.3283\n",
      "echo: 10642  loss: 0.3283\n",
      "echo: 10643  loss: 0.3283\n",
      "echo: 10644  loss: 0.3283\n",
      "echo: 10645  loss: 0.3283\n",
      "echo: 10646  loss: 0.3283\n",
      "echo: 10647  loss: 0.3283\n",
      "echo: 10648  loss: 0.3283\n",
      "echo: 10649  loss: 0.3283\n",
      "echo: 10650  loss: 0.3283\n",
      "echo: 10651  loss: 0.3283\n",
      "echo: 10652  loss: 0.3283\n",
      "echo: 10653  loss: 0.3283\n",
      "echo: 10654  loss: 0.3283\n",
      "echo: 10655  loss: 0.3283\n",
      "echo: 10656  loss: 0.3283\n",
      "echo: 10657  loss: 0.3283\n",
      "echo: 10658  loss: 0.3283\n",
      "echo: 10659  loss: 0.3283\n",
      "echo: 10660  loss: 0.3283\n",
      "echo: 10661  loss: 0.3283\n",
      "echo: 10662  loss: 0.3283\n",
      "echo: 10663  loss: 0.3283\n",
      "echo: 10664  loss: 0.3283\n",
      "echo: 10665  loss: 0.3283\n",
      "echo: 10666  loss: 0.3283\n",
      "echo: 10667  loss: 0.3283\n",
      "echo: 10668  loss: 0.3283\n",
      "echo: 10669  loss: 0.3283\n",
      "echo: 10670  loss: 0.3283\n",
      "echo: 10671  loss: 0.3283\n",
      "echo: 10672  loss: 0.3283\n",
      "echo: 10673  loss: 0.3283\n",
      "echo: 10674  loss: 0.3283\n",
      "echo: 10675  loss: 0.3283\n",
      "echo: 10676  loss: 0.3283\n",
      "echo: 10677  loss: 0.3283\n",
      "echo: 10678  loss: 0.3283\n",
      "echo: 10679  loss: 0.3283\n",
      "echo: 10680  loss: 0.3283\n",
      "echo: 10681  loss: 0.3283\n",
      "echo: 10682  loss: 0.3283\n",
      "echo: 10683  loss: 0.3283\n",
      "echo: 10684  loss: 0.3283\n",
      "echo: 10685  loss: 0.3283\n",
      "echo: 10686  loss: 0.3283\n",
      "echo: 10687  loss: 0.3283\n",
      "echo: 10688  loss: 0.3283\n",
      "echo: 10689  loss: 0.3283\n",
      "echo: 10690  loss: 0.3283\n",
      "echo: 10691  loss: 0.3283\n",
      "echo: 10692  loss: 0.3283\n",
      "echo: 10693  loss: 0.3283\n",
      "echo: 10694  loss: 0.3283\n",
      "echo: 10695  loss: 0.3283\n",
      "echo: 10696  loss: 0.3283\n",
      "echo: 10697  loss: 0.3283\n",
      "echo: 10698  loss: 0.3283\n",
      "echo: 10699  loss: 0.3283\n",
      "echo: 10700  loss: 0.3283\n",
      "echo: 10701  loss: 0.3283\n",
      "echo: 10702  loss: 0.3283\n",
      "echo: 10703  loss: 0.3283\n",
      "echo: 10704  loss: 0.3283\n",
      "echo: 10705  loss: 0.3283\n",
      "echo: 10706  loss: 0.3283\n",
      "echo: 10707  loss: 0.3283\n",
      "echo: 10708  loss: 0.3283\n",
      "echo: 10709  loss: 0.3283\n",
      "echo: 10710  loss: 0.3283\n",
      "echo: 10711  loss: 0.3283\n",
      "echo: 10712  loss: 0.3283\n",
      "echo: 10713  loss: 0.3283\n",
      "echo: 10714  loss: 0.3283\n",
      "echo: 10715  loss: 0.3283\n",
      "echo: 10716  loss: 0.3283\n",
      "echo: 10717  loss: 0.3283\n",
      "echo: 10718  loss: 0.3283\n",
      "echo: 10719  loss: 0.3283\n",
      "echo: 10720  loss: 0.3283\n",
      "echo: 10721  loss: 0.3283\n",
      "echo: 10722  loss: 0.3283\n",
      "echo: 10723  loss: 0.3283\n",
      "echo: 10724  loss: 0.3283\n",
      "echo: 10725  loss: 0.3283\n",
      "echo: 10726  loss: 0.3283\n",
      "echo: 10727  loss: 0.3283\n",
      "echo: 10728  loss: 0.3283\n",
      "echo: 10729  loss: 0.3283\n",
      "echo: 10730  loss: 0.3283\n",
      "echo: 10731  loss: 0.3283\n",
      "echo: 10732  loss: 0.3283\n",
      "echo: 10733  loss: 0.3283\n",
      "echo: 10734  loss: 0.3283\n",
      "echo: 10735  loss: 0.3283\n",
      "echo: 10736  loss: 0.3283\n",
      "echo: 10737  loss: 0.3283\n",
      "echo: 10738  loss: 0.3283\n",
      "echo: 10739  loss: 0.3283\n",
      "echo: 10740  loss: 0.3283\n",
      "echo: 10741  loss: 0.3283\n",
      "echo: 10742  loss: 0.3283\n",
      "echo: 10743  loss: 0.3283\n",
      "echo: 10744  loss: 0.3283\n",
      "echo: 10745  loss: 0.3283\n",
      "echo: 10746  loss: 0.3283\n",
      "echo: 10747  loss: 0.3283\n",
      "echo: 10748  loss: 0.3283\n",
      "echo: 10749  loss: 0.3283\n",
      "echo: 10750  loss: 0.3283\n",
      "echo: 10751  loss: 0.3283\n",
      "echo: 10752  loss: 0.3283\n",
      "echo: 10753  loss: 0.3283\n",
      "echo: 10754  loss: 0.3283\n",
      "echo: 10755  loss: 0.3283\n",
      "echo: 10756  loss: 0.3283\n",
      "echo: 10757  loss: 0.3283\n",
      "echo: 10758  loss: 0.3283\n",
      "echo: 10759  loss: 0.3283\n",
      "echo: 10760  loss: 0.3283\n",
      "echo: 10761  loss: 0.3283\n",
      "echo: 10762  loss: 0.3283\n",
      "echo: 10763  loss: 0.3283\n",
      "echo: 10764  loss: 0.3283\n",
      "echo: 10765  loss: 0.3283\n",
      "echo: 10766  loss: 0.3283\n",
      "echo: 10767  loss: 0.3283\n",
      "echo: 10768  loss: 0.3283\n",
      "echo: 10769  loss: 0.3283\n",
      "echo: 10770  loss: 0.3283\n",
      "echo: 10771  loss: 0.3283\n",
      "echo: 10772  loss: 0.3283\n",
      "echo: 10773  loss: 0.3283\n",
      "echo: 10774  loss: 0.3283\n",
      "echo: 10775  loss: 0.3283\n",
      "echo: 10776  loss: 0.3283\n",
      "echo: 10777  loss: 0.3283\n",
      "echo: 10778  loss: 0.3283\n",
      "echo: 10779  loss: 0.3283\n",
      "echo: 10780  loss: 0.3283\n",
      "echo: 10781  loss: 0.3283\n",
      "echo: 10782  loss: 0.3283\n",
      "echo: 10783  loss: 0.3283\n",
      "echo: 10784  loss: 0.3283\n",
      "echo: 10785  loss: 0.3283\n",
      "echo: 10786  loss: 0.3283\n",
      "echo: 10787  loss: 0.3283\n",
      "echo: 10788  loss: 0.3283\n",
      "echo: 10789  loss: 0.3283\n",
      "echo: 10790  loss: 0.3283\n",
      "echo: 10791  loss: 0.3283\n",
      "echo: 10792  loss: 0.3283\n",
      "echo: 10793  loss: 0.3283\n",
      "echo: 10794  loss: 0.3283\n",
      "echo: 10795  loss: 0.3283\n",
      "echo: 10796  loss: 0.3283\n",
      "echo: 10797  loss: 0.3283\n",
      "echo: 10798  loss: 0.3283\n",
      "echo: 10799  loss: 0.3283\n",
      "echo: 10800  loss: 0.3283\n",
      "echo: 10801  loss: 0.3283\n",
      "echo: 10802  loss: 0.3283\n",
      "echo: 10803  loss: 0.3283\n",
      "echo: 10804  loss: 0.3283\n",
      "echo: 10805  loss: 0.3283\n",
      "echo: 10806  loss: 0.3283\n",
      "echo: 10807  loss: 0.3283\n",
      "echo: 10808  loss: 0.3283\n",
      "echo: 10809  loss: 0.3283\n",
      "echo: 10810  loss: 0.3283\n",
      "echo: 10811  loss: 0.3283\n",
      "echo: 10812  loss: 0.3283\n",
      "echo: 10813  loss: 0.3283\n",
      "echo: 10814  loss: 0.3283\n",
      "echo: 10815  loss: 0.3283\n",
      "echo: 10816  loss: 0.3283\n",
      "echo: 10817  loss: 0.3283\n",
      "echo: 10818  loss: 0.3283\n",
      "echo: 10819  loss: 0.3283\n",
      "echo: 10820  loss: 0.3283\n",
      "echo: 10821  loss: 0.3283\n",
      "echo: 10822  loss: 0.3283\n",
      "echo: 10823  loss: 0.3283\n",
      "echo: 10824  loss: 0.3283\n",
      "echo: 10825  loss: 0.3283\n",
      "echo: 10826  loss: 0.3283\n",
      "echo: 10827  loss: 0.3283\n",
      "echo: 10828  loss: 0.3283\n",
      "echo: 10829  loss: 0.3283\n",
      "echo: 10830  loss: 0.3283\n",
      "echo: 10831  loss: 0.3283\n",
      "echo: 10832  loss: 0.3283\n",
      "echo: 10833  loss: 0.3283\n",
      "echo: 10834  loss: 0.3283\n",
      "echo: 10835  loss: 0.3283\n",
      "echo: 10836  loss: 0.3283\n",
      "echo: 10837  loss: 0.3283\n",
      "echo: 10838  loss: 0.3283\n",
      "echo: 10839  loss: 0.3283\n",
      "echo: 10840  loss: 0.3283\n",
      "echo: 10841  loss: 0.3283\n",
      "echo: 10842  loss: 0.3283\n",
      "echo: 10843  loss: 0.3283\n",
      "echo: 10844  loss: 0.3283\n",
      "echo: 10845  loss: 0.3283\n",
      "echo: 10846  loss: 0.3283\n",
      "echo: 10847  loss: 0.3283\n",
      "echo: 10848  loss: 0.3283\n",
      "echo: 10849  loss: 0.3283\n",
      "echo: 10850  loss: 0.3283\n",
      "echo: 10851  loss: 0.3283\n",
      "echo: 10852  loss: 0.3283\n",
      "echo: 10853  loss: 0.3283\n",
      "echo: 10854  loss: 0.3283\n",
      "echo: 10855  loss: 0.32829\n",
      "echo: 10856  loss: 0.32829\n",
      "echo: 10857  loss: 0.32829\n",
      "echo: 10858  loss: 0.32829\n",
      "echo: 10859  loss: 0.32829\n",
      "echo: 10860  loss: 0.32829\n",
      "echo: 10861  loss: 0.32829\n",
      "echo: 10862  loss: 0.32829\n",
      "echo: 10863  loss: 0.32829\n",
      "echo: 10864  loss: 0.32829\n",
      "echo: 10865  loss: 0.32829\n",
      "echo: 10866  loss: 0.32829\n",
      "echo: 10867  loss: 0.32829\n",
      "echo: 10868  loss: 0.32829\n",
      "echo: 10869  loss: 0.32829\n",
      "echo: 10870  loss: 0.32829\n",
      "echo: 10871  loss: 0.32829\n",
      "echo: 10872  loss: 0.32829\n",
      "echo: 10873  loss: 0.32829\n",
      "echo: 10874  loss: 0.32829\n",
      "echo: 10875  loss: 0.32829\n",
      "echo: 10876  loss: 0.32829\n",
      "echo: 10877  loss: 0.32829\n",
      "echo: 10878  loss: 0.32829\n",
      "echo: 10879  loss: 0.32829\n",
      "echo: 10880  loss: 0.32829\n",
      "echo: 10881  loss: 0.32829\n",
      "echo: 10882  loss: 0.32829\n",
      "echo: 10883  loss: 0.32829\n",
      "echo: 10884  loss: 0.32829\n",
      "echo: 10885  loss: 0.32829\n",
      "echo: 10886  loss: 0.32829\n",
      "echo: 10887  loss: 0.32829\n",
      "echo: 10888  loss: 0.32829\n",
      "echo: 10889  loss: 0.32829\n",
      "echo: 10890  loss: 0.32829\n",
      "echo: 10891  loss: 0.32829\n",
      "echo: 10892  loss: 0.32829\n",
      "echo: 10893  loss: 0.32829\n",
      "echo: 10894  loss: 0.32829\n",
      "echo: 10895  loss: 0.32829\n",
      "echo: 10896  loss: 0.32829\n",
      "echo: 10897  loss: 0.32829\n",
      "echo: 10898  loss: 0.32829\n",
      "echo: 10899  loss: 0.32829\n",
      "echo: 10900  loss: 0.32829\n",
      "echo: 10901  loss: 0.32829\n",
      "echo: 10902  loss: 0.32829\n",
      "echo: 10903  loss: 0.32829\n",
      "echo: 10904  loss: 0.32829\n",
      "echo: 10905  loss: 0.32829\n",
      "echo: 10906  loss: 0.32829\n",
      "echo: 10907  loss: 0.32829\n",
      "echo: 10908  loss: 0.32829\n",
      "echo: 10909  loss: 0.32829\n",
      "echo: 10910  loss: 0.32829\n",
      "echo: 10911  loss: 0.32829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 10912  loss: 0.32829\n",
      "echo: 10913  loss: 0.32829\n",
      "echo: 10914  loss: 0.32829\n",
      "echo: 10915  loss: 0.32829\n",
      "echo: 10916  loss: 0.32829\n",
      "echo: 10917  loss: 0.32829\n",
      "echo: 10918  loss: 0.32829\n",
      "echo: 10919  loss: 0.32829\n",
      "echo: 10920  loss: 0.32829\n",
      "echo: 10921  loss: 0.32829\n",
      "echo: 10922  loss: 0.32829\n",
      "echo: 10923  loss: 0.32829\n",
      "echo: 10924  loss: 0.32829\n",
      "echo: 10925  loss: 0.32829\n",
      "echo: 10926  loss: 0.32829\n",
      "echo: 10927  loss: 0.32829\n",
      "echo: 10928  loss: 0.32829\n",
      "echo: 10929  loss: 0.32829\n",
      "echo: 10930  loss: 0.32829\n",
      "echo: 10931  loss: 0.32829\n",
      "echo: 10932  loss: 0.32829\n",
      "echo: 10933  loss: 0.32829\n",
      "echo: 10934  loss: 0.32829\n",
      "echo: 10935  loss: 0.32829\n",
      "echo: 10936  loss: 0.32829\n",
      "echo: 10937  loss: 0.32829\n",
      "echo: 10938  loss: 0.32829\n",
      "echo: 10939  loss: 0.32829\n",
      "echo: 10940  loss: 0.32829\n",
      "echo: 10941  loss: 0.32829\n",
      "echo: 10942  loss: 0.32829\n",
      "echo: 10943  loss: 0.32829\n",
      "echo: 10944  loss: 0.32829\n",
      "echo: 10945  loss: 0.32829\n",
      "echo: 10946  loss: 0.32829\n",
      "echo: 10947  loss: 0.32829\n",
      "echo: 10948  loss: 0.32829\n",
      "echo: 10949  loss: 0.32829\n",
      "echo: 10950  loss: 0.32829\n",
      "echo: 10951  loss: 0.32829\n",
      "echo: 10952  loss: 0.32829\n",
      "echo: 10953  loss: 0.32829\n",
      "echo: 10954  loss: 0.32829\n",
      "echo: 10955  loss: 0.32829\n",
      "echo: 10956  loss: 0.32829\n",
      "echo: 10957  loss: 0.32829\n",
      "echo: 10958  loss: 0.32829\n",
      "echo: 10959  loss: 0.32829\n",
      "echo: 10960  loss: 0.32829\n",
      "echo: 10961  loss: 0.32829\n",
      "echo: 10962  loss: 0.32829\n",
      "echo: 10963  loss: 0.32829\n",
      "echo: 10964  loss: 0.32829\n",
      "echo: 10965  loss: 0.32829\n",
      "echo: 10966  loss: 0.32829\n",
      "echo: 10967  loss: 0.32829\n",
      "echo: 10968  loss: 0.32829\n",
      "echo: 10969  loss: 0.32829\n",
      "echo: 10970  loss: 0.32829\n",
      "echo: 10971  loss: 0.32829\n",
      "echo: 10972  loss: 0.32829\n",
      "echo: 10973  loss: 0.32829\n",
      "echo: 10974  loss: 0.32829\n",
      "echo: 10975  loss: 0.32829\n",
      "echo: 10976  loss: 0.32829\n",
      "echo: 10977  loss: 0.32829\n",
      "echo: 10978  loss: 0.32829\n",
      "echo: 10979  loss: 0.32829\n",
      "echo: 10980  loss: 0.32829\n",
      "echo: 10981  loss: 0.32829\n",
      "echo: 10982  loss: 0.32829\n",
      "echo: 10983  loss: 0.32829\n",
      "echo: 10984  loss: 0.32829\n",
      "echo: 10985  loss: 0.32829\n",
      "echo: 10986  loss: 0.32829\n",
      "echo: 10987  loss: 0.32829\n",
      "echo: 10988  loss: 0.32829\n",
      "echo: 10989  loss: 0.32829\n",
      "echo: 10990  loss: 0.32829\n",
      "echo: 10991  loss: 0.32829\n",
      "echo: 10992  loss: 0.32829\n",
      "echo: 10993  loss: 0.32829\n",
      "echo: 10994  loss: 0.32829\n",
      "echo: 10995  loss: 0.32829\n",
      "echo: 10996  loss: 0.32829\n",
      "echo: 10997  loss: 0.32829\n",
      "echo: 10998  loss: 0.32829\n",
      "echo: 10999  loss: 0.32829\n",
      "echo: 11000  loss: 0.32829\n",
      "echo: 11001  loss: 0.32829\n",
      "echo: 11002  loss: 0.32829\n",
      "echo: 11003  loss: 0.32829\n",
      "echo: 11004  loss: 0.32829\n",
      "echo: 11005  loss: 0.32829\n",
      "echo: 11006  loss: 0.32829\n",
      "echo: 11007  loss: 0.32829\n",
      "echo: 11008  loss: 0.32829\n",
      "echo: 11009  loss: 0.32829\n",
      "echo: 11010  loss: 0.32829\n",
      "echo: 11011  loss: 0.32829\n",
      "echo: 11012  loss: 0.32829\n",
      "echo: 11013  loss: 0.32829\n",
      "echo: 11014  loss: 0.32829\n",
      "echo: 11015  loss: 0.32829\n",
      "echo: 11016  loss: 0.32829\n",
      "echo: 11017  loss: 0.32829\n",
      "echo: 11018  loss: 0.32829\n",
      "echo: 11019  loss: 0.32829\n",
      "echo: 11020  loss: 0.32829\n",
      "echo: 11021  loss: 0.32829\n",
      "echo: 11022  loss: 0.32829\n",
      "echo: 11023  loss: 0.32829\n",
      "echo: 11024  loss: 0.32829\n",
      "echo: 11025  loss: 0.32829\n",
      "echo: 11026  loss: 0.32829\n",
      "echo: 11027  loss: 0.32829\n",
      "echo: 11028  loss: 0.32829\n",
      "echo: 11029  loss: 0.32829\n",
      "echo: 11030  loss: 0.32829\n",
      "echo: 11031  loss: 0.32829\n",
      "echo: 11032  loss: 0.32829\n",
      "echo: 11033  loss: 0.32829\n",
      "echo: 11034  loss: 0.32829\n",
      "echo: 11035  loss: 0.32829\n",
      "echo: 11036  loss: 0.32829\n",
      "echo: 11037  loss: 0.32829\n",
      "echo: 11038  loss: 0.32829\n",
      "echo: 11039  loss: 0.32829\n",
      "echo: 11040  loss: 0.32829\n",
      "echo: 11041  loss: 0.32829\n",
      "echo: 11042  loss: 0.32829\n",
      "echo: 11043  loss: 0.32829\n",
      "echo: 11044  loss: 0.32829\n",
      "echo: 11045  loss: 0.32829\n",
      "echo: 11046  loss: 0.32829\n",
      "echo: 11047  loss: 0.32829\n",
      "echo: 11048  loss: 0.32829\n",
      "echo: 11049  loss: 0.32829\n",
      "echo: 11050  loss: 0.32829\n",
      "echo: 11051  loss: 0.32829\n",
      "echo: 11052  loss: 0.32829\n",
      "echo: 11053  loss: 0.32829\n",
      "echo: 11054  loss: 0.32829\n",
      "echo: 11055  loss: 0.32829\n",
      "echo: 11056  loss: 0.32829\n",
      "echo: 11057  loss: 0.32829\n",
      "echo: 11058  loss: 0.32829\n",
      "echo: 11059  loss: 0.32829\n",
      "echo: 11060  loss: 0.32829\n",
      "echo: 11061  loss: 0.32829\n",
      "echo: 11062  loss: 0.32829\n",
      "echo: 11063  loss: 0.32829\n",
      "echo: 11064  loss: 0.32829\n",
      "echo: 11065  loss: 0.32829\n",
      "echo: 11066  loss: 0.32829\n",
      "echo: 11067  loss: 0.32829\n",
      "echo: 11068  loss: 0.32829\n",
      "echo: 11069  loss: 0.32829\n",
      "echo: 11070  loss: 0.32829\n",
      "echo: 11071  loss: 0.32829\n",
      "echo: 11072  loss: 0.32829\n",
      "echo: 11073  loss: 0.32829\n",
      "echo: 11074  loss: 0.32829\n",
      "echo: 11075  loss: 0.32829\n",
      "echo: 11076  loss: 0.32829\n",
      "echo: 11077  loss: 0.32829\n",
      "echo: 11078  loss: 0.32829\n",
      "echo: 11079  loss: 0.32829\n",
      "echo: 11080  loss: 0.32829\n",
      "echo: 11081  loss: 0.32829\n",
      "echo: 11082  loss: 0.32829\n",
      "echo: 11083  loss: 0.32829\n",
      "echo: 11084  loss: 0.32829\n",
      "echo: 11085  loss: 0.32829\n",
      "echo: 11086  loss: 0.32829\n",
      "echo: 11087  loss: 0.32829\n",
      "echo: 11088  loss: 0.32829\n",
      "echo: 11089  loss: 0.32829\n",
      "echo: 11090  loss: 0.32829\n",
      "echo: 11091  loss: 0.32829\n",
      "echo: 11092  loss: 0.32829\n",
      "echo: 11093  loss: 0.32829\n",
      "echo: 11094  loss: 0.32829\n",
      "echo: 11095  loss: 0.32829\n",
      "echo: 11096  loss: 0.32829\n",
      "echo: 11097  loss: 0.32829\n",
      "echo: 11098  loss: 0.32829\n",
      "echo: 11099  loss: 0.32829\n",
      "echo: 11100  loss: 0.32829\n",
      "echo: 11101  loss: 0.32829\n",
      "echo: 11102  loss: 0.32829\n",
      "echo: 11103  loss: 0.32829\n",
      "echo: 11104  loss: 0.32829\n",
      "echo: 11105  loss: 0.32829\n",
      "echo: 11106  loss: 0.32829\n",
      "echo: 11107  loss: 0.32829\n",
      "echo: 11108  loss: 0.32829\n",
      "echo: 11109  loss: 0.32829\n",
      "echo: 11110  loss: 0.32829\n",
      "echo: 11111  loss: 0.32829\n",
      "echo: 11112  loss: 0.32829\n",
      "echo: 11113  loss: 0.32829\n",
      "echo: 11114  loss: 0.32829\n",
      "echo: 11115  loss: 0.32829\n",
      "echo: 11116  loss: 0.32829\n",
      "echo: 11117  loss: 0.32829\n",
      "echo: 11118  loss: 0.32829\n",
      "echo: 11119  loss: 0.32829\n",
      "echo: 11120  loss: 0.32829\n",
      "echo: 11121  loss: 0.32829\n",
      "echo: 11122  loss: 0.32829\n",
      "echo: 11123  loss: 0.32829\n",
      "echo: 11124  loss: 0.32829\n",
      "echo: 11125  loss: 0.32829\n",
      "echo: 11126  loss: 0.32829\n",
      "echo: 11127  loss: 0.32829\n",
      "echo: 11128  loss: 0.32829\n",
      "echo: 11129  loss: 0.32829\n",
      "echo: 11130  loss: 0.32829\n",
      "echo: 11131  loss: 0.32829\n",
      "echo: 11132  loss: 0.32829\n",
      "echo: 11133  loss: 0.32829\n",
      "echo: 11134  loss: 0.32829\n",
      "echo: 11135  loss: 0.32829\n",
      "echo: 11136  loss: 0.32829\n",
      "echo: 11137  loss: 0.32829\n",
      "echo: 11138  loss: 0.32829\n",
      "echo: 11139  loss: 0.32829\n",
      "echo: 11140  loss: 0.32829\n",
      "echo: 11141  loss: 0.32829\n",
      "echo: 11142  loss: 0.32829\n",
      "echo: 11143  loss: 0.32829\n",
      "echo: 11144  loss: 0.32829\n",
      "echo: 11145  loss: 0.32829\n",
      "echo: 11146  loss: 0.32829\n",
      "echo: 11147  loss: 0.32829\n",
      "echo: 11148  loss: 0.32829\n",
      "echo: 11149  loss: 0.32829\n",
      "echo: 11150  loss: 0.32829\n",
      "echo: 11151  loss: 0.32829\n",
      "echo: 11152  loss: 0.32829\n",
      "echo: 11153  loss: 0.32829\n",
      "echo: 11154  loss: 0.32829\n",
      "echo: 11155  loss: 0.32829\n",
      "echo: 11156  loss: 0.32829\n",
      "echo: 11157  loss: 0.32829\n",
      "echo: 11158  loss: 0.32829\n",
      "echo: 11159  loss: 0.32829\n",
      "echo: 11160  loss: 0.32829\n",
      "echo: 11161  loss: 0.32829\n",
      "echo: 11162  loss: 0.32829\n",
      "echo: 11163  loss: 0.32829\n",
      "echo: 11164  loss: 0.32829\n",
      "echo: 11165  loss: 0.32829\n",
      "echo: 11166  loss: 0.32829\n",
      "echo: 11167  loss: 0.32829\n",
      "echo: 11168  loss: 0.32829\n",
      "echo: 11169  loss: 0.32829\n",
      "echo: 11170  loss: 0.32829\n",
      "echo: 11171  loss: 0.32829\n",
      "echo: 11172  loss: 0.32829\n",
      "echo: 11173  loss: 0.32829\n",
      "echo: 11174  loss: 0.32829\n",
      "echo: 11175  loss: 0.32829\n",
      "echo: 11176  loss: 0.32829\n",
      "echo: 11177  loss: 0.32829\n",
      "echo: 11178  loss: 0.32829\n",
      "echo: 11179  loss: 0.32829\n",
      "echo: 11180  loss: 0.32829\n",
      "echo: 11181  loss: 0.32829\n",
      "echo: 11182  loss: 0.32829\n",
      "echo: 11183  loss: 0.32829\n",
      "echo: 11184  loss: 0.32829\n",
      "echo: 11185  loss: 0.32829\n",
      "echo: 11186  loss: 0.32829\n",
      "echo: 11187  loss: 0.32829\n",
      "echo: 11188  loss: 0.32829\n",
      "echo: 11189  loss: 0.32829\n",
      "echo: 11190  loss: 0.32829\n",
      "echo: 11191  loss: 0.32829\n",
      "echo: 11192  loss: 0.32829\n",
      "echo: 11193  loss: 0.32829\n",
      "echo: 11194  loss: 0.32829\n",
      "echo: 11195  loss: 0.32829\n",
      "echo: 11196  loss: 0.32829\n",
      "echo: 11197  loss: 0.32829\n",
      "echo: 11198  loss: 0.32829\n",
      "echo: 11199  loss: 0.32829\n",
      "echo: 11200  loss: 0.32829\n",
      "echo: 11201  loss: 0.32829\n",
      "echo: 11202  loss: 0.32829\n",
      "echo: 11203  loss: 0.32829\n",
      "echo: 11204  loss: 0.32829\n",
      "echo: 11205  loss: 0.32829\n",
      "echo: 11206  loss: 0.32829\n",
      "echo: 11207  loss: 0.32829\n",
      "echo: 11208  loss: 0.32829\n",
      "echo: 11209  loss: 0.32829\n",
      "echo: 11210  loss: 0.32829\n",
      "echo: 11211  loss: 0.32829\n",
      "echo: 11212  loss: 0.32829\n",
      "echo: 11213  loss: 0.32829\n",
      "echo: 11214  loss: 0.32829\n",
      "echo: 11215  loss: 0.32829\n",
      "echo: 11216  loss: 0.32829\n",
      "echo: 11217  loss: 0.32829\n",
      "echo: 11218  loss: 0.32829\n",
      "echo: 11219  loss: 0.32829\n",
      "echo: 11220  loss: 0.32829\n",
      "echo: 11221  loss: 0.32829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 11222  loss: 0.32829\n",
      "echo: 11223  loss: 0.32829\n",
      "echo: 11224  loss: 0.32829\n",
      "echo: 11225  loss: 0.32829\n",
      "echo: 11226  loss: 0.32829\n",
      "echo: 11227  loss: 0.32829\n",
      "echo: 11228  loss: 0.32829\n",
      "echo: 11229  loss: 0.32829\n",
      "echo: 11230  loss: 0.32829\n",
      "echo: 11231  loss: 0.32829\n",
      "echo: 11232  loss: 0.32829\n",
      "echo: 11233  loss: 0.32829\n",
      "echo: 11234  loss: 0.32829\n",
      "echo: 11235  loss: 0.32829\n",
      "echo: 11236  loss: 0.32829\n",
      "echo: 11237  loss: 0.32829\n",
      "echo: 11238  loss: 0.32829\n",
      "echo: 11239  loss: 0.32829\n",
      "echo: 11240  loss: 0.32829\n",
      "echo: 11241  loss: 0.32829\n",
      "echo: 11242  loss: 0.32829\n",
      "echo: 11243  loss: 0.32829\n",
      "echo: 11244  loss: 0.32829\n",
      "echo: 11245  loss: 0.32829\n",
      "echo: 11246  loss: 0.32829\n",
      "echo: 11247  loss: 0.32829\n",
      "echo: 11248  loss: 0.32829\n",
      "echo: 11249  loss: 0.32829\n",
      "echo: 11250  loss: 0.32829\n",
      "echo: 11251  loss: 0.32829\n",
      "echo: 11252  loss: 0.32829\n",
      "echo: 11253  loss: 0.32829\n",
      "echo: 11254  loss: 0.32829\n",
      "echo: 11255  loss: 0.32829\n",
      "echo: 11256  loss: 0.32829\n",
      "echo: 11257  loss: 0.32829\n",
      "echo: 11258  loss: 0.32829\n",
      "echo: 11259  loss: 0.32829\n",
      "echo: 11260  loss: 0.32829\n",
      "echo: 11261  loss: 0.32829\n",
      "echo: 11262  loss: 0.32829\n",
      "echo: 11263  loss: 0.32829\n",
      "echo: 11264  loss: 0.32829\n",
      "echo: 11265  loss: 0.32829\n",
      "echo: 11266  loss: 0.32829\n",
      "echo: 11267  loss: 0.32829\n",
      "echo: 11268  loss: 0.32829\n",
      "echo: 11269  loss: 0.32829\n",
      "echo: 11270  loss: 0.32829\n",
      "echo: 11271  loss: 0.32829\n",
      "echo: 11272  loss: 0.32829\n",
      "echo: 11273  loss: 0.32829\n",
      "echo: 11274  loss: 0.32829\n",
      "echo: 11275  loss: 0.32829\n",
      "echo: 11276  loss: 0.32829\n",
      "echo: 11277  loss: 0.32829\n",
      "echo: 11278  loss: 0.32829\n",
      "echo: 11279  loss: 0.32829\n",
      "echo: 11280  loss: 0.32829\n",
      "echo: 11281  loss: 0.32829\n",
      "echo: 11282  loss: 0.32829\n",
      "echo: 11283  loss: 0.32829\n",
      "echo: 11284  loss: 0.32829\n",
      "echo: 11285  loss: 0.32829\n",
      "echo: 11286  loss: 0.32829\n",
      "echo: 11287  loss: 0.32829\n",
      "echo: 11288  loss: 0.32829\n",
      "echo: 11289  loss: 0.32829\n",
      "echo: 11290  loss: 0.32829\n",
      "echo: 11291  loss: 0.32829\n",
      "echo: 11292  loss: 0.32829\n",
      "echo: 11293  loss: 0.32829\n",
      "echo: 11294  loss: 0.32829\n",
      "echo: 11295  loss: 0.32829\n",
      "echo: 11296  loss: 0.32829\n",
      "echo: 11297  loss: 0.32829\n",
      "echo: 11298  loss: 0.32829\n",
      "echo: 11299  loss: 0.32829\n",
      "echo: 11300  loss: 0.32829\n",
      "echo: 11301  loss: 0.32829\n",
      "echo: 11302  loss: 0.32829\n",
      "echo: 11303  loss: 0.32829\n",
      "echo: 11304  loss: 0.32829\n",
      "echo: 11305  loss: 0.32829\n",
      "echo: 11306  loss: 0.32829\n",
      "echo: 11307  loss: 0.32829\n",
      "echo: 11308  loss: 0.32829\n",
      "echo: 11309  loss: 0.32829\n",
      "echo: 11310  loss: 0.32829\n",
      "echo: 11311  loss: 0.32829\n",
      "echo: 11312  loss: 0.32829\n",
      "echo: 11313  loss: 0.32829\n",
      "echo: 11314  loss: 0.32829\n",
      "echo: 11315  loss: 0.32829\n",
      "echo: 11316  loss: 0.32829\n",
      "echo: 11317  loss: 0.32829\n",
      "echo: 11318  loss: 0.32829\n",
      "echo: 11319  loss: 0.32829\n",
      "echo: 11320  loss: 0.32829\n",
      "echo: 11321  loss: 0.32829\n",
      "echo: 11322  loss: 0.32829\n",
      "echo: 11323  loss: 0.32829\n",
      "echo: 11324  loss: 0.32829\n",
      "echo: 11325  loss: 0.32829\n",
      "echo: 11326  loss: 0.32829\n",
      "echo: 11327  loss: 0.32829\n",
      "echo: 11328  loss: 0.32829\n",
      "echo: 11329  loss: 0.32829\n",
      "echo: 11330  loss: 0.32829\n",
      "echo: 11331  loss: 0.32829\n",
      "echo: 11332  loss: 0.32829\n",
      "echo: 11333  loss: 0.32829\n",
      "echo: 11334  loss: 0.32829\n",
      "echo: 11335  loss: 0.32829\n",
      "echo: 11336  loss: 0.32829\n",
      "echo: 11337  loss: 0.32829\n",
      "echo: 11338  loss: 0.32829\n",
      "echo: 11339  loss: 0.32829\n",
      "echo: 11340  loss: 0.32829\n",
      "echo: 11341  loss: 0.32829\n",
      "echo: 11342  loss: 0.32829\n",
      "echo: 11343  loss: 0.32829\n",
      "echo: 11344  loss: 0.32829\n",
      "echo: 11345  loss: 0.32829\n",
      "echo: 11346  loss: 0.32829\n",
      "echo: 11347  loss: 0.32829\n",
      "echo: 11348  loss: 0.32829\n",
      "echo: 11349  loss: 0.32829\n",
      "echo: 11350  loss: 0.32829\n",
      "echo: 11351  loss: 0.32829\n",
      "echo: 11352  loss: 0.32829\n",
      "echo: 11353  loss: 0.32829\n",
      "echo: 11354  loss: 0.32829\n",
      "echo: 11355  loss: 0.32829\n",
      "echo: 11356  loss: 0.32829\n",
      "echo: 11357  loss: 0.32829\n",
      "echo: 11358  loss: 0.32829\n",
      "echo: 11359  loss: 0.32829\n",
      "echo: 11360  loss: 0.32829\n",
      "echo: 11361  loss: 0.32829\n",
      "echo: 11362  loss: 0.32829\n",
      "echo: 11363  loss: 0.32829\n",
      "echo: 11364  loss: 0.32829\n",
      "echo: 11365  loss: 0.32829\n",
      "echo: 11366  loss: 0.32829\n",
      "echo: 11367  loss: 0.32829\n",
      "echo: 11368  loss: 0.32829\n",
      "echo: 11369  loss: 0.32829\n",
      "echo: 11370  loss: 0.32829\n",
      "echo: 11371  loss: 0.32829\n",
      "echo: 11372  loss: 0.32829\n",
      "echo: 11373  loss: 0.32829\n",
      "echo: 11374  loss: 0.32829\n",
      "echo: 11375  loss: 0.32829\n",
      "echo: 11376  loss: 0.32829\n",
      "echo: 11377  loss: 0.32829\n",
      "echo: 11378  loss: 0.32829\n",
      "echo: 11379  loss: 0.32829\n",
      "echo: 11380  loss: 0.32829\n",
      "echo: 11381  loss: 0.32829\n",
      "echo: 11382  loss: 0.32829\n",
      "echo: 11383  loss: 0.32829\n",
      "echo: 11384  loss: 0.32829\n",
      "echo: 11385  loss: 0.32829\n",
      "echo: 11386  loss: 0.32829\n",
      "echo: 11387  loss: 0.32829\n",
      "echo: 11388  loss: 0.32829\n",
      "echo: 11389  loss: 0.32829\n",
      "echo: 11390  loss: 0.32829\n",
      "echo: 11391  loss: 0.32829\n",
      "echo: 11392  loss: 0.32829\n",
      "echo: 11393  loss: 0.32829\n",
      "echo: 11394  loss: 0.32829\n",
      "echo: 11395  loss: 0.32829\n",
      "echo: 11396  loss: 0.32829\n",
      "echo: 11397  loss: 0.32829\n",
      "echo: 11398  loss: 0.32829\n",
      "echo: 11399  loss: 0.32829\n",
      "echo: 11400  loss: 0.32829\n",
      "echo: 11401  loss: 0.32829\n",
      "echo: 11402  loss: 0.32829\n",
      "echo: 11403  loss: 0.32829\n",
      "echo: 11404  loss: 0.32829\n",
      "echo: 11405  loss: 0.32829\n",
      "echo: 11406  loss: 0.32829\n",
      "echo: 11407  loss: 0.32829\n",
      "echo: 11408  loss: 0.32829\n",
      "echo: 11409  loss: 0.32829\n",
      "echo: 11410  loss: 0.32829\n",
      "echo: 11411  loss: 0.32829\n",
      "echo: 11412  loss: 0.32829\n",
      "echo: 11413  loss: 0.32829\n",
      "echo: 11414  loss: 0.32829\n",
      "echo: 11415  loss: 0.32829\n",
      "echo: 11416  loss: 0.32829\n",
      "echo: 11417  loss: 0.32829\n",
      "echo: 11418  loss: 0.32829\n",
      "echo: 11419  loss: 0.32829\n",
      "echo: 11420  loss: 0.32829\n",
      "echo: 11421  loss: 0.32829\n",
      "echo: 11422  loss: 0.32829\n",
      "echo: 11423  loss: 0.32829\n",
      "echo: 11424  loss: 0.32829\n",
      "echo: 11425  loss: 0.32829\n",
      "echo: 11426  loss: 0.32829\n",
      "echo: 11427  loss: 0.32829\n",
      "echo: 11428  loss: 0.32829\n",
      "echo: 11429  loss: 0.32829\n",
      "echo: 11430  loss: 0.32829\n",
      "echo: 11431  loss: 0.32829\n",
      "echo: 11432  loss: 0.32829\n",
      "echo: 11433  loss: 0.32829\n",
      "echo: 11434  loss: 0.32829\n",
      "echo: 11435  loss: 0.32829\n",
      "echo: 11436  loss: 0.32829\n",
      "echo: 11437  loss: 0.32829\n",
      "echo: 11438  loss: 0.32829\n",
      "echo: 11439  loss: 0.32829\n",
      "echo: 11440  loss: 0.32829\n",
      "echo: 11441  loss: 0.32829\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7144c264656d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"迭代损失\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpause\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mioff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mpause\u001b[1;34m(interval)\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_qt5.py\u001b[0m in \u001b[0;36mstart_event_loop\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m             timer = QtCore.QTimer.singleShot(int(timeout * 1000),\n\u001b[0;32m    433\u001b[0m                                              event_loop.quit)\n\u001b[1;32m--> 434\u001b[1;33m         \u001b[0mevent_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexec_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstop_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib qt5\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "warnings.filterwarnings('ignore') # 关闭警告\n",
    "np.random.seed(0)\n",
    "X, y = sklearn.datasets.make_moons(200,noise=0.2)\n",
    "X = torch.from_numpy(X).type(torch.FloatTensor)\n",
    "y = torch.from_numpy(y).type(torch.LongTensor)\n",
    "\n",
    "#our class must extend nn.Module\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(2,3)\n",
    "        self.fc2 = nn.Linear(3,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x)\n",
    "        return x\n",
    "              \n",
    "    def predict(self,x):\n",
    "        pred = self.forward(x)\n",
    "        ans = []\n",
    "        for t in pred:\n",
    "            if t[0]>t[1]:\n",
    "                ans.append(0)\n",
    "            else:\n",
    "                ans.append(1)\n",
    "        return torch.tensor(ans)\n",
    "  \n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    " \n",
    "epochs = 50000\n",
    "px = []\n",
    "losses = []\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.ion() \n",
    "for i in range(epochs):\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred,y)\n",
    "    print(\"echo:\",i+1, \" loss:\", round(loss.item(),5))\n",
    "    losses.append(loss.item())\n",
    "#     px.append(i)\n",
    "#     if i%10 == 0:\n",
    "#         plt.clf() \n",
    "#         plt.plot(px, losses, label = \"CrossEntropy\")\n",
    "#         plt.grid()\n",
    "#         plt.title(\"训练过程的loss曲线\")\n",
    "#         plt.xlabel(\"迭代次数\")\n",
    "#         plt.ylabel(\"迭代损失\")\n",
    "#         plt.legend()\n",
    "#         plt.pause(0.5)\n",
    "#         plt.ioff() \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(accuracy_score(model.predict(X),y))\n",
    "\n",
    "# 模型存储\n",
    "# torch.save(model.state_dict(), \"simple_net\") # 存储训练好的模型参数\n",
    "# model1 = Net() # 加载的时候自行写模型\n",
    "# model1.load_state_dict(torch.load(\"simple_net\")) # 加载模型参数\n",
    "# model1.eval() # 固定非参数层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些注意的点，下面代码暂时没有涉及：\n",
    "1.训练时每次epoch都要使用mini-batch数据\n",
    "2.注意每次输入和输出的shape以及对应的数据类型\n",
    "3.每个epoch后分别记录一次训练集和验证集的loss和ACC、AUC等评分指标，并最终画图展示以观察拟合情况（过拟合or欠拟合）\n",
    "4.由于模型涉及随机初始化和随机GD，相同模型和数据重复结果未必相同，所以要重复执行整个流程多次，叠加多个图＆平均多个结果。\n",
    "5.LSTM很容易过拟合，dropout是必要的步骤，可以在embedding后和LSTM层后都加入\n",
    "6.模型的复杂度要和数据量相适应！\n",
    "7.批量归一化也是比较重要的\n",
    "8.设置早停\n",
    "更多内容见：https://blog.csdn.net/fu_jian_ping/article/details/109147133?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164423947516781683998851%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=164423947516781683998851&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-109147133.first_rank_v2_pc_rank_v29&utm_term=LSTM%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4&spm=1018.2226.3001.4187"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden_inital' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-27a2a982e487>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 跑模型，得到每个时点的预测结果\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0mhidden_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_final\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 计算损失函数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-27a2a982e487>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# hidden_inital是初始存储状态，可不写，那样默认全是0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_inital\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 返回值：第一个是每个时点的输出值，shape是[batch,sep,output_size];第二个是每个最后一个时点的所有隐层的存储状态，shape是[batch_size,layer_size,hidden_size]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 套输出层\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 把输出结果降维\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hidden_inital' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "num_time_steps = 50\n",
    "start = np.random.randint(3, size = 1)[0]\n",
    "time_steps = np.linspace(start, start + 10, num_time_steps)\n",
    "data = np.sin(time_steps).reshape(num_time_steps, 1)\n",
    "x = torch.tensor(data[:-1]).float().view(1, num_time_steps-1, 1)\n",
    "y = torch.tensor(data[1:]).float().view(1, num_time_steps-1, 1)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size): # 参数设置别忘了（这些参数不会默认传给forward函数）\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = input_size, # 特征维度\n",
    "            hidden_size = hidden_size, # 隐层神经元数\n",
    "            num_layers = num_layers, # 隐层数\n",
    "            batch_first = True # 是否让batch参数总是在第一位\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, output_size) # 输出线性层，如果是分类问题这里的output_size应该是类别数，且要再下面加一层softmax\n",
    "        \n",
    "    def forward(self, x, hidden_inital): # hidden_inital是初始存储状态，可不写，那样默认全是0\n",
    "        out, hidden_final = self.rnn(x, hidden_inital) # 返回值：第一个是每个时点的输出值，shape是[batch,sep,output_size];第二个是每个最后一个时点的所有隐层的存储状态，shape是[batch_size,layer_size,hidden_size]\n",
    "        out = self.linear(out) # 套输出层\n",
    "        out = out.unsqueeze(dim = 0) # 把输出结果降维\n",
    "        return out, hidden_final\n",
    "\n",
    "def xavier(m): # 参数Xavier初始化\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "model = Net(1, 20, 1, 1) # 创建网络（别忘了输入参数）\n",
    "model.apply(xavier) # 应用初始化参数\n",
    "criterion = nn.MSELoss() # 损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.03) # 梯度优化器\n",
    "hidden_inital = torch.zeros(1, 1, 20) # 初始隐层状态设置（非必须项）\n",
    "\n",
    "epoch = 6\n",
    "for iter in range(epoch):\n",
    "    output, hidden_final = model(x, hidden_inital) # 跑模型，得到每个时点的预测结果\n",
    "    hidden_final = hidden_final.detach()\n",
    "    loss = criterion(output, y) # 计算损失函数\n",
    "    optimizer.zero_grad() # 清零过往参数的梯度结果\n",
    "    loss.backward() # 根据损失函数值计算梯度\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2) # 梯度裁剪，参数介绍：参数集合；最大梯度范数；梯度范数类型\n",
    "    optimizer.step() # 根据梯度更新参数\n",
    "    if iter % 100 == 0: print(\"Iteration: {} loss {}\".format(iter, loss.item())) # 每100次迭代返回一次结果\n",
    "\n",
    "pred = [] # 每个时点的预测结果\n",
    "test_x = x[:, 0, :]\n",
    "for i in range(x.shape[1]):\n",
    "    test_x = test_x.view(1,1,1)\n",
    "    pre, h = model(test_x, hidden_inital)\n",
    "    test_x = pre\n",
    "    pred.append(pre.squeeze().detach())\n",
    "x = x.data.numpy().ravel()\n",
    "y = y.data.numpy()\n",
    "plt.scatter(time_steps[:-1], x.squeeze())\n",
    "plt.plot(time_steps[:-1], x.ravel())\n",
    "plt.scatter(time_steps[:-1], pred)\n",
    "plt.show()\n",
    "\n",
    "# 模型存储\n",
    "# torch.save(model.state_dict(), \"simple_net\") # 存储训练好的模型参数\n",
    "# model1 = Net() # 加载的时候自行写模型\n",
    "# model1.load_state_dict(torch.load(\"simple_net\")) # 加载模型参数\n",
    "# model1.eval() # 固定非参数层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size,vocab_size. embedding_dim): # 参数设置别忘了（这些参数不会默认传给forward函数）\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size. embedding_dim)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = input_size, # 特征维度\n",
    "            hidden_size = hidden_size, # 隐层神经元数\n",
    "            num_layers = num_layers, # 隐层数\n",
    "            batch_first = True # 是否让batch参数总是在第一位\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, output_size) # 输出线性层，如果是分类问题这里的output_size应该是类别数，且要再下面加一层softmax\n",
    "        \n",
    "    def forward(self, x, hidden_inital): # hidden_inital是初始存储状态，可不写，那样默认全是0\n",
    "        out, hidden_final = self.rnn(x, hidden_inital) # 返回值：第一个是每个时点的输出值，shape是[batch,sep,output_size];第二个是每个最后一个时点的所有隐层的存储状态，shape是[batch_size,layer_size,hidden_size]\n",
    "        out = self.linear(out) # 套输出层\n",
    "        out = out.unsqueeze(dim = 0) # 把输出结果降维\n",
    "        return out, hidden_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "data: tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "label: tensor([[0],\n",
      "        [1]], dtype=torch.int32)\n",
      "i: 1\n",
      "data: tensor([[5., 6.],\n",
      "        [7., 8.]])\n",
      "label: tensor([[0],\n",
      "        [2]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data.dataset as Dataset\n",
    "import torch.utils.data.dataloader as DataLoader\n",
    "import numpy as np\n",
    "Data = np.asarray([[1, 2], [3, 4],[5, 6], [7, 8]])\n",
    "Label = np.asarray([[0], [1], [0], [2]])\n",
    "class subDataset(Dataset.Dataset):\n",
    "    def __init__(self, Data, Label):\n",
    "        self.Data = Data\n",
    "        self.Label = Label\n",
    "    def __len__(self):\n",
    "        return len(self.Data)\n",
    "    def __getitem__(self, index):\n",
    "        data = torch.Tensor(self.Data[index])\n",
    "        label = torch.IntTensor(self.Label[index])\n",
    "        return data, label\n",
    "dataset = subDataset(Data, Label)\n",
    "dataloader = DataLoader.DataLoader(dataset, batch_size= 2, shuffle = False)\n",
    "for i, item in enumerate(dataloader):\n",
    "    print('i:', i)\n",
    "    data, label = item\n",
    "    print('data:', data)\n",
    "    print('label:', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer源码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写框架的时候两个要点：\n",
    "- 先写大框架，再写小框架，一级一级往下写\n",
    "- 不确定的输入形式可以先不写，后面补上\n",
    "- 时刻清楚数据在每个部分流转时候的形态和类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 2.280869\n",
      "Epoch: 0002 cost = 3.970184\n",
      "Epoch: 0003 cost = 2.795365\n",
      "Epoch: 0004 cost = 5.159268\n",
      "Epoch: 0005 cost = 3.878076\n",
      "Epoch: 0006 cost = 5.810416\n",
      "Epoch: 0007 cost = 5.532382\n",
      "Epoch: 0008 cost = 4.226982\n",
      "Epoch: 0009 cost = 5.170188\n",
      "Epoch: 0010 cost = 1.904359\n",
      "Epoch: 0011 cost = 2.123083\n",
      "Epoch: 0012 cost = 2.040266\n",
      "Epoch: 0013 cost = 2.028134\n",
      "Epoch: 0014 cost = 2.044340\n",
      "Epoch: 0015 cost = 1.865151\n",
      "Epoch: 0016 cost = 1.688594\n",
      "Epoch: 0017 cost = 1.676789\n",
      "Epoch: 0018 cost = 1.732380\n",
      "Epoch: 0019 cost = 1.743735\n",
      "Epoch: 0020 cost = 1.714740\n",
      "ich mochte ein bier P -> ['E', 'E', 'E', 'E', 'E']\n",
      "first head of last state enc_self_attns\n"
     ]
    }
   ],
   "source": [
    "# code by Tae Hwan Jung(Jeff Jung) @graykode, Derek Miller @dmmiller612\n",
    "# Reference : https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
    "#           https://github.com/JayParks/transformer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "\n",
    "def make_batch(sentences):\n",
    "    input_batch = [[src_vocab[n] for n in sentences[0].split()]]\n",
    "    output_batch = [[tgt_vocab[n] for n in sentences[1].split()]]\n",
    "    target_batch = [[tgt_vocab[n] for n in sentences[2].split()]]\n",
    "    return torch.LongTensor(input_batch), torch.LongTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "def get_sinusoid_encoding_table(n_position, d_model):\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "    return torch.FloatTensor(sinusoid_table)\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
    "\n",
    "def get_attn_subsequent_mask(seq):\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
    "    subsequent_mask = torch.from_numpy(subsequent_mask).byte()\n",
    "    return subsequent_mask\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "        self.linear = nn.Linear(n_heads * d_v, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = self.linear(context)\n",
    "        return self.layer_norm(output + residual), attn # output: [batch_size x len_q x d_model]\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        residual = inputs # inputs : [batch_size, len_q, d_model]\n",
    "        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        return self.layer_norm(output + residual)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        self.dec_enc_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
    "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
    "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        dec_outputs = self.pos_ffn(dec_outputs)\n",
    "        return dec_outputs, dec_self_attn, dec_enc_attn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(src_len+1, d_model),freeze=True)\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs): # enc_inputs : [batch_size x source_len]\n",
    "        enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(torch.LongTensor([[1,2,3,4,0]]))\n",
    "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
    "        enc_self_attns = []\n",
    "        for layer in self.layers:\n",
    "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
    "            enc_self_attns.append(enc_self_attn)\n",
    "        return enc_outputs, enc_self_attns\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(tgt_len+1, d_model),freeze=True)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs): # dec_inputs : [batch_size x target_len]\n",
    "        dec_outputs = self.tgt_emb(dec_inputs) + self.pos_emb(torch.LongTensor([[5,1,2,3,4]]))\n",
    "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs)\n",
    "        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs)\n",
    "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n",
    "\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
    "\n",
    "        dec_self_attns, dec_enc_attns = [], []\n",
    "        for layer in self.layers:\n",
    "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            dec_self_attns.append(dec_self_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return dec_outputs, dec_self_attns, dec_enc_attns\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder() # 定义encoder类\n",
    "        self.decoder = Decoder() # 定义Decoder类\n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False) # \n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
    "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        dec_logits = self.projection(dec_outputs) # dec_logits : [batch_size x src_vocab_size x tgt_vocab_size]\n",
    "        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns\n",
    "\n",
    "def showgraph(attn):\n",
    "    attn = attn[-1].squeeze(0)[0]\n",
    "    attn = attn.squeeze(0).data.numpy()\n",
    "    fig = plt.figure(figsize=(n_heads, n_heads)) # [n_heads, n_heads]\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attn, cmap='viridis')\n",
    "    ax.set_xticklabels(['']+sentences[0].split(), fontdict={'fontsize': 14}, rotation=90)\n",
    "    ax.set_yticklabels(['']+sentences[2].split(), fontdict={'fontsize': 14})\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__': # 框架搭建完毕，开始使用\n",
    "    # 数据集，只有一个样本，包含编码器输入，解码器输入和真实标签三部分\n",
    "    sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E'] # P是填充字符；S是开始字符；E是结束字符\n",
    "    src_vocab = {'P': 0, 'ich': 1, 'mochte': 2, 'ein': 3, 'bier': 4} # 编码端字典\n",
    "    src_vocab_size = len(src_vocab) # 字典长度\n",
    "\n",
    "    tgt_vocab = {'P': 0, 'i': 1, 'want': 2, 'a': 3, 'beer': 4, 'S': 5, 'E': 6} # 解码端字典，一般与编码端是一样的\n",
    "#     number_dict = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "    tgt_vocab_size = len(tgt_vocab) # 解码端字典长度\n",
    "\n",
    "    src_len = 5 # length of source # 编码端输入句子长度\n",
    "    tgt_len = 5 # length of target # 解码端输入句子长度\n",
    "\n",
    "    d_model = 512  # Embedding Size # 输入向量经过embedding层后输入的维度\n",
    "    d_ff = 2048  # FeedForward dimension # 全连接层的维度\n",
    "    d_k = d_v = 64  # dimension of K(=Q), V # 输入转化为query,key,value向量的维度\n",
    "    n_layers = 6  # number of Encoder of Decoder Layer # 编码和解码器中block的数量\n",
    "    n_heads = 8  # number of heads in Multi-Head Attention # 多头注意力机制的数量\n",
    "\n",
    "    model = Transformer() # 定义transformer层\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss() # 定义损失函数\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001) # 定义迭代优化器\n",
    "\n",
    "    # 数据处理，将文本型数据进行分词，并转为在字典中对应的索引，这里由于只有一个样本，就没使用迭代器\n",
    "    enc_inputs, dec_inputs, target_batch = make_batch(sentences) # 输出三个部分，分别是编码器、解码器和标签的索引LongTenor列表\n",
    "\n",
    "    for epoch in range(20): # 开始迭代\n",
    "        optimizer.zero_grad() # 参数梯度清零\n",
    "        outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs) # 输入输出\n",
    "        loss = criterion(outputs, target_batch.contiguous().view(-1)) # 计算损失函数\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss)) # 打印迭代结果\n",
    "        loss.backward() # 反向传播\n",
    "        optimizer.step() # 更新参数\n",
    "\n",
    "    # Test\n",
    "    predict, _, _, _ = model(enc_inputs, dec_inputs)\n",
    "    predict = predict.data.max(1, keepdim=True)[1]\n",
    "    print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
    "\n",
    "    print('first head of last state enc_self_attns')\n",
    "    showgraph(enc_self_attns)\n",
    "\n",
    "    print('first head of last state dec_self_attns')\n",
    "    showgraph(dec_self_attns)\n",
    "\n",
    "    print('first head of last state dec_enc_attns')\n",
    "    showgraph(dec_enc_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度模型流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入预处理好的特征和标签tensor以及其他参数，输出拆分好的训练集和验证集的dataloder类以及和测试集的全数据集合\n",
    "def create_datasets(Data, Label, batch_size, allocate_size = [0.6, 0.2, 0.2], shuffle = False): # 不管是什么类型的数据，样本维度需要是第一个维度\n",
    "    import torch\n",
    "    import torch.utils.data.dataset as Dataset\n",
    "    import torch.utils.data.dataloader as DataLoader\n",
    "    import numpy as np\n",
    "    \n",
    "    # 创建Dataset子类\n",
    "    class subDataset(Dataset.Dataset):\n",
    "        def __init__(self, Data, Label):\n",
    "            self.Data = Data\n",
    "            self.Label = Label\n",
    "        def __len__(self):\n",
    "            return len(self.Data)\n",
    "        def __getitem__(self, index):\n",
    "            data = torch.Tensor(self.Data[index])\n",
    "            label = torch.IntTensor(self.Label[index])\n",
    "            return data, label\n",
    "        \n",
    "    # 随机拆分训练集，验证集和测试集\n",
    "    train_size, valid_size, test_size = allocate_size # 数据占比\n",
    "    num_train = len(Data) # 样本总量\n",
    "    indices = list(range(num_train)) # 生成索引列\n",
    "    np.random.shuffle(indices) # 打乱索引顺序\n",
    "    split1, split2 = int(np.floor(train_size * num_train)), int(np.floor((train_size+valid_size) * num_train)) # 获得两个索引分割点\n",
    "    index = [indices[0:split1], indices[split1:split2], indices[split2:]] # 按分割点将索引分成三个列表\n",
    "    \n",
    "    # 开始分割数据集\n",
    "    train_set, valid_set = subDataset(Data[index[0]], Label[index[0]]), subDataset(Data[index[1]], Label[index[1]]) # 根据索引列表创建数据集的Dataset类\n",
    "    test_data = {'test_data':Data[index[2]], 'test_label':Label[index[2]]} # 测试集\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=shuffle, num_workers=0) # 训练集dataloader\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=shuffle, num_workers=0) # 验证集dataloader\n",
    "    return train_loader, valid_loader, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorchtools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-152-14ac22bf1d15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# import EarlyStopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpytorchtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorchtools'"
     ]
    }
   ],
   "source": [
    "# 创建模型训练函数，输出训练好的模型以及训练集和验证集随着epoch迭代的loss\n",
    "def train_model(model, criterion, optimizer, train_loader, valid_loader, n_epochs, stop_cnt, is_plt = True):\n",
    "    import numpy as np\n",
    "    avg_train_losses, avg_valid_losses = [], []\n",
    "    threshold = np.inf\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_losses, valid_losses = [], []\n",
    "        # 训练模型\n",
    "        model.train() # # 模型中有Dropout和BN层，训练时需要加上\n",
    "        for data, target in train_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2) # 梯度裁剪，参数介绍：参数集合；最大梯度范数；梯度范数类型\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        # 用模型计算验证集损失\n",
    "        model.eval() # 模型中有Dropout和BN层，验证或测试时需要加上\n",
    "        for data, target in valid_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        # 记录迭代结果\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        # 输出本次迭代指标结果\n",
    "        print(\"epoch: {} train_loss: {} valid_loss: {}\").format(epoch, train_loss, valid_loss)\n",
    "        \n",
    "        # 设置早停机制，对于某次迭代的损失结果，如果往后n次迭代损失与之相比都没有更低，就停止迭代\n",
    "        if valid_loss < threshold: threshold, cnt = valid_loss, 0\n",
    "        elif cnt <= stop_cnt: cnt += 1\n",
    "        else: break\n",
    "    \n",
    "    # 如果允许画出，则画出训练集和验证集的每个epoch的平均损失函数的变化图\n",
    "    if is_plt: \n",
    "        import matplotlib.pyplot as plt\n",
    "        fig = plt.figure(figsize=(10,8))\n",
    "        plt.plot(range(1, len(train_loss)+1), train_loss, label='Training Loss')\n",
    "        plt.plot(range(1, len(valid_loss)+1), valid_loss, label='Validation Loss')\n",
    "        minposs = valid_loss.index(min(valid_loss))+1 \n",
    "        plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('loss')\n",
    "        plt.ylim(0, 0.5)\n",
    "        plt.xlim(0, len(train_loss)+1)\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return  model, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型测试函数，输入训练好的模型和测试集,输出测试集预测概率和预测结果，以及可选择的是否评价\n",
    "def predict_model(model, test_X, test_y = None, metrics=None):\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    res = np.array(model(test_X).detach())\n",
    "    if test_y:\n",
    "        from sklearn.metrics import recall_score, roc_auc_score, accuracy_score\n",
    "        metrics_dict = {}\n",
    "        for i in metrics:\n",
    "            if \"auc\" in i:\n",
    "                \n",
    "            elif \"loss\" in \n",
    "        AUC = accuracy_score(test_y, res)\n",
    "        AUC = roc_auc_score(test_label, pred_res)\n",
    "        Recall = recall_score(test_label, pred_res)\n",
    "        print(\"ACC: {} AUC: {} Recall: {}\".format(ACC, AUC, Recall))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=1000, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# 数据准备\n",
    "\n",
    "\n",
    "# 定义网络结构、损失函数和梯度优化器\n",
    "class Net(nn.Module): # 定义网络结构\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(X.shape[1],1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)\n",
    "def xavier(m): # 参数Xavier初始化函数\n",
    "    if type(m) == nn.Linear: nn.init.xavier_uniform_(m.weight)\n",
    "model = Net() # 网络结构实例化\n",
    "model.apply(xavier) # 应用初始化参数\n",
    "criterion = nn.CrossEntropyLoss() # 损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters()) # 梯度迭代器\n",
    "\n",
    "\n",
    "# # 模型训练及调参\n",
    "# n_epochs = 100\n",
    "# stop_cnt = 100\n",
    "# train_loader, valid_loader, test_data = create_datasets(batch_size)\n",
    "# model, train_loss, valid_loss = train_model(model, criterion, optimizer, train_loader, valid_loader, n_epochs, stop_cnt, is_plt = True)\n",
    "\n",
    "# # 测试集效果\n",
    "# testing(model, test_data, test_label, threshold = 0.5)\n",
    "\n",
    "# 模型存储\n",
    "torch.save(model.state_dict(), \"simple_net\") # 存储训练好的模型参数\n",
    "model1 = Net() # 加载的时候自行写模型\n",
    "model1.load_state_dict(torch.load(\"simple_net\")) # 加载模型参数\n",
    "model1.eval() # 固定非参数层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=1000, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"simple_net\") # 存储训练好的模型参数\n",
    "model1 = Net() # 加载的时候自行写模型\n",
    "model1.load_state_dict(torch.load(\"simple_net\")) # 加载模型参数\n",
    "model1.eval() # 固定非参数层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 8.0000e-01,  1.8000e+00,  2.4000e+00,  ...,  1.0330e+04,\n",
      "          -5.5000e+01,  0.0000e+00],\n",
      "         [ 2.8000e+00,  3.2000e+00,  3.3000e+00,  ...,  1.0275e+04,\n",
      "          -5.5000e+01,  0.0000e+00],\n",
      "         [ 2.9000e+00,  2.8000e+00,  2.6000e+00,  ...,  1.0235e+04,\n",
      "          -4.0000e+01,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 6.0000e-01,  8.0000e-01,  6.0000e-01,  ...,  1.0130e+04,\n",
      "          -5.0000e+00,  8.4000e-01],\n",
      "         [ 1.7000e+00,  1.7000e+00,  1.1000e+00,  ...,  1.0130e+04,\n",
      "           0.0000e+00,  5.6000e-01],\n",
      "         [ 4.1000e+00,  4.6000e+00,  3.7000e+00,  ...,  1.0205e+04,\n",
      "           7.5000e+01,  2.1600e+00]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import recall_score, roc_auc_score, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # 关闭警告\n",
    "\n",
    "def create_dataloader(X, y, batch_size=1, shuffle=True): # X & y are tensor，and the first dim is sample_size\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    torch_dataset = TensorDataset(X, y)\n",
    "    loader = DataLoader(dataset=torch_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def get_loss(task):\n",
    "    if task == \"binary\":\n",
    "        criterion = torch.nn.BCELoss() # 注：此损失函数要求：1.每个样本只能有一个概率值，即输出是1 dim的tensor；2.要求标签是float类型\n",
    "    elif task == \"multiclass\":\n",
    "        criterion = torch.nn.CrossEntropyLoss() # 注：此损失函数要求：1.每个样本必须有每个类别的概率，即便是2分类，即输出是2 dim的tensor；2.要求标签是long类型\n",
    "    elif task == \"regression_1\":\n",
    "        criterion = torch.nn.L1Loss()\n",
    "    elif task == \"regression_2\":\n",
    "        criterion = torch.nn.MSELoss()\n",
    "    else:\n",
    "        raise ValueError(\"Please input correct value!!!\")\n",
    "    return criterion\n",
    "\n",
    "\n",
    "def get_optimizer(params, opt_criterion, learning_rate, l2):\n",
    "    if learning_rate <= 0 or l2 < 0: \n",
    "        raise ValueError(\"Please input correct learning_rate and l2!!!\")\n",
    "    if opt_criterion.lower() == \"adam\":\n",
    "        optimizer = torch.optim.Adam(params, lr=learning_rate, weight_decay=l2)\n",
    "    elif opt_criterion.lower() == \"adagrad\":\n",
    "        optimizer = torch.optim.Adagrad(params, lr=learning_rate, weight_decay=l2)\n",
    "    elif opt_criterion.lower() == \"rmsprop\":\n",
    "        optimizer = torch.optim.RMSprop(params, lr=learning_rate, weight_decay=l2)\n",
    "    elif opt_criterion.lower() == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(params, lr=learning_rate, weight_decay=l2)\n",
    "    else:\n",
    "        raise ValueError(\"Please input correct opt_criterion!!!\")\n",
    "    return optimizer\n",
    "\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.constant_(m.weight, 1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        \n",
    "def predict_model(model, test_X, test_y = None, batch_size=1, task=None, metrics=None):\n",
    "    test_loader = create_dataloader(test_X, test_y, batch_size=batch_size, shuffle=False)\n",
    "    loss_func = get_loss(task)\n",
    "    model = model.eval()\n",
    "    pred_ans, loss = [], 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device).float()\n",
    "            target = target.to(device).float().squeeze()\n",
    "            output = model(data)\n",
    "            pred_ans.append(output.data.numpy())\n",
    "            loss += loss_func(output, target).item()\n",
    "    y_pred = np.concatenate(pred_ans).astype(\"float64\")\n",
    "    \n",
    "    if test_y:\n",
    "        metrics_d = {}\n",
    "        for i in metrics:\n",
    "            if \"auc\" in i:\n",
    "                metrics_d[\"auc\"] = roc_auc_score(test_y.squeeze().data.numpy(), y_pred) # roc_auc_score的真实标签必须在前面\n",
    "            elif \"loss\" in i:\n",
    "                metrics_d[\"loss\"] = loss\n",
    "        return metrics_d\n",
    "    else:\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "def train_model(model, X, y, valid_data=None, valid_split=0., batch_size=1, opt_criterion=\"adam\", task=\"binary\", \\\n",
    "                metrics=[\"loss\",\"auc\"], eval_metric=\"auc\", epochs=100, early_stopping_bounds=None, \\\n",
    "                learning_rate=0.01, l2=0.01, shuffle=True, save_path=None, device=\"cpu\", verbose=0, is_plt=True):\n",
    "    \n",
    "    # 模型设置\n",
    "    model = model.to(device) # 将参数部署到指定设备\n",
    "    model.apply(weight_init) # 初始化参数\n",
    "    loss_func = get_loss(task) # 获取损失函数\n",
    "    optimizer = get_optimizer(model.parameters(), opt_criterion, learning_rate, l2) # 获取梯度优化器\n",
    "    \n",
    "    # 数据设置\n",
    "    if len(valid_data) == 2: # 优先自主设置验证集\n",
    "        train_loader = create_dataloader(X, y, batch_size=batch_size, shuffle=shuffle)\n",
    "        val_X, val_y = valid_data[0], valid_data[1]\n",
    "        valid_loader = create_dataloader(val_X, val_y, batch_size=batch_size, shuffle=shuffle)\n",
    "    elif 1 > valid_split > 0: # 从数据中拆分验证集\n",
    "        len_data = list(range(X.shape[0])) # 获取索引列表\n",
    "        np.random.shuffle(len_data) # 打乱索引列表\n",
    "        train_index, valid_index = len_data[:int((1-valid_split)*X.shape[0])], len_data[int((1-valid_split)*X.shape[0]):] # 获取训练集和验证集各自索引列表\n",
    "        X, val_X, y, val_y = X[train_index], X[valid_index], y[train_index], y[valid_index]\n",
    "        train_loader = create_dataloader(X, y, batch_size=batch_size, shuffle=shuffle)\n",
    "        valid_loader = create_dataloader(val_X, val_y, batch_size=batch_size, shuffle=shuffle)\n",
    "    elif valid_split == 0:\n",
    "        train_loader = create_dataloader(X, y, batch_size=batch_size, shuffle=shuffle)\n",
    "        valid_loader = None\n",
    "    else:\n",
    "        raise ValueError(\"Please input correct valid_dataset!!!\")\n",
    "\n",
    "    # 训练模型\n",
    "    metrics_total = [] # 所有迭代次数指标结果集合\n",
    "    metrics_dict = {} # 每个epoch指标结果\n",
    "    if eval_metric or save_path: # 如果使用早停或者要保存参数\n",
    "        if eval_metric not in [\"auc\", \"loss\", None]:\n",
    "                raise ValueError(\"Please input correct eval_metric!!!!\")\n",
    "        threshold = float(\"inf\") # 早停阈值（实时更新）\n",
    "        cnt = 0 # 早停计数器\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        metrics_dict[\"epoch\"] = epoch\n",
    "        \n",
    "        # 训练\n",
    "        model = model.train()\n",
    "        for data, target in train_loader:\n",
    "            data = data.to(device).float()\n",
    "            target = target.to(device).float().squeeze() # 标签后面做运算时候必须是1dim\n",
    "            output = model(data)\n",
    "            loss = loss_func(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10, norm_type=2) # 梯度裁剪，参数介绍：参数集合；最大梯度范数；梯度范数类型\n",
    "            optimizer.step()\n",
    "            \n",
    "        # 训练集指标获取\n",
    "        metric_train_dict = predict_model(model, X, y, batch_size=batch_size, task=task, metrics=metrics)\n",
    "        for i in metric_train_dict: metrics_dict[\"train_\"+i] = metric_train_dict[i]\n",
    "        if not valid_loader: \n",
    "            print(metrics_dict) # 如果不需要验证直接输出指标结果\n",
    "            metrics_dict.pop(\"epoch\")\n",
    "            metrics_total.append(metrics_dict) # 记录指标结果\n",
    "        \n",
    "        # 验证\n",
    "        if valid_loader: # 如果需要验证\n",
    "            # 验证集指标获取\n",
    "            metric_val_dict = predict_model(model, val_X, val_y, batch_size=batch_size, task=task, metrics=metrics)\n",
    "            for i in metric_val_dict: metrics_dict[\"val_\"+i] = metric_val_dict[i]\n",
    "            print(metrics_dict)\n",
    "            metrics_dict.pop(\"epoch\")\n",
    "            metrics_total.append(metrics_dict)\n",
    "            \n",
    "            if eval_metric or save_path:        \n",
    "                val_metric = metrics_dict[\"val_\"+eval_metric] if eval_metric == \"loss\" else -metrics_dict[\"val_\"+eval_metric]\n",
    "                if val_metric < threshold:\n",
    "                    threshold, cnt = val_metric, 0\n",
    "                    if save_path:\n",
    "                        torch.save(model.state_dict(), save_path) # 载入模型参数时：model = Network().load_state_dict(torch.load(path)), Network是该自定义的网络\n",
    "                        print(\"model is saved\")\n",
    "                elif val_metric >= threshold and eval_metric:\n",
    "                    cnt += 1\n",
    "                    if cnt > early_stopping_bounds: \n",
    "                        print(\"EarlyStopping!!!\")\n",
    "                        break\n",
    "    \n",
    "        \n",
    "    # 指标可视化\n",
    "    if is_plt:\n",
    "        metrics_total_dict = defaultdict(list)\n",
    "        for m_dict in metrics_total:\n",
    "            for key in m_dict:\n",
    "                metrics_total_dict[key].append(m_dict[key])\n",
    "        metrics_total_list = list(metrics_total_dict.values())\n",
    "        metrics_total_names = list(metrics_total_dict.keys())\n",
    "        \n",
    "        fig, ax = plt.subplots(len(metrics_total_list), figsize=(15,10))\n",
    "        for i in range(len(ax)):\n",
    "            train_data, valid_data = metrics_total_list[i], metrics_total_list[min(i+2), len(ax)-1)]\n",
    "            train_name, valid_name = metrics_total_names[i], metrics_total_names[min(i+2), len(ax)-1]\n",
    "            ax[i].plot(range(1, len(train_data)+1), train_data, label=train_name)\n",
    "            ax[i].plot(range(1, len(valid_data)+1), valid_data, label=valid_name)\n",
    "            ax[i].set_xlabel('epoch')\n",
    "            ax[i].grid(True)\n",
    "            ax[i].legend()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [list(range(1,10)), list(range(1,10))]\n",
    "y = [list(range(2,11)), list(range(2,11))]\n",
    "fig, ax = plt.subplots(2, figsize=(15,10))\n",
    "for i in range(len(ax)):\n",
    "    ax[i].plot(range(len(x[i])), x[i], label=\"jjj\")\n",
    "    ax[i].plot(range(len(y[i])), y[i], label=\"j\")\n",
    "    ax[i].set_xlabel(\"hshhshs\")\n",
    "    ax[i].set_ylabel('loss')\n",
    "    ax[i].set_xlim(0, 10)\n",
    "    ax[i].set_ylim(0,10)\n",
    "    ax[i].grid(True)\n",
    "    ax[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=72, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # 关闭警告\n",
    "pd.set_option('display.max_columns', 5000)\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "\n",
    "data = pd.read_csv('C:/Users/24848/Desktop/onehr.data', sep=',', header=None)\n",
    "data = data.iloc[:1000, :]\n",
    "\n",
    "for i in data.columns[1:]:\n",
    "    data[i] = np.where(data[i] == \"?\", 0, data[i])\n",
    "    data[i] = data[i].astype(\"float\")\n",
    "X_np, y_np = np.array(data.iloc[:,1:-1]), np.array(data.iloc[:,-1])\n",
    "X, y = torch.from_numpy(X_np).unsqueeze(dim=0), torch.from_numpy(y_np).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "for data, target in create_dataloader(X, y):\n",
    "    print(target.squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(np.array([1,0,1]).astype(\"float\"), np.array([0.1,0.2,0.3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本数据预处理过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 使用gensim处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts: [['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'], ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'management', 'system'], ['system', 'human', 'system', 'engineering', 'testing', 'eps'], ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'], ['generation', 'random', 'binary', 'unordered', 'trees'], ['intersection', 'graph', 'paths', 'trees'], ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'], ['graph', 'minors', 'survey']]\n",
      "\n",
      "texts1: [['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n",
      "\n",
      "ID:  {'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n",
      "Freq:  {1: 2, 2: 2, 0: 2, 4: 2, 7: 3, 5: 3, 3: 2, 6: 2, 8: 2, 9: 3, 10: 3, 11: 2}\n",
      "\n",
      "corpus: [[(0, 1), (1, 1), (2, 1)], [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(2, 1), (5, 1), (7, 1), (8, 1)], [(1, 1), (5, 2), (8, 1)], [(3, 1), (6, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(4, 1), (10, 1), (11, 1)]]\n",
      "TFIDF矩阵: [[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)], [(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)], [(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)], [(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)], [(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)], [(9, 1.0)], [(9, 0.7071067811865475), (10, 0.7071067811865475)], [(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)], [(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 9.7702928e-03,  8.1651136e-03,  1.2809705e-03,  5.0975773e-03,\n",
       "         1.4081288e-03, -6.4551616e-03, -1.4280510e-03,  6.4491653e-03,\n",
       "        -4.6173073e-03, -3.9930656e-03,  4.9244044e-03,  2.7130984e-03,\n",
       "        -1.8479753e-03, -2.8769446e-03,  6.0107303e-03, -5.7167388e-03,\n",
       "        -3.2367038e-03, -6.4878250e-03, -4.2346334e-03, -8.5809948e-03,\n",
       "        -4.4697905e-03, -8.5112313e-03,  1.4037776e-03, -8.6181974e-03,\n",
       "        -9.9166557e-03, -8.2016252e-03, -6.7726658e-03,  6.6805840e-03,\n",
       "         3.7845564e-03,  3.5616636e-04, -2.9579829e-03, -7.4283220e-03,\n",
       "         5.3341867e-04,  4.9989222e-04,  1.9561767e-04,  8.5259438e-04,\n",
       "         7.8633073e-04, -6.8161491e-05, -8.0070542e-03, -5.8702733e-03,\n",
       "        -8.3829118e-03, -1.3120436e-03,  1.8206357e-03,  7.4171280e-03,\n",
       "        -1.9634271e-03, -2.3252917e-03,  9.4871549e-03,  7.9703328e-05,\n",
       "        -2.4045228e-03,  8.6048460e-03,  2.6870037e-03, -5.3439736e-03,\n",
       "         6.5881060e-03,  4.5101522e-03, -7.0544672e-03, -3.2317400e-04,\n",
       "         8.3448651e-04,  5.7473565e-03, -1.7176557e-03, -2.8065301e-03,\n",
       "         1.7484308e-03,  8.4717036e-04,  1.1928272e-03, -2.6342822e-03,\n",
       "        -5.9857843e-03,  7.3229838e-03,  7.5873756e-03,  8.2963565e-03,\n",
       "        -8.5988473e-03,  2.6364254e-03, -3.5599638e-03,  9.6204039e-03,\n",
       "         2.9037667e-03,  4.6411133e-03,  2.3856140e-03,  6.6084769e-03,\n",
       "        -5.7432912e-03,  7.8944108e-03, -2.4109220e-03, -4.5618867e-03,\n",
       "        -2.0609903e-03,  9.7335577e-03, -6.8565919e-03, -2.1917201e-03,\n",
       "         7.0009995e-03, -5.5749417e-05, -6.2949681e-03, -6.3935257e-03,\n",
       "         8.9403940e-03,  6.4295745e-03,  4.7735930e-03, -3.2620477e-03,\n",
       "        -9.2676207e-03,  3.7868882e-03,  7.1605491e-03, -5.6328895e-03,\n",
       "        -7.8650145e-03, -2.9727411e-03, -4.9318983e-03, -2.3151112e-03],\n",
       "       [ 7.0871473e-03, -1.5683782e-03,  7.9461364e-03, -9.4874427e-03,\n",
       "        -8.0296379e-03, -6.6422895e-03, -4.0041069e-03,  4.9910326e-03,\n",
       "        -3.8136279e-03, -8.3215833e-03,  8.4132208e-03, -3.7471871e-03,\n",
       "         8.6089820e-03, -4.8962692e-03,  3.9189272e-03,  4.9236091e-03,\n",
       "         2.3943025e-03, -2.8213640e-03,  2.8496354e-03, -8.2571078e-03,\n",
       "        -2.7652937e-03, -2.5905678e-03,  7.2491039e-03, -3.4629786e-03,\n",
       "        -6.5992209e-03,  4.3399399e-03, -4.7540484e-04, -3.5954388e-03,\n",
       "         6.8824128e-03,  3.8715161e-03, -3.8985382e-03,  7.7237905e-04,\n",
       "         9.1438834e-03,  7.7564754e-03,  6.3607404e-03,  4.6693156e-03,\n",
       "         2.3858545e-03, -1.8414279e-03, -6.3720327e-03, -2.9998334e-04,\n",
       "        -1.5642069e-03, -5.7058921e-04, -6.2630447e-03,  7.4330512e-03,\n",
       "        -6.5910248e-03, -7.2386782e-03, -2.7575779e-03, -1.5144736e-03,\n",
       "        -7.6353899e-03,  6.9955905e-04, -5.3253169e-03, -1.2739648e-03,\n",
       "        -7.3665036e-03,  1.9612678e-03,  3.2734126e-03, -2.4741203e-05,\n",
       "        -5.4490739e-03, -1.7256952e-03,  7.0866016e-03,  3.7357579e-03,\n",
       "        -8.8817989e-03, -3.4116022e-03,  2.3567537e-03,  2.1376403e-03,\n",
       "        -9.4648376e-03,  4.5697815e-03, -8.6582452e-03, -7.3883547e-03,\n",
       "         3.4832379e-03, -3.4728234e-03,  3.5656209e-03,  8.8950340e-03,\n",
       "        -3.5753339e-03,  9.3197320e-03,  1.7111641e-03,  9.8462272e-03,\n",
       "         5.7051540e-03, -9.1497740e-03, -3.3269732e-03,  6.5306681e-03,\n",
       "         5.6031067e-03,  8.7057864e-03,  6.9263605e-03,  8.0408510e-03,\n",
       "        -9.8231193e-03,  4.2975070e-03, -5.0308690e-03,  3.5132242e-03,\n",
       "         6.0578999e-03,  4.3924102e-03,  7.5100451e-03,  1.4993446e-03,\n",
       "        -1.2642510e-03,  5.7697659e-03, -5.6368350e-03,  3.7106704e-05,\n",
       "         9.4556073e-03, -5.4809595e-03,  3.8159816e-03, -8.1130704e-03],\n",
       "       [-5.1577436e-03, -6.6702794e-03, -7.7790986e-03,  8.3131464e-03,\n",
       "        -1.9829201e-03, -6.8569588e-03, -4.1555981e-03,  5.1456233e-03,\n",
       "        -2.8699716e-03, -3.7507527e-03,  1.6218971e-03, -2.7771019e-03,\n",
       "        -1.5848217e-03,  1.0748033e-03, -2.9788127e-03,  8.5217617e-03,\n",
       "         3.9120731e-03, -9.9617615e-03,  6.2614209e-03, -6.7562214e-03,\n",
       "         7.6965551e-04,  4.4055167e-03, -5.1048612e-03, -2.1112841e-03,\n",
       "         8.0978349e-03, -4.2450288e-03, -7.6384838e-03,  9.2606070e-03,\n",
       "        -2.1561245e-03, -4.7208075e-03,  8.5732946e-03,  4.2845840e-03,\n",
       "         4.3260953e-03,  9.2872158e-03, -8.4555410e-03,  5.2568498e-03,\n",
       "         2.0399450e-03,  4.1894992e-03,  1.6983944e-03,  4.4654319e-03,\n",
       "         4.4875946e-03,  6.1062989e-03, -3.2030295e-03, -4.5770612e-03,\n",
       "        -4.2664065e-04,  2.5344712e-03, -3.2641180e-03,  6.0594808e-03,\n",
       "         4.1553383e-03,  7.7668522e-03,  2.5700203e-03,  8.1190439e-03,\n",
       "        -1.3876136e-03,  8.0802785e-03,  3.7180968e-03, -8.0496678e-03,\n",
       "        -3.9347606e-03, -2.4725979e-03,  4.8944671e-03, -8.7241287e-04,\n",
       "        -2.8317324e-03,  7.8359870e-03,  9.3256142e-03, -1.6153983e-03,\n",
       "        -5.1607513e-03, -4.7031282e-03, -4.8474614e-03, -9.6056210e-03,\n",
       "         1.3724195e-03, -4.2261453e-03,  2.5274432e-03,  5.6161173e-03,\n",
       "        -4.0670903e-03, -9.5993746e-03,  1.5471478e-03, -6.7020725e-03,\n",
       "         2.4959005e-03, -3.7817324e-03,  7.0804814e-03,  6.4040697e-04,\n",
       "         3.5619752e-03, -2.7399310e-03, -1.7110463e-03,  7.6550203e-03,\n",
       "         1.4080878e-03, -5.8521517e-03, -7.8367768e-03,  1.2330448e-03,\n",
       "         6.4565083e-03,  5.5579669e-03, -8.9796642e-03,  8.5946647e-03,\n",
       "         4.0481547e-03,  7.4717780e-03,  9.7491713e-03, -7.2917030e-03,\n",
       "        -9.0425937e-03,  5.8376994e-03,  9.3939463e-03,  3.5079459e-03]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]\n",
    "\n",
    "# 去掉停用词\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]\n",
    "print(\"texts: {}\".format(texts))\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)   # 生成词典\n",
    "# 去掉出现少于两次的单词，去掉出现多于5次的单词，剩下的单词保留前100个\n",
    "dictionary.filter_extremes(no_below = 2, no_above = 5, keep_n = 100)\n",
    "texts1 = [[word for word in text if word in dictionary.token2id] for text in texts] # 更新texts\n",
    "print(\"\\ntexts1: {}\".format(texts1))\n",
    "print(\"\\nID: \", dictionary.token2id) # diction.token2id 存放的是单词-id key-value对\n",
    "print(\"Freq: \", dictionary.dfs) # diction.dfs 存放的是单词的出现频率\n",
    "\n",
    "# 1. 下面是用TF-IDF模型进行编码\n",
    "# 参考资料1：https://zhuanlan.zhihu.com/p/37175253\n",
    "# 参考资料2：https://www.cnblogs.com/keye/p/9190304.html\n",
    "\n",
    "# 建立语料库\n",
    "corpus = [dictionary.doc2bow(text) for text in texts] # 将文本数据转为id和频率数据（主要看dic的状态，texts不更新结果也一样）\n",
    "print(\"\\ncorpus: {}\".format(corpus))\n",
    "\n",
    "# 建立TF-IDF模型并生成矩阵\n",
    "tfidf_model = models.TfidfModel(corpus) # 训练TF-IDF模型，这个模型可以保存，详情见参考资料\n",
    "corpus_tfidf = tfidf_model[corpus] # 进行预测，获得TF-IDF矩阵，但是是个迭代器类型，只能用for来把每个样本的向量取出\n",
    "# 注意：上面这个预测过程不仅仅可以预测原数据，也可以预测别的数据，只要保证数据格式符合要求，且其映射成的字典需与模型训练集的相同\n",
    "print(\"TFIDF矩阵: {}\".format([v for v in corpus_tfidf]))\n",
    "\n",
    "# 2. 下面是用work2vec方法进行编码\n",
    "# 关于使用方法的详情，请见官方文档：https://radimrehurek.com/gensim/models/word2vec.html\n",
    "model = models.word2vec.Word2Vec(texts1, min_count=1) # 训练模型，输入的是分词的结果，参数细节见https://blog.csdn.net/u011748542/article/details/85880852\n",
    "model.wv.most_similar(\"user\",topn=10) # 计算训练集见过的词中与输入词的相似度，并降序排列取出前十个(输入词必须见过)\n",
    "model.wv.similarity('user', 'computer') # 计算两个输入词的相似性（输入词必须都见过）\n",
    "# model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1) # 猜词，结果是queen，这个不使用前面的数据了\n",
    "model.wv[texts1[0]].mean(axis = 1) # 测试模型，输入的词必须是训练过的，这里输入第一个样本的分词结果，得到第一个样本的全部词向量，然后做平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用nn.embedding+预训练处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用于深度学习NLP问题，纯词向量特征，每个词一个向量后pooling在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3],\n",
      "        [3],\n",
      "        [4],\n",
      "        [4],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [7],\n",
      "        [7]]), batch_sizes=tensor([2, 2, 2, 2, 2, 2, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "PackedSequence(data=tensor([[ 0.1443,  0.0625, -0.0232,  0.1436,  0.1275],\n",
      "        [ 0.2047,  0.1032,  0.0009,  0.2001,  0.1720],\n",
      "        [ 0.2918,  0.1329, -0.0132,  0.2646,  0.2305],\n",
      "        [ 0.3570,  0.1644,  0.0121,  0.3265,  0.2902],\n",
      "        [ 0.4013,  0.1807,  0.0043,  0.3498,  0.3163],\n",
      "        [ 0.4515,  0.1960,  0.0237,  0.3876,  0.3774],\n",
      "        [ 0.4726,  0.2066,  0.0198,  0.3947,  0.3901],\n",
      "        [ 0.5081,  0.2103,  0.0323,  0.4091,  0.4491],\n",
      "        [ 0.5179,  0.2178,  0.0305,  0.4110,  0.4556],\n",
      "        [ 0.5428,  0.2147,  0.0372,  0.4115,  0.5099],\n",
      "        [ 0.5474,  0.2205,  0.0364,  0.4120,  0.5132],\n",
      "        [ 0.5649,  0.2140,  0.0388,  0.4063,  0.5607],\n",
      "        [ 0.5669,  0.2188,  0.0385,  0.4065,  0.5624]], grad_fn=<CatBackward>), batch_sizes=tensor([2, 2, 2, 2, 2, 2, 1]), sorted_indices=None, unsorted_indices=None) \n",
      "\n",
      "\n",
      "tensor([[[ 0.1443,  0.0625, -0.0232,  0.1436,  0.1275],\n",
      "         [ 0.2918,  0.1329, -0.0132,  0.2646,  0.2305],\n",
      "         [ 0.4013,  0.1807,  0.0043,  0.3498,  0.3163],\n",
      "         [ 0.4726,  0.2066,  0.0198,  0.3947,  0.3901],\n",
      "         [ 0.5179,  0.2178,  0.0305,  0.4110,  0.4556],\n",
      "         [ 0.5474,  0.2205,  0.0364,  0.4120,  0.5132],\n",
      "         [ 0.5669,  0.2188,  0.0385,  0.4065,  0.5624]],\n",
      "\n",
      "        [[ 0.2047,  0.1032,  0.0009,  0.2001,  0.1720],\n",
      "         [ 0.3570,  0.1644,  0.0121,  0.3265,  0.2902],\n",
      "         [ 0.4515,  0.1960,  0.0237,  0.3876,  0.3774],\n",
      "         [ 0.5081,  0.2103,  0.0323,  0.4091,  0.4491],\n",
      "         [ 0.5428,  0.2147,  0.0372,  0.4115,  0.5099],\n",
      "         [ 0.5649,  0.2140,  0.0388,  0.4063,  0.5607],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<TransposeBackward0>) \n",
      "\n",
      "\n",
      "tensor([[[0.3482, 0.6518],\n",
      "         [0.3367, 0.6633],\n",
      "         [0.3292, 0.6708],\n",
      "         [0.3260, 0.6740],\n",
      "         [0.3256, 0.6744],\n",
      "         [0.3267, 0.6733],\n",
      "         [0.3285, 0.6715]],\n",
      "\n",
      "        [[0.3417, 0.6583],\n",
      "         [0.3319, 0.6681],\n",
      "         [0.3276, 0.6724],\n",
      "         [0.3266, 0.6734],\n",
      "         [0.3275, 0.6725],\n",
      "         [0.3292, 0.6708],\n",
      "         [0.3580, 0.6420]]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataset as Dataset\n",
    "import torch.utils.data.dataloader as DataLoader\n",
    "# 看下面内容前，一定先看参考资料：https://blog.csdn.net/lssc4205/article/details/79474735，弄清楚三个pad函数是干嘛的\n",
    "\n",
    "# 1. 假设我们已经完成了所有的分词工作，并且要使用预训练好的大型embedding矩阵进行编码\n",
    "# 注：（其实可以将nn.Embedding作为层放到网络中随模型一起进行有监督的训练来编码，那样最原始的输入不需要embedding，但暂时不讨论这个）\n",
    "# 2. 假设我们获得了大型embedding矩阵的词字典及每个词的索引位置，并将自己分好词的数据集中每个词转换为对应的词索引\n",
    "# 3. 文本转为索引后，经常面临不同batch维度不一致问题，导致数据不能输入模型，下面是一般的处理过程：\n",
    "\n",
    "# 3.1 首先把词索引矩阵list一下，变成下面的列表型，每个元素都是一个样本的词向量索引列表，然后按索引长度对样本降序排序：（降序的原因在后面）\n",
    "train_x = [torch.tensor([0, 1, 3]),\n",
    "           torch.tensor([2, 3]),\n",
    "           torch.tensor([3])]\n",
    "\n",
    "# 3.2 然后使用pad_sequence对长度不齐的部分进行数据补值，默认值自选padding_value\n",
    "train_x1 = nn.utils.rnn.pad_sequence(train_x, batch_first=True, padding_value=0) # 填充值可以随便选，后面不会被计算\n",
    "\n",
    "# 3.3 维度问题解决后，进行假装进行embedding\n",
    "weight = torch.FloatTensor([[1, 2.3, 3], \n",
    "                            [4, 5.1, 6.3],\n",
    "                            [0.3,4.5,2.4],\n",
    "                            [3, 4, 5.0]]) # 假装weight就是大型矩阵\n",
    "embedding = nn.Embedding.from_pretrained(weight) # 直接承接预训练结果\n",
    "embedded = embedding(train_x1) # 对填充后的tensor列表进行embedding操作，结果集中原本不存在的部分也被编码了，但后面会去除\n",
    "\n",
    "# 3.4 关键步骤：对上述结果压缩为PackedSequence类型，第二个参数是每个样本的词个数组成的列表，必须用最开始数据的长度列表（压缩的原因见最后）\n",
    "padded_data = nn.utils.rnn.pack_padded_sequence(embedded, [len(data) for data in train_x], batch_first=True)\n",
    "# 解答：样本按词数降序排序的原因就是为了在告诉函数长度列表后，他能知道每个seq中哪些样本的词是被填补的词，这部分是不会输入模型的\n",
    "# 注意：其实到3.4为止，其结果就可以被RNN和LSTM给接受了（attention暂时不知)，可以直接输入模型，模型自己就知道该跑哪些部分，但是线性层不可以使用\n",
    "\n",
    "# 3.5 将数据转回tensor格式，所有填补的词的embedding向量在这里都是0向量\n",
    "packed_data = nn.utils.rnn.pad_packed_sequence(padded_data, batch_first=True) # 这个结果可以输入模型了\n",
    "# 注：压缩原因：一般为了避免不必要的存储和计算（填充词即便是0也会占资源，调用计算），在RNN和LSTM模型中，不把3.5后结果扔进去，而是用3.4后的结果，填充的部分自动忽略\n",
    "# 当从循环网络出来后接线性层时，再把得到的结果经过3.5过程转化为tensor；如果不是RNN或LSTM，就不一定能这样做了\n",
    "\n",
    "\n",
    "#  以下步骤待定，不一定采用，下次再写\n",
    "class MyData(Dataset.Dataset):\n",
    "    def __init__(self, train_x):\n",
    "        self.train_x = train_x\n",
    "    def __len__(self):\n",
    "        return len(self.train_x)\n",
    "    def __getitem__(self, item):\n",
    "        return self.train_x[item]\n",
    "\n",
    "def collate_fn(train_data):\n",
    "    train_data.sort(key = lambda data: len(data), reverse = True)\n",
    "    data_length = [len(data) for data in train_data]\n",
    "    train_data = nn.utils.rnn.pad_sequence(train_data, batch_first=True, padding_value=0) # padding过程\n",
    "    return train_data.unsqueeze(-1), data_length\n",
    "\n",
    "train_data = MyData(train_x)\n",
    "train_dataloader = DataLoader.DataLoader(train_data, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "# padding后的结果\n",
    "# for data, length in train_dataloader:\n",
    "#     print(data)\n",
    "#     print(length)\n",
    "# padding后会有资源浪费，继续padded可以让每次循环输入后没有0填充值（必须再padding后操作）\n",
    "\n",
    "net = nn.LSTM(1, 5, batch_first=True) # 简单的模型\n",
    "Linear = nn.Linear(5, 2)\n",
    "for data, length in train_dataloader:\n",
    "    data = nn.utils.rnn.pack_padded_sequence(data, length, batch_first=True) # padded操作\n",
    "    print(data) # padded后的数据\n",
    "    output, hidden = net(data.float()) # 将padded后的数据输入模型得到output，这个padded后的输入仅能在LSTM或RNN层输入，不能输入普通线性层\n",
    "    print(output,\"\\n\\n\")\n",
    "    output, out_len = nn.utils.rnn.pad_packed_sequence(output, batch_first=True) # 将output转回正常形态以方便接下来接入线性层\n",
    "    print(output,\"\\n\\n\") # 普通状态的output展示\n",
    "    print(F.softmax(Linear(output), dim = 2)) # 输出概率结果\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m train_x \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m]),\n\u001b[0;32m      2\u001b[0m            torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]),\n\u001b[0;32m      3\u001b[0m            torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m3\u001b[39m])]\n\u001b[1;32m----> 4\u001b[0m train_x1 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpad_sequence(train_x, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      5\u001b[0m weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2.3\u001b[39m, \u001b[38;5;241m3\u001b[39m], \n\u001b[0;32m      6\u001b[0m                             [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5.1\u001b[39m, \u001b[38;5;241m6.3\u001b[39m],\n\u001b[0;32m      7\u001b[0m                             [\u001b[38;5;241m0.3\u001b[39m,\u001b[38;5;241m4.5\u001b[39m,\u001b[38;5;241m2.4\u001b[39m],\n\u001b[0;32m      8\u001b[0m                             [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5.0\u001b[39m]]) \u001b[38;5;66;03m# 假装weight就是大型矩阵\u001b[39;00m\n\u001b[0;32m      9\u001b[0m embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding\u001b[38;5;241m.\u001b[39mfrom_pretrained(weight) \u001b[38;5;66;03m# 直接承接预训练结果\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "train_x = [torch.tensor([0, 1, 3]),\n",
    "           torch.tensor([2, 3]),\n",
    "           torch.tensor([3])]\n",
    "train_x1 = nn.utils.rnn.pad_sequence(train_x, batch_first=True, padding_value=0)\n",
    "weight = torch.FloatTensor([[1, 2.3, 3], \n",
    "                            [4, 5.1, 6.3],\n",
    "                            [0.3,4.5,2.4],\n",
    "                            [3, 4, 5.0]]) # 假装weight就是大型矩阵\n",
    "embedding = nn.Embedding.from_pretrained(weight) # 直接承接预训练结果\n",
    "embedded = embedding(train_x1)\n",
    "padded_embed = nn.utils.rnn.pack_padded_sequence(embedded, [len(data) for data in train_x], batch_first=True)\n",
    "linear = nn.Linear(3,1)\n",
    "linear(padded_embed)\n",
    "# nn.utils.rnn.pad_packed_sequence(padded_embed, batch_first=True)\n",
    "# 4. 接下来是embedding，把每个tensor换成词向量，然后再进行下面的操作，假装embedding过了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
